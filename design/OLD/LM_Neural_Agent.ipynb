{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import random_split\n",
    "import itertools\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"LM_PMPNN_GRID_Functions_011/all_scores.csv\")\n",
    "\n",
    "# Ensure the index is set correctly if it's not already\n",
    "df['index'] = df['index'].astype(int)\n",
    "df.set_index('index', inplace=True)\n",
    "\n",
    "df['parent_index'] = pd.to_numeric(df['parent_index'], errors='coerce').fillna(-1).astype(int)\n",
    "\n",
    "# Convert design_method to a binary variable\n",
    "df['action'] = (df['design_method'] == 'ProteinMPNN').astype(int)\n",
    "\n",
    "# Function to get parent scores\n",
    "def get_parent_scores(row):\n",
    "    # Check if the parent_index is -1, indicating a \"Parent\" entry\n",
    "    if row['parent_index'] == -1:\n",
    "        # Return the row's own scores, or alternatively, return default scores\n",
    "        return pd.Series([row['interface_score'], row['total_score'], row['catalytic_score']])\n",
    "    \n",
    "    # If parent_index is valid (not -1), proceed to find the parent row\n",
    "    parent_row = df.loc[row['parent_index']]\n",
    "    return pd.Series([parent_row['interface_score'], parent_row['total_score'], parent_row['catalytic_score']])\n",
    "\n",
    "score_columns = ['interface_score', 'total_score', 'catalytic_score']\n",
    "scaler = StandardScaler()\n",
    "df[score_columns] = scaler.fit_transform(df[score_columns])\n",
    "\n",
    "# Apply the function to get parent scores\n",
    "df[['parent_interface_score', 'parent_total_score', 'parent_catalytic_score']] = df.apply(get_parent_scores, axis=1)\n",
    "\n",
    "# Calculate the reward as the difference between the child's and its parent's scores\n",
    "df['reward'] = (df['parent_interface_score'] - df['interface_score']) + \\\n",
    "               (df['parent_total_score'] - df['total_score']) + \\\n",
    "               (df['parent_catalytic_score'] - df['catalytic_score'])\n",
    "\n",
    "# Select relevant columns for the dataset\n",
    "df = df[['parent_interface_score', 'parent_total_score', 'parent_catalytic_score', 'action', 'reward']]\n",
    "\n",
    "df = df.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIzymesDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        state = torch.tensor([row['parent_interface_score'], row['parent_total_score'], row['parent_catalytic_score']], dtype=torch.float)\n",
    "        action = torch.tensor(row['action'], dtype=torch.long)\n",
    "        reward = torch.tensor(row['reward'], dtype=torch.float)\n",
    "        return state, action, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Dataset\n",
    "dataset = AIzymesDataset(df)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "val_size = int(dataset_size * 0.2)  # 20% of the data for validation\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralBanditModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \n",
    "        super(NeuralBanditModel, self).__init__()\n",
    "        \n",
    "        hidden_size = 2 ** (input_size) - 1\n",
    "        #hidden_size = math.factorial(input_size)\n",
    "        \n",
    "        self.fc1  = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2  = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3  = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class NeuralBanditAgent:\n",
    "    def __init__(self, n_actions, n_features, hidden_size=3, epsilon_0=0.9, epsilon_1=0.1, epsilon_decay=0.995, lr=0.01):\n",
    "        self.n_actions = n_actions\n",
    "        self.model = NeuralBanditModel(n_features, hidden_size, n_actions)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.action_counts = np.zeros(n_actions, dtype=int)\n",
    "        self.rewards = []\n",
    "        self.states = []\n",
    "        self.norm_rewards = []\n",
    "        self.norm_states = []\n",
    "        self.actions_taken = []\n",
    "        self.loss = []\n",
    "        self.predecessors = []\n",
    "        self.normalization    = {\n",
    "            'states_mean': 0,\n",
    "            'states_std': 0,\n",
    "            'rewards_mean': 0,\n",
    "            'rewards_std': 0\n",
    "        }  \n",
    "        self.epsilon_0 = epsilon_0\n",
    "        self.epsilon_1 = epsilon_1\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.fitting_times = []\n",
    "        self.RMSE_weights = []\n",
    "        self.initial_model_state = copy.deepcopy([param.data.numpy() for param in self.model.parameters()])\n",
    "        self.initial_model_state = np.concatenate([arr.flatten() for arr in self.initial_model_state])\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon_0 + self.epsilon_1:\n",
    "            return random.randint(0, self.n_actions - 1)  # Random action\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            predicted_rewards = self.model(state_tensor)\n",
    "            return torch.argmax(predicted_rewards).item()\n",
    "    \n",
    "    def set_normalization_parameters(self, dataloader):\n",
    "        \"\"\"\n",
    "        Calculate and set normalization parameters (mean and std) for states and rewards\n",
    "        based on the data provided by the dataloader.\n",
    "\n",
    "        Parameters:\n",
    "        - dataloader: DataLoader providing batches of (states, actions, rewards).\n",
    "        \"\"\"\n",
    "        state_sums = None\n",
    "        reward_sum = 0.0\n",
    "        state_squares_sum = None\n",
    "        reward_square_sum = 0.0\n",
    "        num_samples = 0\n",
    "\n",
    "        for states, _, rewards in dataloader:\n",
    "            if state_sums is None:\n",
    "                state_sums = torch.zeros(states.shape[1])\n",
    "                state_squares_sum = torch.zeros(states.shape[1])\n",
    "            \n",
    "            state_sums += states.sum(dim=0)\n",
    "            state_squares_sum += (states ** 2).sum(dim=0)\n",
    "            reward_sum += rewards.sum()\n",
    "            reward_square_sum += (rewards ** 2).sum()\n",
    "            num_samples += states.shape[0]\n",
    "\n",
    "        # Calculate mean\n",
    "        state_means = state_sums / num_samples\n",
    "        reward_mean = reward_sum / num_samples\n",
    "\n",
    "        # Calculate std\n",
    "        state_stds = torch.sqrt(state_squares_sum / num_samples - state_means ** 2)\n",
    "        reward_std = torch.sqrt(reward_square_sum / num_samples - reward_mean ** 2)\n",
    "\n",
    "\n",
    "        # Set normalization parameters\n",
    "        self.normalization = {\n",
    "            'states_mean': state_means,\n",
    "            'states_std': state_stds,\n",
    "            'rewards_mean': reward_mean,\n",
    "            'rewards_std': reward_std\n",
    "        }\n",
    "\n",
    "    def normalize(self, states, rewards):\n",
    "        \"\"\"\n",
    "        Normalize states and rewards for a batch of data.\n",
    "\n",
    "        Parameters:\n",
    "        - states: Tensor, batch of states.\n",
    "        - rewards: Tensor, batch of rewards.\n",
    "\n",
    "        Returns:\n",
    "        - norm_states: Tensor, normalized states.\n",
    "        - norm_rewards: Tensor, normalized rewards.\n",
    "        \"\"\"\n",
    "\n",
    "        norm_states = (states - torch.FloatTensor(self.normalization['states_mean'])) / torch.FloatTensor(self.normalization['states_std'])\n",
    "\n",
    "        norm_rewards = (rewards - self.normalization['rewards_mean']) / self.normalization['rewards_std']\n",
    "\n",
    "        return norm_states, norm_rewards\n",
    "\n",
    "    def reinitialize_weights(self, model):\n",
    "        #Reinitializes the weights of a given model.\n",
    "        if isinstance(model, nn.Linear):\n",
    "            model.reset_parameters()\n",
    "        elif hasattr(model, 'children'):\n",
    "            for child in model.children():\n",
    "                self.reinitialize_weights(child)\n",
    "        \n",
    "    def update_model(self, states, actions, rewards):\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        #self.epsilon_0 *= self.epsilon_decay\n",
    "        \n",
    "        start_time = time.time()  # Start time\n",
    "\n",
    "\n",
    "        norm_states, norm_rewards = self.normalize(states, rewards)\n",
    "\n",
    "        # Predict rewards for all actions given the batch of states\n",
    "        predicted_rewards = self.model(norm_states)\n",
    "\n",
    "        # Calculate loss for the actions taken\n",
    "        # This assumes a model output shape of [batch_size, n_actions] and that actions are indices\n",
    "        criterion = nn.MSELoss()\n",
    "        action_indices = actions.unsqueeze(1)  # Add an extra dimension to index predicted_rewards\n",
    "        predicted_rewards_for_actions = predicted_rewards.gather(1, action_indices).squeeze()\n",
    "        loss = criterion(predicted_rewards_for_actions, norm_rewards)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        end_time = time.time()\n",
    "        self.fitting_times.append(end_time - start_time)\n",
    "        \n",
    "        \n",
    "    def plot(self):\n",
    "\n",
    "        interface_scores = [i[0] for i in self.norm_states]\n",
    "        total_scores     = [i[1] for i in self.norm_states]\n",
    "                                \n",
    "        # Creating the 2x2 plot\n",
    "        fig, axs = plt.subplots(2, 3, figsize=(10, 6))\n",
    "\n",
    "        # Reward and Loss\n",
    "        cumulative_rewards = np.cumsum(self.rewards[1:])\n",
    "        cumulative_loss    = np.cumsum(self.loss[1:])\n",
    "        cumulative_rewards /= np.amax(cumulative_rewards)\n",
    "        cumulative_loss    /= np.amax(cumulative_loss)\n",
    "        \n",
    "        axs[0, 0].plot(range(len(cumulative_loss)), cumulative_loss, label='Cumulative Loss')\n",
    "        axs[0, 0].plot(range(len(cumulative_rewards)), cumulative_rewards, label='Cumulative Rewards')\n",
    "        axs[0, 0].set_xlabel('Episodes')\n",
    "        axs[0, 0].set_ylabel('Reward or Loss')\n",
    "        axs[0, 0].set_title('Reward or Loss Over Episodes')\n",
    "        axs[0, 0].set_xlim(0, len(cumulative_loss))\n",
    "        axs[0, 0].set_ylim(0,1)\n",
    "        axs[0, 0].legend()\n",
    "\n",
    "        # All three scores vs. episodes\n",
    "        axs[0, 1].plot(range(len(total_scores)), total_scores, label='Total Score')\n",
    "        axs[0, 1].plot(range(len(interface_scores)), interface_scores, label='Interface Score')\n",
    "        axs[0, 1].set_xlabel('Episodes')\n",
    "        axs[0, 1].set_ylabel('Scores')\n",
    "        axs[0, 1].set_title('Scores Over Episodes')\n",
    "        axs[0, 1].set_xlim(0, len(total_scores))\n",
    "        axs[0, 1].set_ylim(ymin=0)\n",
    "        axs[0, 1].legend()\n",
    "\n",
    "        axs[0, 2].plot(range(len(self.fitting_times)), self.fitting_times)\n",
    "        axs[0, 2].set_title(\"Fitting Times\")\n",
    "        axs[0, 2].set_ylabel(\"Time (seconds)\")\n",
    "        axs[0, 2].set_xlim(0, len(self.fitting_times))\n",
    "        axs[0, 2].set_ylim(ymin=0)\n",
    "    \n",
    "        # Scatter plot of Interface Score vs. Total Score\n",
    "        scatter = axs[1, 0].scatter(total_scores, interface_scores, c=self.actions_taken, cmap='viridis')\n",
    "        lim = np.amax(total_scores + interface_scores)\n",
    "        axs[1, 0].set_xlim(0,lim)\n",
    "        axs[1, 0].set_ylim(0,lim)\n",
    "        axs[1, 0].plot(axs[1, 0].get_xlim(), axs[1, 0].get_ylim(), ls=\"--\", c=\".3\")\n",
    "        axs[1, 0].set_xlabel('Total Score')\n",
    "        axs[1, 0].set_ylabel('Interface Score')\n",
    "        axs[1, 0].set_title('Interface Score vs. Total Score')\n",
    "\n",
    "        # change of model over optimization\n",
    "        axs[1, 1].plot(range(len(self.RMSE_weights)), self.RMSE_weights)\n",
    "        axs[1, 1].set_xlim(0, len(self.RMSE_weights))\n",
    "        #axs[1, 1].set_yscale(\"log\")  \n",
    "        axs[1, 1].set_xlabel('Episodes')\n",
    "        axs[1, 1].set_ylabel('RMSE')\n",
    "        axs[1, 1].set_title('RMSE of Weights compared to initial model')\n",
    "\n",
    "        percent = float(sum(self.actions_taken))/len(self.actions_taken)\n",
    "        axs[1, 2].pie([1-percent,percent], labels=[\"0\",\"1\"], autopct='%.0f%%')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def save_model(self, file_name=\"./\"):\n",
    "        for i, (model, optimizer) in enumerate(zip(self.models, self.optimizers)):\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'{file_name}_model_{i}.pt')\n",
    "        data = {\n",
    "        'action_counts': self.action_counts,\n",
    "        'rewards': self.rewards,\n",
    "        'states': self.states,\n",
    "        'norm_rewards': self.norm_rewards,\n",
    "        'norm_states': self.norm_states,\n",
    "        'actions_taken': self.actions_taken,\n",
    "        'loss': self.loss,\n",
    "        'predecessors': self.predecessors,\n",
    "        'normalization': self.normalization,\n",
    "        'fitting_times': self.fitting_times\n",
    "        }\n",
    "        with open(f'{file_name}_agent_variables.pkl', 'wb') as f:\n",
    "            pickle.dump(data, f)    \n",
    "        print(f\"Saved model\")\n",
    "                \n",
    "    def load_model(self, file_name=\"./\"):\n",
    "        for i, (model, optimizer) in enumerate(zip(self.models, self.optimizers)):\n",
    "            file_path = f'{file_name}_model_{i}.pt'\n",
    "            if os.path.isfile(file_path):\n",
    "                checkpoint = torch.load(file_path)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            else:\n",
    "                print(f\"No saved model found\")\n",
    "                return\n",
    "        file_path = f'{file_name}_agent_variables.pkl'\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                self.action_counts = data['action_counts']\n",
    "                self.rewards = data['rewards']\n",
    "                self.states = data['states']\n",
    "                self.norm_rewards = data['norm_rewards']\n",
    "                self.norm_states = data['norm_states']\n",
    "                self.actions_taken = data['actions_taken']\n",
    "                self.loss = data['loss']\n",
    "                self.predecessors = data['predecessors']\n",
    "                self.normalization = data['normalization']\n",
    "                self.fitting_times = data['fitting_times']\n",
    "            \n",
    "    def delete_model(self, file_name=\"./\"):\n",
    "        for i in range(self.n_actions):\n",
    "            file_path = f'{file_name}_model_{i}.pt'\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        file_path = f'{file_name}_agent_variables.pkl'\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        print(f\"Deleted models\")\n",
    "        \n",
    "    def save_weights_biases(self, file_name=\"./\"):\n",
    "        torch.save(self.model.state_dict(), f'{file_name}model_weights.pth')\n",
    "        \n",
    "    def load_weights_biases(self, file_name=\"./\"):\n",
    "        if os.path.isfile(f'{file_name}model_weights.pth'):\n",
    "            self.model.load_state_dict(torch.load(f'{file_name}model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26286/3931308165.py:11: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  action = torch.tensor(row['action'], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0073\n",
      "Validation Loss: 0.9404\n",
      "Validation Loss: 0.9250\n",
      "Validation Loss: 0.9277\n",
      "Validation Loss: 0.9265\n",
      "Validation Loss: 0.9249\n",
      "Validation Loss: 0.9259\n",
      "Validation Loss: 0.9317\n",
      "Validation Loss: 0.9256\n",
      "Validation Loss: 0.9290\n",
      "Validation Loss: 0.9243\n",
      "Validation Loss: 0.9285\n",
      "Validation Loss: 0.9286\n",
      "Validation Loss: 0.9269\n",
      "Validation Loss: 0.9276\n",
      "Validation Loss: 0.9393\n",
      "Validation Loss: 0.9323\n",
      "Validation Loss: 0.9310\n",
      "Validation Loss: 0.9408\n",
      "Validation Loss: 0.9344\n",
      "Validation Loss: 0.9306\n",
      "Validation Loss: 0.9288\n",
      "Validation Loss: 0.9276\n",
      "Validation Loss: 0.9254\n",
      "Validation Loss: 0.9302\n",
      "Validation Loss: 0.9277\n",
      "Validation Loss: 0.9264\n",
      "Validation Loss: 0.9316\n",
      "Validation Loss: 0.9277\n",
      "Validation Loss: 0.9273\n",
      "Validation Loss: 0.9294\n"
     ]
    }
   ],
   "source": [
    "n_actions = 2\n",
    "n_features = 3\n",
    "agent = NeuralBanditAgent(n_actions, n_features, hidden_size=6, lr=0.05, epsilon_decay=0.995)\n",
    "agent.set_normalization_parameters(train_loader)\n",
    "\n",
    "num_epochs = 30\n",
    "evaluate_model(agent, val_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for states, actions, rewards in train_loader:\n",
    "        agent.update_model(states, actions, rewards)\n",
    "    evaluate_model(agent, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(agent, val_loader):\n",
    "    agent.model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # No need to track gradients for validation\n",
    "        for states, actions, rewards in val_loader:\n",
    "            states, rewards = agent.normalize(states, rewards)\n",
    "            predicted_rewards = agent.model(states)\n",
    "            criterion = nn.MSELoss()\n",
    "            action_indices = actions.unsqueeze(1)  # Add an extra dimension to index predicted_rewards\n",
    "            predicted_rewards_for_actions = predicted_rewards.gather(1, action_indices).squeeze()\n",
    "            loss = criterion(predicted_rewards_for_actions, rewards)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    print(f'Validation Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Call the evaluate function with your model and the validation DataLoader\n",
    "#evaluate_model(agent, val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
