{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba281b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import statistics\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "#import mdtraj as md\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB import PDBParser\n",
    "from datetime import datetime\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from scipy.stats import gmean, pearsonr\n",
    "import glob\n",
    "\n",
    "# Setting initial options\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None \n",
    "matplotlib_axes_logger.setLevel('INFO')\n",
    "\n",
    "DESIGN_COUNT = {}\n",
    "\n",
    "if GRID:\n",
    "    USERNAME = os.getlogin()\n",
    "#     FOLDER_HOME = f'{os.getcwd()}/{DESIGN_FOLDER}'\n",
    "# if BLUEPEBBLE:\n",
    "#     FOLDER_HOME = f'{os.getcwd()}/{DESIGN_FOLDER}'\n",
    "# if BACKGROUND_JOB:\n",
    "#     FOLDER_HOME = f'{os.getcwd()}/{DESIGN_FOLDER}'\n",
    "\n",
    "FOLDER_HOME = f'{os.getcwd()}/{DESIGN_FOLDER}'\n",
    "\n",
    "os.makedirs(FOLDER_HOME, exist_ok=True)\n",
    "FOLDER_INPUT = f'{os.getcwd()}/Input'\n",
    "if not os.path.isdir(FOLDER_INPUT): print(\"ERROR! Input folder missing!\")\n",
    "LOG_FILE = f'{FOLDER_HOME}.log'\n",
    "ALL_SCORES_CSV = f'{FOLDER_HOME}/all_scores.csv'\n",
    "VARIABLES_JSON  = f'{FOLDER_HOME}/variables.json'\n",
    "\n",
    "# Configure logging file\n",
    "log_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "# Remove all handlers associated with the root logger\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Basic configuration for logging to a file\n",
    "logging.basicConfig(filename=LOG_FILE, level=logging.DEBUG, format=log_format, datefmt=date_format)\n",
    "#logging.basicConfig(filename=LOG_FILE, level=logging.INFO, format=log_format, datefmt=date_format)\n",
    "\n",
    "# Create a StreamHandler for console output\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(logging.Formatter(log_format, datefmt=date_format))\n",
    "\n",
    "# Add the console handler to the root logger\n",
    "logging.getLogger().addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200550d5",
   "metadata": {},
   "source": [
    "# main functions - running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e896b53",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFOLDER_HOME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/n_running_jobs.dat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m): jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m jobs\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_potential\u001b[39m(score_type:\u001b[38;5;28mstr\u001b[39m, score:\u001b[38;5;28mfloat\u001b[39m, index:\u001b[38;5;28mint\u001b[39m, all_scores_df:\u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame): \u001b[38;5;66;03m#, updating_parent:bool=False):\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Creates a <score_type>_potential.dat file in FOLDER_HOME/<index> \u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    If score comes from Rosetta Relax - then the score for variant <index>\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    will be added to the <score_type>_potential.dat of the parent index \u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    and the <score_type>_potential value of the dataframe for the parent \u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    will be updated with the average of the parent and child scores. Returns \u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    updated dataframe'''\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Make potential_XXX.dat file\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def controller(RESET=False, EXPLORE=False, UNBLOCK_ALL=False, \n",
    "               PRINT_VAR=True, PLOT_DATA=True, \n",
    "               BLUEPEBBLE=False, GRID=True):\n",
    "    \n",
    "    # Main AI.zymes functions. Controls the whole design process\n",
    "\n",
    "    # Startup, will only be executed once in the beginning\n",
    "    setup_aizymes(RESET, EXPLORE) \n",
    "    \n",
    "    # Check if Startup is done, if done, read in all_scores_df\n",
    "    all_scores_df = startup_controller(UNBLOCK_ALL, \n",
    "                                       RESET,\n",
    "                                       PRINT_VAR=PRINT_VAR, \n",
    "                                       PLOT_DATA=PLOT_DATA)\n",
    "    \n",
    "    while not os.path.exists(os.path.join(FOLDER_HOME, str(MAX_DESIGNS))):\n",
    "\n",
    "        # Check how many jobs are currently running\n",
    "        # **\n",
    "        num_running_jobs = check_running_jobs()\n",
    "        \n",
    "        if num_running_jobs >= MAX_JOBS: \n",
    "            \n",
    "            time.sleep(60)\n",
    "            \n",
    "        else:\n",
    "                        \n",
    "            # Update scores\n",
    "            all_scores_df = update_scores(all_scores_df)\n",
    "            \n",
    "            # Check if parent designs are done, if not, start design\n",
    "            parent_done, all_scores_df = start_parent_design(all_scores_df)\n",
    "\n",
    "            if parent_done:\n",
    "                \n",
    "                # Boltzmann Selection\n",
    "                selected_index = boltzmann_selection(all_scores_df)\n",
    "\n",
    "                # Decide Fate of selected index\n",
    "                if selected_index is not None:\n",
    "                    all_scores_df = start_calculation(all_scores_df, selected_index)\n",
    "    \n",
    "        time.sleep(1)\n",
    "        \n",
    "    print(f\"Stopped because {os.path.join(FOLDER_HOME, str(MAX_DESIGNS))} exists.\")\n",
    "\n",
    "def check_running_jobs():\n",
    "    \n",
    "    if GRID:\n",
    "        jobs = subprocess.check_output([\"qstat\", \"-u\", USERNAME]).decode(\"utf-8\").split(\"\\n\")\n",
    "        jobs = [job for job in jobs if SUBMIT_PREFIX in job]\n",
    "        return len(jobs)\n",
    "        \n",
    "    if BLUEPEBBLE:\n",
    "        jobs = subprocess.check_output([\"squeue\",\"--me\"]).decode(\"utf-8\").split(\"\\n\")\n",
    "        jobs = [job for job in jobs if SUBMIT_PREFIX in job]\n",
    "        return len(jobs)\n",
    "        \n",
    "    if BACKGROUND_JOB:\n",
    "        with open(f'{FOLDER_HOME}/n_running_jobs.dat', 'r'): jobs = int(f.read())\n",
    "        return jobs\n",
    "    \n",
    "    if ABBIE_LOCAL:\n",
    "        return 0\n",
    "\n",
    "def update_potential(score_type:str, index:int, all_scores_df:pd.DataFrame):\n",
    "    '''Creates a <score_type>_potential.dat file in FOLDER_HOME/<index> \n",
    "    If latest score comes from Rosetta Relax - then the score for variant <index>\n",
    "    will be added to the <score_type>_potential.dat of the parent index \n",
    "    and the <score_type>_potential value of the dataframe for the parent \n",
    "    will be updated with the average of the parent and child scores. Returns \n",
    "    updated dataframe'''\n",
    "    \n",
    "    score = all_scores_df.at[index, f'{score_type}_score']\n",
    "\n",
    "    score_taken_from = all_scores_df.at[index, 'score_taken_from']\n",
    "\n",
    "    parent_index = int(all_scores_df.at[index, \"parent_index\"]) #parent index is stored as a string because can be \"Parent\"\n",
    "\n",
    "    filename = f\"{FOLDER_HOME}/{index}/{score_type}_potential.dat\"\n",
    "\n",
    "    parent_filename = f\"{FOLDER_HOME}/{parent_index}/{score_type}_potential.dat\"\n",
    "\n",
    "    ## overwrites contents of filename - always overwrite the current index - only append for parent\n",
    "    with open(filename, \"w\") as f: \n",
    "            f.write(str(score))\n",
    "\n",
    "    all_scores_df.at[index, f'{score_type}_potential'] = score\n",
    "\n",
    "    if score_taken_from == \"Relax\" and parent_index != \"Parent\":\n",
    "        #Add new index scores to parent potential file (unless parent index is \"Parent\")\n",
    "\n",
    "        ## appends to parent_filename\n",
    "        with open(parent_filename, \"a\") as f: \n",
    "            f.write(f\"\\n{str(score)}\")\n",
    "        with open(parent_filename, \"r\") as f:\n",
    "            potentials = f.readlines()\n",
    "        \n",
    "        all_scores_df.at[parent_index, f'{score_type}_potential'] = np.average([float(i) for i in potentials])\n",
    "\n",
    "    all_scores_df = all_scores_df.dropna(subset=['index'])\n",
    "    \n",
    "    return all_scores_df\n",
    "        \n",
    "def update_scores(all_scores_df):\n",
    "    \n",
    "    for _, row in all_scores_df.iterrows():\n",
    "\n",
    "        index = int(row['index'])\n",
    "        parent_index = row['parent_index']\n",
    "          \n",
    "        # do NOT update score if score was taken from a relax file. Prevents repeated scoring!\n",
    "        if row['score_taken_from'] == 'Relax': continue\n",
    "            \n",
    "            \n",
    "        # default scorefile path\n",
    "        score_file_path = f\"{FOLDER_HOME}/{int(index)}/score_rosetta_relax.sc\"\n",
    "        \n",
    "        # default pdb file path\n",
    "        if row[\"design_method\"] == \"ProteinMPNN\":\n",
    "            pdb_path = f\"{FOLDER_HOME}/{int(index)}/{WT}_Rosetta_Relax_{int(index)}.pdb\"\n",
    "        else:\n",
    "            pdb_path = f\"{FOLDER_HOME}/{int(index)}/{WT}_Rosetta_Design_{int(index)}.pdb\"\n",
    "        \n",
    "        if not os.path.exists(score_file_path):\n",
    "\n",
    "            # change scorefile path if run is a RosettaDesign and if score_rosetta_relax.sc does not exist\n",
    "            if row['design_method'] == \"RosettaDesign\":\n",
    "                score_file_path = f\"{FOLDER_HOME}/{int(index)}/score_rosetta_design.sc\" \n",
    "\n",
    "                # do NOT update score if score was taken from a design file. Prevents repeated scoring!\n",
    "                if row['score_taken_from'] == 'Design': continue\n",
    "\n",
    "        # do NOT update score if score_file_path does not exist. Usually means job is not done.\n",
    "        if not os.access(score_file_path, os.R_OK) or not os.access(pdb_path, os.R_OK): continue\n",
    "\n",
    "        #all_scores_df['score_taken_from'] = all_scores_df['score_taken_from'].astype('object')    \n",
    "        if score_file_path == f\"{FOLDER_HOME}/{int(index)}/score_rosetta_relax.sc\":\n",
    "            all_scores_df.at[index, 'score_taken_from'] = 'Relax'\n",
    "        if score_file_path == f\"{FOLDER_HOME}/{int(index)}/score_rosetta_design.sc\":\n",
    "            all_scores_df.at[index, 'score_taken_from'] = 'Design'\n",
    "           \n",
    "        with open(score_file_path, \"r\") as f: scores = f.readlines()\n",
    "        \n",
    "        if len(scores) < 3: continue # if the timing is bad, the score file is not fully written. Check if len(scores) > 2!\n",
    "        \n",
    "        headers = scores[1].split()\n",
    "        scores  = scores[2].split()\n",
    "\n",
    "        catalytic_score = 0.0\n",
    "        interface_score = 0.0\n",
    "        for idx_headers, header in enumerate(headers):\n",
    "            if header == 'total_score':                total_score      = float(scores[idx_headers])\n",
    "            if header == 'interface_delta_X':          interface_score += float(scores[idx_headers])\n",
    "            if header in ['if_X_angle_constraint', \n",
    "                          'if_X_atom_pair_constraint', \n",
    "                          'if_X_dihedral_constraint']: interface_score -= float(scores[idx_headers])   \n",
    "            #Use 6-3-2 weighting when calculating the catalytic score\n",
    "            if header in ['atom_pair_constraint']:     catalytic_score += float(scores[idx_headers])       \n",
    "            if header in ['angle_constraint']:         catalytic_score += float(scores[idx_headers])       \n",
    "            if header in ['dihedral_constraint']:      catalytic_score += float(scores[idx_headers])  \n",
    "\n",
    "        efield_score, efields_df = calc_efields_score(pdb_path)    #efields_df \n",
    "\n",
    "        #update_efieldsdf(index, efields_df)              \n",
    "\n",
    "        # Update scores\n",
    "        all_scores_df.at[index, 'total_score']     = total_score\n",
    "        all_scores_df.at[index, 'interface_score'] = interface_score                \n",
    "        all_scores_df.at[index, 'catalytic_score'] = catalytic_score\n",
    "        all_scores_df.at[index, 'efield_score'] = efield_score\n",
    "        \n",
    "        # This is just for book keeping. AIzymes will always use the most up_to_date scores saved above\n",
    "        if score_file_path == f\"{FOLDER_HOME}/{int(index)}/score_rosetta_relax.sc\":\n",
    "            all_scores_df.at[index, 'relax_total_score']     = total_score\n",
    "            all_scores_df.at[index, 'relax_interface_score'] = interface_score                \n",
    "            all_scores_df.at[index, 'relax_catalytic_score'] = catalytic_score\n",
    "            all_scores_df.at[index, 'relax_efield_score'] = efield_score\n",
    "        if score_file_path == f\"{FOLDER_HOME}/{int(index)}/score_rosetta_design.sc\":\n",
    "            all_scores_df.at[index, 'design_total_score']     = total_score\n",
    "            all_scores_df.at[index, 'design_interface_score'] = interface_score                \n",
    "            all_scores_df.at[index, 'design_catalytic_score'] = catalytic_score\n",
    "            all_scores_df.at[index, 'design_efield_score'] = efield_score\n",
    "        \n",
    "        for score_type in ['total', 'interface', 'catalytic', 'efield']:     \n",
    "\n",
    "            all_scores_df = update_potential(score_type = score_type,\n",
    "                                             index= index, \n",
    "                                             all_scores_df = all_scores_df,)   \n",
    "\n",
    "        logging.info(f\"Updated scores and potentials of index {int(index)}.\")\n",
    "        if all_scores_df.at[index, 'score_taken_from'] == 'Relax':\n",
    "            logging.info(f\"Adjusted potentials of parent of index {int(index)}.\")\n",
    "        \n",
    "        #unblock index if relaxed file exists\n",
    "        if all_scores_df.at[int(index), \"blocked\"] == True:\n",
    "            if f\"{WT}_Rosetta_Relax_{int(index)}.pdb\" in os.listdir(os.path.join(FOLDER_HOME, str(int(index)))):\n",
    "                all_scores_df.at[index, \"blocked\"] = False\n",
    "                logging.debug(f\"Unblocked index {int(index)}.\")\n",
    "\n",
    "     \n",
    "        # Update catalytic residues\n",
    "        if not row[\"design_method\"] == \"ProteinMPNN\":\n",
    "            all_scores_df = save_cat_res_into_all_scores_df(all_scores_df, index, pdb_path)\n",
    "\n",
    "        # Update sequence and mutations\n",
    "        reference_sequence = extract_sequence_from_pdb(f\"{FOLDER_INPUT}/{WT}.pdb\")\n",
    "        current_sequence = extract_sequence_from_pdb(pdb_path)\n",
    "        mutations = sum(1 for a, b in zip(current_sequence, reference_sequence) if a != b)\n",
    "        all_scores_df['sequence'] = all_scores_df['sequence'].astype('object')\n",
    "        all_scores_df.at[index, 'sequence']  = current_sequence\n",
    "        all_scores_df.at[index, 'mutations'] = int(mutations)\n",
    "        \n",
    "    save_all_scores_df(all_scores_df)\n",
    "\n",
    "    return all_scores_df\n",
    "\n",
    "def normalize_scores(unblocked_all_scores_df, print_norm=False, norm_all=False, extension=\"score\"):\n",
    "    \n",
    "    def neg_norm_array(array, score_type):\n",
    "\n",
    "        if len(array) > 1:  ##check that it's not only one value\n",
    "            \n",
    "            array    = -array\n",
    "            \n",
    "            if norm_all:\n",
    "                if print_norm:\n",
    "                    print(score_type,NORM[score_type],end=\" \")\n",
    "                array = (array-NORM[score_type][0])/(NORM[score_type][1]-NORM[score_type][0])\n",
    "                array[array < 0] = 0.0\n",
    "                if np.any(array > 1.0): print(\"\\nNORMALIZATION ERROR!\",score_type,\"has a value >1!\") \n",
    "            else:\n",
    "                if print_norm:\n",
    "                    print(score_type,[np.nanmin(array),np.nanmax(array)],end=\" \")\n",
    "                array = (array-np.nanmin(array))/(np.nanmax(array)-np.nanmin(array))\n",
    "                array[array < 0] = 0.0\n",
    "            return array\n",
    "        \n",
    "        else:\n",
    "            # do not normalize if array only contains 1 value\n",
    "            return [1]\n",
    "         \n",
    "    catalytic_scores    = unblocked_all_scores_df[f\"catalytic_{extension}\"]\n",
    "    catalytic_scores    = neg_norm_array(catalytic_scores, f\"catalytic_{extension}\")   \n",
    "    \n",
    "    total_scores        = unblocked_all_scores_df[f\"total_{extension}\"]\n",
    "    total_scores        = neg_norm_array(total_scores, f\"total_{extension}\")   \n",
    "    \n",
    "    interface_scores    = unblocked_all_scores_df[f\"interface_{extension}\"]\n",
    "    interface_scores    = neg_norm_array(interface_scores, f\"interface_{extension}\")  \n",
    "    \n",
    "    efield_scores    = unblocked_all_scores_df[f\"efield_{extension}\"]   ### to be worked on\n",
    "    efield_scores    = neg_norm_array(-1*efield_scores, f\"efield_{extension}\")   ### to be worked on, with MINUS here\n",
    "    \n",
    "    if len(total_scores) == 0:\n",
    "        combined_scores = []\n",
    "    else:\n",
    "        combined_scores     = np.stack((catalytic_scores, total_scores, interface_scores, efield_scores))  ### add electrostatic score here  ###Adjust nromalization\n",
    "        combined_scores     = gmean(combined_scores, axis=0)\n",
    "          \n",
    "    if print_norm:\n",
    "        if combined_scores != []:\n",
    "            print(\"HIGHSCORE:\",\"{:.2f}\".format(np.amax(combined_scores)),end=\" \")\n",
    "            print(\"Designs:\",len(combined_scores),end=\" \")\n",
    "            PARENTS = [i for i in os.listdir(f'{FOLDER_HOME}/{FOLDER_PARENT}') if i[-4:] == \".pdb\"]\n",
    "            print(\"Parents:\",len(PARENTS))\n",
    "        \n",
    "    return catalytic_scores, total_scores, interface_scores, combined_scores\n",
    "        \n",
    "def boltzmann_selection(all_scores_df):\n",
    "\n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "    all_scores_df = all_scores_df[all_scores_df[\"blocked\"] == False] # Remove blocked indices\n",
    "       \n",
    "    all_scores_df = all_scores_df.dropna(subset=['total_score'])     # Remove indices without score (design running)\n",
    "    \n",
    "    # AB FORGOT WHAT THIS IS DOING BUT ITS PROBABLY IMPORTANT! CHECK WHERE filtered_indices is used in original code\n",
    "    # relaxed_indices = all_scores_df[all_scores_df['score_taken_from'] == 'Relax']\n",
    "    # relaxed_indices = [str(i) for i in relaxed_indices.index]\n",
    "    # parent_indices  = set(all_scores_df['parent_index'].values)\n",
    "    # filtered_indices = [index for index in relaxed_indices  if index not in parent_indices]\n",
    "    # filtered_indices = [index for index in filtered_indices if index not in blocked_df]\n",
    "\n",
    "    # If there are structures that ran through RosettaRelax but have never been used for design, run 1 design\n",
    "    # NOT NEEDED! THIS IS BEEING TAKEN CARE OF IN start_parent_design\n",
    "    # relaxed_all_scores_df = unblocked_all_scores_df[unblocked_all_scores_df['score_taken_from'] == 'Relax']\n",
    "    #if len(relaxed_all_scores_df[\"index\"]) >= 1:\n",
    "    #    selected_index = int(relaxed_all_scores_df.index[0])\n",
    "    #    logging.info(f\"{selected_index} selected because its relaxed but nothing was designed from it.\")\n",
    "    #    return selected_index\n",
    "                \n",
    "    # Do Boltzmann Selection if some scores exist\n",
    "    _, _, _, combined_potentials = normalize_scores(all_scores_df, norm_all=False, \\\n",
    "                                                    extension=\"potential\", print_norm = False) \n",
    "        \n",
    "    if len(combined_potentials) > 0:\n",
    "        \n",
    "        if isinstance(KBT_BOLTZMANN, (float, int)):\n",
    "            kbt_boltzmann = KBT_BOLTZMANN\n",
    "        elif len(KBT_BOLTZMANN) > 2:\n",
    "            logging.error(f\"KBT_BOLTZMANN must either be a single value or list of two values.\")\n",
    "            logging.error(f\"KBT_BOLTZMANN is {KBT_BOLTZMANN}\")\n",
    "        else:\n",
    "            # Ramp down kbT_boltzmann over time (i.e., with increaseing indices)\n",
    "            kbt_boltzmann = KBT_BOLTZMANN[0] * 10**(-KBT_BOLTZMANN[1]*all_scores_df['index'].max())\n",
    "\n",
    "        boltzmann_factors = np.exp(combined_potentials / (kbt_boltzmann))  \n",
    "        probabilities = boltzmann_factors / sum(boltzmann_factors)        \n",
    "        \n",
    "        #selected_index = int(np.random.choice(np.array(all_scores_df[\"index\"].tolist()), p=probabilities))\n",
    "        if len(all_scores_df[\"index\"] > 0):\n",
    "            selected_index = int(np.random.choice(all_scores_df[\"index\"].to_numpy(), p=probabilities))\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        selected_index = 0\n",
    "\n",
    "    return selected_index\n",
    "\n",
    "def start_parent_design(all_scores_df):\n",
    "\n",
    "    number_of_indices = len(all_scores_df)\n",
    "    PARENTS = [i for i in os.listdir(f'{FOLDER_HOME}/{FOLDER_PARENT}') if i[-4:] == \".pdb\"]\n",
    "    \n",
    "    if number_of_indices < N_PARENT_JOBS * len(PARENTS):\n",
    "        \n",
    "        parent_done = False\n",
    "        selected_index = int(number_of_indices / N_PARENT_JOBS)\n",
    "        parent = PARENTS[selected_index][:-4]\n",
    "        \n",
    "        new_index, all_scores_df = create_new_index(parent_index=\"Parent\", all_scores_df=all_scores_df)\n",
    "        all_scores_df['design_method'] = all_scores_df['design_method'].astype('object') #?\n",
    "        all_scores_df.at[new_index, 'design_method'] = \"RosettaDesign\"\n",
    "        all_scores_df['luca'] = all_scores_df['luca'].astype('object') #?\n",
    "        all_scores_df.at[new_index, 'luca'] = parent\n",
    "        \n",
    "        #Add cat res to new entry\n",
    "        all_scores_df = save_cat_res_into_all_scores_df(all_scores_df, new_index, \n",
    "                                                        f'{FOLDER_HOME}/{FOLDER_PARENT}/{PARENTS[selected_index]}',\n",
    "                                                        from_parent_struct=True)\n",
    "        \n",
    "        run_RosettaDesign(parent_index=parent, new_index=new_index, all_scores_df=all_scores_df, parent_done=parent_done)                      \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        parent_done = True\n",
    "    \n",
    "    save_all_scores_df(all_scores_df)\n",
    "    return parent_done, all_scores_df\n",
    "\n",
    "# Decides what to do with selected index\n",
    "def start_calculation(all_scores_df, selected_index):\n",
    "    \n",
    "    logging.debug(f\"Starting new calculation for index {selected_index}.\")\n",
    "\n",
    "    blocked = False\n",
    "    if all_scores_df.at[selected_index, \"blocked\"] == True:\n",
    "        blocked = True\n",
    "        \n",
    "    relaxed = False\n",
    "    if f\"{WT}_Rosetta_Relax_{selected_index}.pdb\" in os.listdir(os.path.join(FOLDER_HOME, str(selected_index))):\n",
    "        relaxed = True\n",
    "\n",
    "    # Check if ESMfold_Rosetta_Relax is done\n",
    "    if relaxed:\n",
    "\n",
    "        # ESMfold_Rosetta_Relax is done, create a new index\n",
    "        new_index, all_scores_df = create_new_index(parent_index=selected_index, all_scores_df=all_scores_df)\n",
    "\n",
    "        #####\n",
    "        # Here, we can add an AI to decide on the next steps\n",
    "        #####\n",
    "\n",
    "        # Run Rosetta_Design with new_index\n",
    "        if random.random() < ProteinMPNN_PROB:  \n",
    "            all_scores_df.at[new_index, 'design_method'] = \"ProteinMPNN\"\n",
    "            run_ProteinMPNN(parent_index=selected_index, new_index=new_index, all_scores_df=all_scores_df) \n",
    "        else:                    \n",
    "            all_scores_df.at[new_index, 'design_method'] = \"RosettaDesign\"\n",
    "            run_RosettaDesign(parent_index=selected_index, new_index=new_index, all_scores_df=all_scores_df) \n",
    "        save_all_scores_df(all_scores_df)\n",
    "        \n",
    "    else:\n",
    "        # ESMfold_Rosetta_Relax is not done, check if Index is blocked\n",
    "        if blocked:\n",
    "            # Blocked --> ESMfold_Rosetta_Relax is still running, do not do antyting. This shouldn't happen!\n",
    "            logging.error(f\"Index {selected_index} is being worked on. Skipping index.\")\n",
    "            logging.error(f\"Note: This should not happen! Check blocking and Boltzman selection.\")\n",
    "        else:\n",
    "            # Not blocked --> submit ESMfold_Rosetta_Relax and block index\n",
    "            logging.info(f\"Index {selected_index} has no relaxed structure, starting ESMfold_Rosetta_Relax.\")\n",
    "            submitted_status = run_ESMfold_RosettaRelax(index=selected_index, all_scores_df=all_scores_df, \\\n",
    "                                                        OnlyRelax=True, EXPLORE=EXPLORE)\n",
    "            \n",
    "            #Check if submission executed correctly before blocking\n",
    "            if submitted_status:\n",
    "                all_scores_df.at[selected_index, \"blocked\"] = True\n",
    "\n",
    "        save_all_scores_df(all_scores_df)\n",
    "        \n",
    "    return all_scores_df\n",
    "\n",
    "def create_new_index(parent_index, all_scores_df):\n",
    "    \n",
    "    # Create a new line with the next index and parent_index\n",
    "    new_index = len(all_scores_df)\n",
    "    \n",
    "    # Append the new line to the DataFrame and save to  all_scores_df.csv\n",
    "    if isinstance(KBT_BOLTZMANN, (float, int)):\n",
    "        kbt_boltzmann = KBT_BOLTZMANN\n",
    "    else:\n",
    "        if len(KBT_BOLTZMANN) == 2:\n",
    "            kbt_boltzmann = KBT_BOLTZMANN[0] * 10 ** (- KBT_BOLTZMANN[1] * new_index)\n",
    "    if parent_index == 'Parent':\n",
    "        generation = 0\n",
    "        luca = \"x\"\n",
    "    else:\n",
    "        generation = all_scores_df['generation'][int(parent_index)]+1\n",
    "        luca       = all_scores_df['luca'][int(parent_index)]\n",
    "        \n",
    "    # all_scores_df = all_scores_df.append({'index': new_index, \n",
    "    #                                       'parent_index': parent_index,\n",
    "    #                                       'kbt_boltzmann': kbt_boltzmann,\n",
    "    #                                       'generation': generation,\n",
    "    #                                       'luca': luca,\n",
    "    #                                       'blocked': False,\n",
    "    #                                       }, ignore_index=True)\n",
    "    \n",
    "    # why do this in two steps?\n",
    "    new_index_df = pd.DataFrame({'index': int(new_index), \n",
    "                                'parent_index': parent_index,\n",
    "                                'kbt_boltzmann': kbt_boltzmann,\n",
    "                                'generation': generation,\n",
    "                                'luca': luca,\n",
    "                                'blocked': False,\n",
    "                                }, index = [0])\n",
    "    all_scores_df = pd.concat([all_scores_df, new_index_df], ignore_index=True)\n",
    "\n",
    "    save_all_scores_df(all_scores_df)\n",
    "\n",
    "    # Create the folders for the new index\n",
    "    os.makedirs(f\"{FOLDER_HOME}/{new_index}/scripts\", exist_ok=True)\n",
    "           \n",
    "    logging.debug(f\"Child index {new_index} created for {parent_index}.\")\n",
    "    \n",
    "    return new_index, all_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152761b",
   "metadata": {},
   "source": [
    "# main functions - design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5725702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ESMfold_RosettaRelax(index, all_scores_df, OnlyRelax=False, ProteinMPNN=False, PreMatchRelax=False,\n",
    "                             ProteinMPNN_parent_index=0, cmd=\"\", bash=False, EXPLORE=False):\n",
    "    \n",
    "    # Giving the ESMfold algorihm the needed inputs\n",
    "    output_file = f'{FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_output_{index}.pdb'\n",
    "    if ProteinMPNN:\n",
    "        sequence_file = f'{FOLDER_HOME}/{index}/ProteinMPNN/{WT}_{index}.seq'\n",
    "    else:\n",
    "        sequence_file = f'{FOLDER_HOME}/{index}/ESMfold/{WT}_{index}.seq'\n",
    "        \n",
    "    # Make directories\n",
    "    os.makedirs(f\"{FOLDER_HOME}/{index}/ESMfold\", exist_ok=True)\n",
    "    os.makedirs(f\"{FOLDER_HOME}/{index}/scripts\", exist_ok=True)\n",
    "        \n",
    "    # Options for EXPLORE, accelerated script for testing\n",
    "    ex = \"-ex1 -ex2\"\n",
    "    if EXPLORE: ex = \"\"\n",
    "        \n",
    "    # Get Name of parent PDB\n",
    "    if OnlyRelax: \n",
    "        PDBFile = f\"{FOLDER_HOME}/{index}/{WT}_Rosetta_Design_{index}.pdb\"\n",
    "    elif PreMatchRelax: \n",
    "        PDBFile = f\"{FOLDER_INPUT}/{WT}.pdb\"\n",
    "    elif ProteinMPNN:\n",
    "        PDBFile = f\"{FOLDER_HOME}/{ProteinMPNN_parent_index}/{WT}_Rosetta_Relax_{ProteinMPNN_parent_index}.pdb\"\n",
    "    else:\n",
    "        print(\"I don't know what you want me to do\")\n",
    "        return\n",
    "    if not os.path.isfile(PDBFile):\n",
    "        logging.error(f\"{PDBFile} not present!\")\n",
    "        return False\n",
    "\n",
    "    # Make sequence file\n",
    "    if OnlyRelax or PreMatchRelax: \n",
    "        seq = extract_sequence_from_pdb(PDBFile)\n",
    "        with open(f\"{FOLDER_HOME}/{index}/ESMfold/{WT}_{index}.seq\",\"w\") as f: f.write(seq)\n",
    "                    \n",
    "    # Get the pdb file from the last step and strip away ligand and hydrogens \n",
    "    cpptraj = f'''parm    {PDBFile}\n",
    "trajin  {PDBFile}\n",
    "strip   :{LIGAND}\n",
    "strip   !@C,N,O,CA\n",
    "trajout {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Apo_{index}.pdb\n",
    "'''\n",
    "    with open(f'{FOLDER_HOME}/{index}/ESMfold/CPPTraj_Apo_{index}.in','w') as f: f.write(cpptraj)\n",
    "\n",
    "    # Get the pdb file from the last step and strip away everything except the ligand\n",
    "    cpptraj = f'''parm    {PDBFile}\n",
    "trajin  {PDBFile}\n",
    "strip   !:{LIGAND}\n",
    "trajout {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Lig_{index}.pdb\n",
    "'''\n",
    "    with open(f'{FOLDER_HOME}/{index}/ESMfold/CPPTraj_Lig_{index}.in','w') as f: f.write(cpptraj)\n",
    "\n",
    "    # Get the ESMfold pdb file and strip away all hydrogens\n",
    "    cpptraj = f'''parm    {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_output_{index}.pdb\n",
    "trajin  {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_output_{index}.pdb\n",
    "strip   !@C,N,O,CA\n",
    "trajout {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_no_hydrogens_{index}.pdb\n",
    "'''\n",
    "    with open(f'{FOLDER_HOME}/{index}/ESMfold/CPPTraj_no_hydrogens_{index}.in','w') as f: f.write(cpptraj)\n",
    "\n",
    "    # Align substrate and ESM prediction of scaffold without hydrogens\n",
    "    cpptraj = f'''parm    {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_no_hydrogens_{index}.pdb\n",
    "reference {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Apo_{index}.pdb [apo]\n",
    "trajin    {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_no_hydrogens_{index}.pdb\n",
    "rmsd      @CA ref [apo]\n",
    "trajout   {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_aligned_{index}.pdb noter\n",
    "'''\n",
    "    with open(f'{FOLDER_HOME}/{index}/ESMfold/CPPTraj_aligned_{index}.in','w') as f: f.write(cpptraj) \n",
    "              \n",
    "    if GRID:           extension = \"linuxgccrelease\"\n",
    "    if BLUEPEBBLE:     extension = \"serialization.linuxgccrelease\"\n",
    "    if BACKGROUND_JOB: extension = \"serialization.linuxgccrelease\"\n",
    "    if ABBIE_LOCAL:    \n",
    "        extension = \"linuxgccrelease\"\n",
    "        bash_args = \"OMP_NUM_THREADS=1\"\n",
    "    else:\n",
    "        bash_args = \"\"\n",
    " \n",
    "    cmd += f\"\"\"\n",
    "    \n",
    "{bash_args} python {FOLDER_HOME}/ESMfold.py {output_file} {sequence_file}\n",
    "\n",
    "sed -i '/PARENT N\\/A/d' {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_output_{index}.pdb\n",
    "cpptraj -i {FOLDER_HOME}/{index}/ESMfold/CPPTraj_Apo_{index}.in           &> \\\n",
    "           {FOLDER_HOME}/{index}/ESMfold/CPPTraj_Apo_{index}.out\n",
    "cpptraj -i {FOLDER_HOME}/{index}/ESMfold/CPPTraj_Lig_{index}.in           &> \\\n",
    "           {FOLDER_HOME}/{index}/ESMfold/CPPTraj_Lig_{index}.out\n",
    "cpptraj -i {FOLDER_HOME}/{index}/ESMfold/CPPTraj_no_hydrogens_{index}.in  &> \\\n",
    "           {FOLDER_HOME}/{index}/ESMfold/CPPTraj_no_hydrogens_{index}.out\n",
    "cpptraj -i {FOLDER_HOME}/{index}/ESMfold/CPPTraj_aligned_{index}.in       &> \\\n",
    "           {FOLDER_HOME}/{index}/ESMfold/CPPTraj_aligned_{index}.out\n",
    "\n",
    "# Assemble the final protein\n",
    "sed -i '/END/d' {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_aligned_{index}.pdb\n",
    "# Return HETATM to ligand output and remove TER\n",
    "sed -i -e 's/^ATOM  /HETATM/' -e '/^TER/d' {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Lig_{index}.pdb\n",
    "\"\"\"\n",
    "    \n",
    "    input_extension_relax = \"\"\n",
    "    if PreMatchRelax:\n",
    "        extension_relax = \"_APO\"\n",
    "        ## No ligand necessary so just use the aligned pdb from ESMfold\n",
    "        cmd += f\"\"\"\n",
    "cp {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_aligned_{index}.pdb \\\n",
    "   {FOLDER_HOME}/{index}/{WT}_ESMfold_{index}{extension_relax}.pdb\n",
    "\"\"\"  \n",
    "\n",
    "    else:\n",
    "        extension_relax = \"\"\n",
    "        remark = generate_remark_from_all_scores_df(all_scores_df, index)\n",
    "        with open(f'{FOLDER_HOME}/{index}/{WT}_ESMfold_{index}.pdb', 'w') as f: f.write(remark+\"\\n\")\n",
    "        cmd += f\"\"\"\n",
    "cat {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_aligned_{index}.pdb >> {FOLDER_HOME}/{index}/{WT}_ESMfold_{index}.pdb\n",
    "cat {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Lig_{index}.pdb     >> {FOLDER_HOME}/{index}/{WT}_ESMfold_{index}.pdb\n",
    "sed -i '/TER/d' {FOLDER_HOME}/{index}/{WT}_ESMfold_{index}.pdb\n",
    "\"\"\"\n",
    "        \n",
    "    cmd += f\"\"\"\n",
    "# Run Rosetta Relax\n",
    "{ROSETTA_PATH}/bin/rosetta_scripts.{extension} \\\n",
    "                -s                                        {FOLDER_HOME}/{index}/{WT}_ESMfold_{index}{extension_relax}.pdb \\\n",
    "                -extra_res_fa                             {FOLDER_INPUT}/{LIGAND}.params \\\n",
    "                -parser:protocol                          {FOLDER_HOME}/{index}/scripts/Rosetta_Relax_{index}.xml \\\n",
    "                -out:file:scorefile                       {FOLDER_HOME}/{index}/score_rosetta_relax.sc \\\n",
    "                -nstruct                                  1 \\\n",
    "                -ignore_zero_occupancy                    false \\\n",
    "                -corrections::beta_nov16                  true \\\n",
    "                -run:preserve_header                      true \\\n",
    "                -overwrite {ex}\n",
    "\n",
    "# Rename the output file\n",
    "mv {WT}_ESMfold_{index}{extension_relax}_0001.pdb {WT}_Rosetta_Relax_{index}{extension_relax}.pdb\n",
    "sed -i '/        H  /d' {WT}_Rosetta_Relax_{index}{extension_relax}.pdb\n",
    "\"\"\"\n",
    "    \n",
    "    if PreMatchRelax:\n",
    "        extension_relax = \"_APO\"\n",
    "        \n",
    "        cmd += f\"\"\"\n",
    "# Align relaxed ESM prediction of scaffold without hydrogens\n",
    "cpptraj -i {FOLDER_HOME}/{index}/ESMfold/{WT}_Rosetta_Relax_aligned_{index}{extension_relax}.in           &> \\\n",
    "           {FOLDER_HOME}/{index}/ESMfold/{WT}_Rosetta_Relax_aligned_{index}{extension_relax}.out\n",
    "sed -i '/END/d' {FOLDER_HOME}/{index}/{WT}_Rosetta_Relax_aligned_{index}{extension_relax}.pdb\n",
    "\"\"\"  \n",
    "        \n",
    "        cpptraj = f'''parm    {FOLDER_HOME}/{index}/{WT}_Rosetta_Relax_{index}{extension_relax}.pdb [protein]\n",
    "parm      {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Apo_{index}.pdb [reference]\n",
    "reference {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Apo_{index}.pdb parm [reference] [apo]\n",
    "trajin    {FOLDER_HOME}/{index}/{WT}_Rosetta_Relax_{index}{extension_relax}.pdb parm [protein]\n",
    "rmsd      @CA ref [apo]\n",
    "trajout   {FOLDER_HOME}/{index}/{WT}_Rosetta_Relax_aligned_{index}{extension_relax}.pdb noter\n",
    "'''\n",
    "        with open(f'{FOLDER_HOME}/{index}/ESMfold/{WT}_Rosetta_Relax_aligned_{index}{extension_relax}.in','w') as f: \n",
    "            f.write(cpptraj) \n",
    "    \n",
    "        # Create the Rosetta_Relax.xml file\n",
    "    repeats = \"3\"\n",
    "    if EXPLORE: repeats = \"1\"\n",
    "    Rosetta_Relax_xml = f\"\"\"\n",
    "<ROSETTASCRIPTS>\n",
    "\n",
    "    <SCOREFXNS>\n",
    "    \n",
    "        <ScoreFunction name      = \"score\"                   weights = \"beta_nov16\" >\n",
    "            <Reweight scoretype  = \"atom_pair_constraint\"    weight  = \"4\" />\n",
    "            <Reweight scoretype  = \"angle_constraint\"        weight  = \"2\" />\n",
    "            <Reweight scoretype  = \"dihedral_constraint\"     weight  = \"1\" />\n",
    "        </ScoreFunction> \n",
    "        \n",
    "        <ScoreFunction name      = \"score_final\"             weights = \"beta_nov16\" >\n",
    "            <Reweight scoretype  = \"atom_pair_constraint\"    weight  = \"4\" />\n",
    "            <Reweight scoretype  = \"angle_constraint\"        weight  = \"2\" />\n",
    "            <Reweight scoretype  = \"dihedral_constraint\"     weight  = \"1\" />\n",
    "        </ScoreFunction>\n",
    "        \n",
    "    </SCOREFXNS>\n",
    "       \n",
    "    <MOVERS>\n",
    "                                  \n",
    "        <FastRelax  name=\"mv_relax\" disable_design=\"false\" repeats=\"{repeats}\" /> \n",
    "\"\"\"\n",
    "    if not PreMatchRelax: Rosetta_Relax_xml += f\"\"\"\n",
    "        <AddOrRemoveMatchCsts     name=\"mv_add_cst\" \n",
    "                                  cst_instruction=\"add_new\" \n",
    "                                  cstfile=\"{FOLDER_INPUT}/{LIGAND}_enzdes.cst\" />\n",
    "\n",
    "\"\"\"\n",
    "    Rosetta_Relax_xml += f\"\"\"\n",
    "\n",
    "        <InterfaceScoreCalculator   name                   = \"mv_inter\" \n",
    "                                    chains                 = \"X\" \n",
    "                                    scorefxn               = \"score_final\" />\n",
    "    </MOVERS>\n",
    "    \n",
    "    <PROTOCOLS>  \n",
    "\n",
    "        <Add mover_name=\"mv_relax\" />\n",
    "\"\"\"\n",
    "    if not PreMatchRelax: Rosetta_Relax_xml += f\"\"\"                                  \n",
    "        <Add mover_name=\"mv_add_cst\" />       \n",
    "        <Add mover_name=\"mv_inter\" />\n",
    "\"\"\"\n",
    "    Rosetta_Relax_xml += f\"\"\"\n",
    "    </PROTOCOLS>\n",
    "    \n",
    "</ROSETTASCRIPTS>\n",
    "\"\"\"\n",
    "    # Write the Rosetta_Relax.xml to a file\n",
    "    with open(f'{FOLDER_HOME}/{index}/scripts/Rosetta_Relax_{index}.xml', 'w') as f:\n",
    "        f.writelines(Rosetta_Relax_xml)      \n",
    "        \n",
    "    if OnlyRelax or PreMatchRelax: \n",
    "        with open(f'{FOLDER_HOME}/{index}/scripts/ESMfold_Rosetta_Relax_{index}.sh', 'w') as file:\n",
    "            file.write(cmd)\n",
    "        logging.info(f\"Run ESMfold & Rosetta_Relax for index {index}.\")\n",
    "        submit_job(index=index, job=\"ESMfold_Rosetta_Relax\", bash=bash)\n",
    "        \n",
    "    if ProteinMPNN:\n",
    "        with open(f'{FOLDER_HOME}/{index}/scripts/ProteinMPNN_ESMfold_Rosetta_Relax_{index}.sh', 'w') as file:\n",
    "            file.write(cmd)\n",
    "        logging.info(f\"Run ProteinMPNN for index {index} based on index {ProteinMPNN_parent_index}.\")\n",
    "        submit_job(index=index, job=\"ProteinMPNN_ESMfold_Rosetta_Relax\", bash=bash)\n",
    "\n",
    "    return True\n",
    "        \n",
    "def run_RosettaDesign(parent_index, new_index, all_scores_df, parent_done=True):\n",
    "\n",
    "    # Options for EXPLORE, accelerated script for testing\n",
    "    ex = \"-ex1 -ex2\"\n",
    "    if EXPLORE: ex = \"\"\n",
    "\n",
    "    if GRID:           extension = \"linuxgccrelease\"\n",
    "    if BLUEPEBBLE:     extension = \"serialization.linuxgccrelease\"\n",
    "    if BACKGROUND_JOB: extension = \"serialization.linuxgccrelease\"\n",
    "\n",
    "    if ABBIE_LOCAL:    extension = \"linuxgccrelease\"\n",
    "\n",
    "    if parent_done:\n",
    "        PDB_input  = f'{FOLDER_HOME}/{parent_index}/{WT}_Rosetta_Relax_{parent_index}.pdb'\n",
    "        PDB_output = f'{WT}_Rosetta_Relax_{parent_index}_0001.pdb'\n",
    "    else:\n",
    "        PDB_input  = f'{FOLDER_HOME}/{FOLDER_PARENT}/{parent_index}.pdb'\n",
    "        PDB_output = f'{parent_index}_0001.pdb'\n",
    "        \n",
    "    all_scores_df = save_cat_res_into_all_scores_df(all_scores_df, new_index, PDB_input, from_parent_struct=True)\n",
    "    \n",
    "    cmd = f\"\"\"{ROSETTA_PATH}/bin/rosetta_scripts.{extension}\\\n",
    "    -s                                        {PDB_input} \\\n",
    "    -in:file:native                           {PDB_input} \\\n",
    "    -run:preserve_header                      true \\\n",
    "    -extra_res_fa                             {FOLDER_INPUT}/{LIGAND}.params \\\n",
    "    -parser:protocol                          {FOLDER_HOME}/{new_index}/scripts/Rosetta_Design_{new_index}.xml \\\n",
    "    -out:file:scorefile                       {FOLDER_HOME}/{new_index}/score_rosetta_design.sc \\\n",
    "    -nstruct                                  1  \\\n",
    "    -ignore_zero_occupancy                    false  \\\n",
    "    -corrections::beta_nov16                  true \\\n",
    "    -overwrite {ex}\n",
    "    \n",
    "mv {PDB_output} {WT}_Rosetta_Design_{new_index}.pdb \n",
    "\"\"\"\n",
    "    # Write the shell command to a file\n",
    "    with open(f'{FOLDER_HOME}/{new_index}/scripts/Rosetta_Design_{new_index}.sh','w') as file: file.write(cmd)\n",
    "                \n",
    "    # Create XML script for Rosetta Design  \n",
    "    repeats = \"3\"\n",
    "    if EXPLORE: repeats = \"1\"\n",
    "        \n",
    "    Rosetta_Design_xml = f\"\"\"\n",
    "<ROSETTASCRIPTS>\n",
    "\n",
    "    <SCOREFXNS>\n",
    "\n",
    "        <ScoreFunction            name=\"score\"                           weights=\"beta_nov16\" >  \n",
    "            <Reweight             scoretype=\"atom_pair_constraint\"       weight=\"4\" />\n",
    "            <Reweight             scoretype=\"angle_constraint\"           weight=\"2\" />    \n",
    "            <Reweight             scoretype=\"dihedral_constraint\"        weight=\"1\" />        \n",
    "            <Reweight             scoretype=\"res_type_constraint\"        weight=\"1\" />              \n",
    "        </ScoreFunction>\n",
    "       \n",
    "        <ScoreFunction            name=\"score_unconst\"                   weights=\"beta_nov16\" >        \n",
    "            <Reweight             scoretype=\"atom_pair_constraint\"       weight=\"0\" />\n",
    "            <Reweight             scoretype=\"dihedral_constraint\"        weight=\"0\" />\n",
    "            <Reweight             scoretype=\"angle_constraint\"           weight=\"0\" />              \n",
    "        </ScoreFunction>\n",
    "\n",
    "        <ScoreFunction            name=\"score_final\"                     weights=\"beta_nov16\" >    \n",
    "            <Reweight             scoretype=\"atom_pair_constraint\"       weight=\"4\" />\n",
    "            <Reweight             scoretype=\"angle_constraint\"           weight=\"2\" />    \n",
    "            <Reweight             scoretype=\"dihedral_constraint\"        weight=\"1\" />               \n",
    "        </ScoreFunction>\n",
    "   \n",
    "   </SCOREFXNS>\n",
    "   \n",
    "    <RESIDUE_SELECTORS>\n",
    "   \n",
    "        <Index                    name=\"sel_design\"\n",
    "                                  resnums=\"{DESIGN}\" />\n",
    "\n",
    "        <Index                    name=\"sel_repack\"\n",
    "                                  resnums=\"{REPACK}\" />\n",
    "\"\"\"\n",
    "    \n",
    "    # Add residue number constraints from REMARK (via all_scores_df['cat_resi'])\n",
    "    cat_resis = all_scores_df.at[new_index, 'cat_resi'].split(';')\n",
    "    for idx, cat_resi in enumerate(cat_resis): \n",
    "        \n",
    "        Rosetta_Design_xml += f\"\"\"\n",
    "        <Index                    name=\"sel_cat_{idx}\"\n",
    "                                  resnums=\"{int(cat_resi)}\" />\n",
    "\"\"\"\n",
    "        \n",
    "    Rosetta_Design_xml += f\"\"\"\n",
    "        <Or                       name=\"sel_desrep\"\n",
    "                                  selectors=\"sel_design,sel_repack\" />\n",
    "\n",
    "        <Not                      name=\"sel_nothing\"\n",
    "                                  selector=\"sel_desrep\" />\n",
    "    </RESIDUE_SELECTORS>\n",
    "   \n",
    "    <TASKOPERATIONS>\n",
    "   \n",
    "        <OperateOnResidueSubset   name=\"tsk_design\"                      selector=\"sel_design\" >\n",
    "                                  <RestrictAbsentCanonicalAASRLT         aas=\"GPAVLIMFYWHKRQNEDST\" />\n",
    "        </OperateOnResidueSubset>\n",
    "\"\"\"\n",
    "    \n",
    "    # Add residue identity constraints from constraint file\n",
    "    with open(f'{FOLDER_HOME}/cst.dat', 'r') as f: cat_resns = f.read()    \n",
    "    cat_resns = cat_resns.split(\";\")\n",
    "    \n",
    "    for idx, cat_resn in enumerate(cat_resns): \n",
    "        Rosetta_Design_xml += f\"\"\"\n",
    "        <OperateOnResidueSubset   name=\"tsk_cat_{idx}\"                   selector=\"sel_cat_{idx}\" >\n",
    "                                  <RestrictAbsentCanonicalAASRLT         aas=\"{cat_resn}\" />\n",
    "        </OperateOnResidueSubset>\n",
    "\"\"\"\n",
    "    \n",
    "    tsk_cat = []\n",
    "    for idx, cat_res in enumerate(cat_resns): \n",
    "        tsk_cat += [f\"tsk_cat_{idx}\"]\n",
    "    tsk_cat = \",\".join(tsk_cat)\n",
    "        \n",
    "    Rosetta_Design_xml += f\"\"\"\n",
    "       \n",
    "        <OperateOnResidueSubset   name=\"tsk_repack\"                      selector=\"sel_repack\" >\n",
    "                                  <RestrictToRepackingRLT />\n",
    "        </OperateOnResidueSubset>\n",
    "       \n",
    "        <OperateOnResidueSubset   name=\"tsk_nothing\"                     selector=\"sel_nothing\" >\n",
    "                                  <PreventRepackingRLT />\n",
    "        </OperateOnResidueSubset>\n",
    "       \n",
    "    </TASKOPERATIONS>\n",
    "\n",
    "    <FILTERS>\n",
    "   \n",
    "        <HbondsToResidue          name=\"flt_hbonds\"\n",
    "                                  scorefxn=\"score\"\n",
    "                                  partners=\"1\"\n",
    "                                  residue=\"1X\"\n",
    "                                  backbone=\"true\"\n",
    "                                  sidechain=\"true\"\n",
    "                                  from_other_chains=\"true\"\n",
    "                                  from_same_chain=\"false\"\n",
    "                                  confidence=\"0\" />\n",
    "    </FILTERS>\n",
    "   \n",
    "    <MOVERS>\n",
    "       \n",
    "        <FavorSequenceProfile     name=\"mv_native\"\n",
    "                                  weight=\"{CST_WEIGHT}\"\n",
    "                                  use_native=\"true\"\n",
    "                                  matrix=\"IDENTITY\"\n",
    "                                  scorefxns=\"score\" />  \n",
    "                               \n",
    "        <AddOrRemoveMatchCsts     name=\"mv_add_cst\"\n",
    "                                  cst_instruction=\"add_new\"\n",
    "                                  cstfile=\"{FOLDER_INPUT}/{LIGAND}_enzdes.cst\" />\n",
    "\n",
    "        <FastDesign               name                   = \"mv_design\"\n",
    "                                  disable_design         = \"false\"\n",
    "                                  task_operations        = \"tsk_design,tsk_repack,tsk_nothing,{tsk_cat}\"\n",
    "                                  repeats                = \"{repeats}\"\n",
    "                                  ramp_down_constraints  = \"false\"\n",
    "                                  scorefxn               = \"score\" />\n",
    "                                  \n",
    "        <FastRelax                name                   = \"mv_relax\"\n",
    "                                  disable_design         = \"true\"\n",
    "                                  task_operations        = \"tsk_design,tsk_repack,tsk_nothing,{tsk_cat}\"\n",
    "                                  repeats                = \"1\"\n",
    "                                  ramp_down_constraints  = \"false\"\n",
    "                                  scorefxn               = \"score_unconst\" />  \n",
    "                                  \n",
    "        <InterfaceScoreCalculator name                   = \"mv_inter\"\n",
    "                                  chains                 = \"X\"\n",
    "                                  scorefxn               = \"score_final\" />\n",
    "                                 \n",
    "    </MOVERS>\n",
    "\n",
    "    <PROTOCOLS>\n",
    "        <Add mover_name=\"mv_native\" />\n",
    "        <Add mover_name=\"mv_add_cst\" />\n",
    "        <Add mover_name=\"mv_design\" />\n",
    "        <Add mover_name=\"mv_relax\" />\n",
    "        <Add mover_name=\"mv_inter\" />\n",
    "    </PROTOCOLS>\n",
    "   \n",
    "</ROSETTASCRIPTS>\n",
    "\n",
    "\"\"\"\n",
    "    # Write the XML script to a file\n",
    "    with open(f'{FOLDER_HOME}/{new_index}/scripts/Rosetta_Design_{new_index}.xml', 'w') as f:\n",
    "        f.writelines(Rosetta_Design_xml)               \n",
    "        \n",
    "    # Submit the job using the submit_job function\n",
    "    logging.info(f\"Run RosettaDesign for index {new_index} based on index {parent_index}.\")\n",
    "    submit_job(index=new_index, job=\"Rosetta_Design\")\n",
    "\n",
    "\n",
    "\n",
    "def run_ProteinMPNN(parent_index, new_index, all_scores_df):\n",
    "    \"\"\"\n",
    "    Executes the ProteinMPNN pipeline for a given protein structure and generates\n",
    "    new protein sequences with potentially higher functional scores.\n",
    "\n",
    "    Parameters:\n",
    "    - parent_index (str): The index of the parent protein variant.\n",
    "    - new_index (str): The index assigned to the new protein variant.\n",
    "    - all_scores_df (DataFrame): A DataFrame containing information for protein variants.\n",
    "    - --------------------------------GLOBAL variables used ----------------------------\n",
    "    - FOLDER_HOME (str): The base directory where ProteinMPNN and related files are located.\n",
    "    - WT (str): The wild type or reference protein identifier.\n",
    "    - DESIGN (str): A string representing positions and types of amino acids to design with RosettaDesign.\n",
    "    - ProteinMPNN_T (float): The sampling temperature for ProteinMPNN.\n",
    "    - EXPLORE (bool): Flag to indicate whether exploration mode is enabled.\n",
    "\n",
    "    Returns:\n",
    "    None: The function primarily works by side effects, finally producing the highes scoring sequence \n",
    "    in the specified directories.\n",
    "    \n",
    "    Note:\n",
    "    This function assumes the ProteinMPNN toolkit is available and properly set up in the specified location.\n",
    "    It involves multiple subprocess calls to Python scripts for processing protein structures and generating new sequences.\n",
    "    \"\"\"\n",
    "    # Ensure ProteinMPNN is available\n",
    "    if not os.path.exists(f'{FOLDER_HOME}/../ProteinMPNN'):\n",
    "        logging.error(f\"ProteinMPNN not installed in {FOLDER_HOME}/../ProteinMPNN.\")\n",
    "        logging.error(\"Install using: git clone https://github.com/dauparas/ProteinMPNN.git\")\n",
    "        return\n",
    "\n",
    "    # Prepare file paths\n",
    "    pdb_file = f\"{FOLDER_HOME}/{parent_index}/{WT}_Rosetta_Relax_{parent_index}.pdb\"\n",
    "    if not os.path.isfile(pdb_file):\n",
    "        logging.error(f\"{pdb_file} not present!\")\n",
    "        return\n",
    "\n",
    "    protein_mpnn_folder = f\"{FOLDER_HOME}/{new_index}/ProteinMPNN\"\n",
    "    os.makedirs(protein_mpnn_folder, exist_ok=True)\n",
    "    shutil.copy(pdb_file, os.path.join(protein_mpnn_folder, f\"{WT}_Rosetta_Relax_{parent_index}.pdb\"))\n",
    "\n",
    "    seq = extract_sequence_from_pdb(pdb_file)\n",
    "    with open(os.path.join(protein_mpnn_folder, f\"Rosetta_Relax_{parent_index}.seq\"), \"w\") as f:\n",
    "        f.write(seq)\n",
    "\n",
    "    # Run ProteinMPNN steps using subprocess\n",
    "    helper_scripts_path = f\"{FOLDER_HOME}/../ProteinMPNN/helper_scripts\"\n",
    "    protein_mpnn_path = f\"{FOLDER_HOME}/../ProteinMPNN\"\n",
    "\n",
    "    # Parse multiple chains\n",
    "    run_command([\n",
    "        \"python\", os.path.join(helper_scripts_path, \"parse_multiple_chains.py\"),\n",
    "        \"--input_path\", protein_mpnn_folder,\n",
    "        \"--output_path\", os.path.join(protein_mpnn_folder, \"parsed_chains.json1\")\n",
    "    ])\n",
    "\n",
    "    # Assign fixed chains\n",
    "    run_command([\n",
    "        \"python\", os.path.join(helper_scripts_path, \"assign_fixed_chains.py\"),\n",
    "        \"--input_path\", os.path.join(protein_mpnn_folder, \"parsed_chains.json1\"),\n",
    "        \"--output_path\", os.path.join(protein_mpnn_folder, \"assigned_chains.json1\"),\n",
    "        \"--chain_list\", 'A'\n",
    "    ])\n",
    "\n",
    "    # Make fixed positions dict\n",
    "    run_command([\n",
    "        \"python\", os.path.join(helper_scripts_path, \"make_fixed_positions_dict.py\"),\n",
    "        \"--input_path\", os.path.join(protein_mpnn_folder, \"parsed_chains.json1\"),\n",
    "        \"--output_path\", os.path.join(protein_mpnn_folder, \"fixed_positions.json1\"),\n",
    "        \"--chain_list\", 'A',\n",
    "        \"--position_list\", \" \".join(DESIGN.split(\",\"))\n",
    "    ])\n",
    "\n",
    "    # Protein MPNN run\n",
    "    run_command([\n",
    "        \"python\", os.path.join(protein_mpnn_path, \"protein_mpnn_run.py\"),\n",
    "        \"--jsonl_path\", os.path.join(protein_mpnn_folder, \"parsed_chains.json1\"),\n",
    "        \"--chain_id_jsonl\", os.path.join(protein_mpnn_folder, \"assigned_chains.json1\"),\n",
    "        \"--fixed_positions_jsonl\", os.path.join(protein_mpnn_folder, \"fixed_positions.json1\"),\n",
    "        \"--out_folder\", protein_mpnn_folder,\n",
    "        \"--num_seq_per_target\", \"100\",\n",
    "        \"--sampling_temp\", ProteinMPNN_T,\n",
    "        \"--seed\", \"37\",\n",
    "        \"--batch_size\", \"1\"\n",
    "    ])\n",
    "    \n",
    "\n",
    "    # Find highest scoring sequence\n",
    "    highest_scoring_sequence = find_highest_scoring_sequence(protein_mpnn_folder, parent_index, input_sequence_path=\n",
    "                                                             f\"{FOLDER_HOME}/input_sequence_with_X_as_wildecard.seq\")\n",
    "\n",
    "    # Save highest scoring sequence and prepare for ESMfold\n",
    "    with open(os.path.join(protein_mpnn_folder, f\"{WT}_{new_index}.seq\"), \"w\") as f:\n",
    "        f.write(highest_scoring_sequence)\n",
    "    \n",
    "    if highest_scoring_sequence:\n",
    "        logging.info(f\"Ran ProteinMPNN for index {parent_index} and found a new sequence with index {new_index}.\")\n",
    "    else:\n",
    "        logging.error(f\"Failed to find a new sequnce for index {parent_index} with ProteinMPNN.\")\n",
    "    \n",
    "    all_scores_df = save_cat_res_into_all_scores_df(all_scores_df, new_index, pdb_file, from_parent_struct=False)\n",
    "        \n",
    "    # Run ESMfold Relax with the ProteinMPNN Flag\n",
    "    run_ESMfold_RosettaRelax(index=new_index, all_scores_df=all_scores_df, OnlyRelax=False, \\\n",
    "                             ProteinMPNN=True, ProteinMPNN_parent_index=parent_index, EXPLORE=EXPLORE)\n",
    "\n",
    "def find_highest_scoring_sequence(folder_path, parent_index, input_sequence_path):\n",
    "    \"\"\"\n",
    "    Identifies the highest scoring protein sequence from a set of generated PNPNN sequences,\n",
    "    excluding the parent and WT sequence (except wildcard positions specified by DESIGN).\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path to the directory containing sequence files (/ProteinMPNN).\n",
    "    - parent_index (str): The index of the parent protein sequence.\n",
    "    - input_sequence_path (str): The path to a file containing the input sequence pattern,\n",
    "      where 'X' represents wildcard positions that can match any character.\n",
    "    - -------------------------------- GLOBAL variables used ----------------------------\n",
    "    - WT (str): The wild type or reference protein identifier.\n",
    "      \n",
    "\n",
    "    Returns:\n",
    "    - highest_scoring_sequence (str): The protein sequence with the highest score \n",
    "      that does not match the parent and WT.\n",
    "    \n",
    "    Note:\n",
    "    This function parses .fa files to find sequences and their scores, and applies\n",
    "    a regex pattern derived from the input sequence to filter sequences.\n",
    "    It assumes the presence of 'global_score' within the sequence descriptor lines\n",
    "    in the .fa file for scoring.\n",
    "    \"\"\"\n",
    "    # Construct the file path for the sequence data\n",
    "    file_path = f'{folder_path}/seqs/{WT}_Rosetta_Relax_{parent_index}.fa'\n",
    "    parent_seq_file = f'{folder_path}/Rosetta_Relax_{parent_index}.seq'\n",
    "    \n",
    "    # Read the parent sequence from its file\n",
    "    with open(parent_seq_file, 'r') as file:\n",
    "        parent_sequence = file.readline().strip()\n",
    "\n",
    "    # Read the input sequence pattern and prepare it for regex matching\n",
    "    with open(input_sequence_path, 'r') as file:\n",
    "        input_sequence = file.readline().strip()\n",
    "    pattern = re.sub('X', '.', input_sequence)  # Replace 'X' with regex wildcard '.'\n",
    "\n",
    "    highest_score = 0\n",
    "    highest_scoring_sequence = ''\n",
    "\n",
    "    # Process the sequence file to find the highest scoring sequence\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                score_match = re.search('global_score=(\\d+\\.\\d+)', line)\n",
    "                if score_match:\n",
    "                    score = float(score_match.group(1))\n",
    "                    sequence = next(file, '').strip()  # Read the next line for the sequence\n",
    "                    \n",
    "                    # Check if the score is higher, the sequence is different from the parent,\n",
    "                    # and does not match the input sequence pattern\n",
    "                    if score > highest_score and sequence != parent_sequence and not re.match(pattern, sequence):\n",
    "                        highest_score = score\n",
    "                        highest_scoring_sequence = sequence\n",
    "\n",
    "    # Return the highest scoring sequence found\n",
    "    return highest_scoring_sequence\n",
    "\n",
    "def run_LigandMPNN(parent_index, new_index, all_scores_df):\n",
    "    \"\"\"\n",
    "    Executes the LigandMPNN pipeline for a given protein-ligand structure and generates\n",
    "    new protein sequences with potentially higher functional scores considering the ligand context.\n",
    "\n",
    "    Parameters:\n",
    "    - parent_index (str): The index of the parent protein variant.\n",
    "    - new_index (str): The index assigned to the new protein variant.\n",
    "    - all_scores_df (DataFrame): A DataFrame containing information for protein variants.\n",
    "    \"\"\"\n",
    "    # Ensure LigandMPNN is available\n",
    "    if not os.path.exists(f'{FOLDER_HOME}/../LigandMPNN'):\n",
    "        logging.error(f\"LigandMPNN not installed in {FOLDER_HOME}/LigandMPNN.\")\n",
    "        logging.error(\"Install using: git clone https://github.com/dauparas/LigandMPNN.git\")\n",
    "        return\n",
    "    ligand_mpnn_path = f\"{FOLDER_HOME}/../LigandMPNN\"\n",
    "\n",
    "    # Prepare file paths\n",
    "    pdb_file = f\"{FOLDER_HOME}/{parent_index}/{WT}_Rosetta_Relax_{parent_index}.pdb\"\n",
    "    if not os.path.isfile(pdb_file):\n",
    "        logging.error(f\"{pdb_file} not present!\")\n",
    "        return\n",
    "\n",
    "    ligand_mpnn_folder = f\"{FOLDER_HOME}/{new_index}/LigandMPNN\"\n",
    "    os.makedirs(ligand_mpnn_folder, exist_ok=True)\n",
    "    shutil.copy(pdb_file, os.path.join(ligand_mpnn_folder, f\"{WT}_Rosetta_Relax_{parent_index}.pdb\"))\n",
    "\n",
    "    # Extract catalytic residue information\n",
    "    cat_resi = int(all_scores_df.at[parent_index, 'cat_resi'])\n",
    "    fixed_residues = f\"A{cat_resi}\"\n",
    "\n",
    "    # Run LigandMPNN\n",
    "    run_command([\n",
    "        \"python\", os.path.join(ligand_mpnn_path, \"run.py\"),\n",
    "        \"--model_type\", \"ligand_mpnn\",\n",
    "        \"--seed\", \"37\",\n",
    "        \"--pdb_path\", os.path.join(ligand_mpnn_folder, f\"{WT}_Rosetta_Relax_{parent_index}.pdb\"),\n",
    "        \"--out_folder\", ligand_mpnn_folder,\n",
    "        #\"--pack_side_chains\", \"1\",\n",
    "        #\"--number_of_packs_per_design\", \"4\",\n",
    "        \"--fixed_residues\", fixed_residues\n",
    "    ])\n",
    "\n",
    "    # Update all_scores_df\n",
    "\n",
    "    logging.info(f\"Ran LigandMPNN for index {parent_index} and generated modifications for new index {new_index}.\")\n",
    "\n",
    "    # Save updates to all_scores_df\n",
    "    #save_all_scores_df(all_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e6193",
   "metadata": {},
   "source": [
    "# main functions - startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd671a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def startup_controller(UNBLOCK_ALL, RESET, PRINT_VAR=True, PLOT_DATA=True):\n",
    "\n",
    "    # Execute setup if variables file not found\n",
    "    if not os.path.isfile(VARIABLES_JSON): setup_aizymes(RESET) #? why is this called again?\n",
    "    \n",
    "    # Creat all input files, not needed here but does not harm\n",
    "    prepare_input_files()\n",
    "    \n",
    "    if PRINT_VAR:\n",
    "        if os.path.isfile(VARIABLES_JSON):\n",
    "            with open(VARIABLES_JSON, 'r') as f: \n",
    "                globals_dict = json.load(f)\n",
    "            if globals_dict['DESIGN_FOLDER'] == DESIGN_FOLDER:\n",
    "                for k, v in globals_dict.items():\n",
    "                    globals()[k] = v\n",
    "                    print(k.ljust(16), ':', v)\n",
    "            else:\n",
    "                print(\"WRONG DESIGN FOLDER!\")\n",
    "                sys.exit()\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    if PLOT_DATA:\n",
    "        plot_scores()\n",
    "        \n",
    "    # Read in current databases of AIzymes\n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "\n",
    "    if UNBLOCK_ALL: \n",
    "        all_scores_df[\"blocked\"] = False\n",
    "   \n",
    "    return all_scores_df \n",
    "\n",
    "def prepare_input_files():\n",
    "                      \n",
    "    # Create the ESMfold.py script\n",
    "    ESMfold_python_script = \"\"\"import sys\n",
    "from transformers import AutoTokenizer, EsmForProteinFolding, EsmConfig\n",
    "import torch\n",
    "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
    "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
    "\n",
    "output_file = sys.argv[1]\n",
    "sequence_file = sys.argv[2]\n",
    "\n",
    "with open(sequence_file) as f: sequence=f.read()\n",
    "\n",
    "def convert_outputs_to_pdb(outputs):\n",
    "    final_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
    "    outputs = {k: v.to(\"cpu\").numpy() for k, v in outputs.items()}\n",
    "    final_atom_positions = final_atom_positions.cpu().numpy()\n",
    "    final_atom_mask = outputs[\"atom37_atom_exists\"]\n",
    "    pdbs = []\n",
    "    for i in range(outputs[\"aatype\"].shape[0]):\n",
    "        aa = outputs[\"aatype\"][i]\n",
    "        pred_pos = final_atom_positions[i]\n",
    "        mask = final_atom_mask[i]\n",
    "        resid = outputs[\"residue_index\"][i] + 1\n",
    "        pred = OFProtein(\n",
    "            aatype=aa,\n",
    "            atom_positions=pred_pos,\n",
    "            atom_mask=mask,\n",
    "            residue_index=resid,\n",
    "            b_factors=outputs[\"plddt\"][i],\n",
    "            chain_index=outputs[\"chain_index\"][i] if \"chain_index\" in outputs else None,\n",
    "        )\n",
    "        pdbs.append(to_pdb(pred))\n",
    "    return pdbs\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\", low_cpu_mem_usage=True)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "model.trunk.set_chunk_size(64)\n",
    "tokenized_input = tokenizer([sequence], return_tensors=\"pt\", add_special_tokens=False)['input_ids']\n",
    "with torch.no_grad(): output = model(tokenized_input)\n",
    "pdb = convert_outputs_to_pdb(output)\n",
    "with open(output_file, \"w\") as f: f.write(\"\".join(pdb))\n",
    "\"\"\"\n",
    "\n",
    "    # Write the ESMfold.py to a file\n",
    "    with open(f\"{FOLDER_HOME}/ESMfold.py\", \"w\") as f: \n",
    "        f.write(ESMfold_python_script)\n",
    "\n",
    "    # Save input sequence with X as wildcard\n",
    "    if not os.path.isfile(f\"{FOLDER_INPUT}/{WT}.pdb\"):\n",
    "        print(f\"Error, scaffold protein structure {FOLDER_INPUT}/{WT}.pdb is missing!\")\n",
    "        sys.exit()        \n",
    "    seq = extract_sequence_from_pdb(f\"{FOLDER_INPUT}/{WT}.pdb\")\n",
    "    design_positions = [int(x) for x in DESIGN.split(',')]\n",
    "    # Replace seq with X at design positions. Note: Subtract 1 from each position to convert to Python's 0-based indexing\n",
    "    seq = ''.join('X' if (i+1) in design_positions else amino_acid for i, amino_acid in enumerate(seq))\n",
    "    with open(f'{FOLDER_HOME}/input_sequence_with_X_as_wildecard.seq', 'w') as f:\n",
    "        f.writelines(seq)    \n",
    "    \n",
    "    # Get the Constraint Residues from enzdes constraints file\n",
    "    with open(f'{FOLDER_INPUT}/{LIGAND}_enzdes.cst', 'r') as f:\n",
    "        cst = f.readlines()    \n",
    "    cst = [i.split()[-1] for i in cst if \"TEMPLATE::   ATOM_MAP: 2 res\" in i]\n",
    "    cst = \";\".join(cst)\n",
    "    with open(f'{FOLDER_HOME}/cst.dat', 'w') as f:\n",
    "        f.write(cst)    \n",
    "    \n",
    "def setup_aizymes(RESET=False, EXPLORE=False):\n",
    "    \n",
    "    # Check if setup needs to run\n",
    "    if not os.path.isfile(VARIABLES_JSON):\n",
    "        if not input(f'''Do you want to start AIzymes? [y/n]\n",
    "\n",
    "''') == 'y':\n",
    "            return #stops the start up. Although VARIABLES_JSON is missing, user elected not to set up AIzymes\n",
    "    else:\n",
    "        if RESET:\n",
    "            if not input(f'''Do you really want to restart AIzymes from scratch? \n",
    "This will delete all existing files in {FOLDER_HOME} [y/n]\n",
    "\n",
    "''') == 'y':\n",
    "                return #stops the start up. Although VARIABLES_JSON exists and RESET set, user canceled\n",
    "        else:\n",
    "            return #stop startup. VARIABLES_JSON exists and RESET not set by user\n",
    "\n",
    "    with open(LOG_FILE, 'w'): pass  #resets logfile\n",
    "    logging.info(f\"Running AI.zymes setup.\")\n",
    "    logging.info(f\"Content of {FOLDER_HOME} deleted.\")\n",
    "    logging.info(f\"Happy AI.zymeing! :)\")\n",
    "   \n",
    "    if os.path.exists(FOLDER_HOME):\n",
    "        for item in os.listdir(FOLDER_HOME):\n",
    "            if item == FOLDER_MATCH: continue\n",
    "            item = f'{FOLDER_HOME}/{item}'\n",
    "            if os.path.isfile(item): \n",
    "                os.remove(item)\n",
    "            elif os.path.isdir(item):\n",
    "                shutil.rmtree(item)\n",
    "    os.makedirs(FOLDER_HOME, exist_ok=True)\n",
    "\n",
    "    prepare_input_files()\n",
    "        \n",
    "    #make empyt all_scores_df\n",
    "    all_scores_df = make_empty_all_scores_df()\n",
    "\n",
    "    # create empty blocked.dat\n",
    "    #np.savetxt(BLOCKED_DAT, np.array([], dtype=int), fmt='%d')\n",
    "\n",
    "    # Save global varliables\n",
    "    variables_to_save = [\n",
    "        'DESIGN_FOLDER', 'FOLDER_MATCH', 'MAX_JOBS', 'N_PARENT_JOBS', 'MAX_DESIGNS', 'KBT_BOLTZMANN', 'CST_WEIGHT',\n",
    "        'ProteinMPNN_PROB', 'WT', 'LIGAND', 'ROSETTA_PATH', 'REPACK', 'DESIGN', 'MATCH', 'FOLDER_PARENT',\n",
    "        'ProteinMPNN_T', 'SUBMIT_PREFIX', 'BLUEPEBBLE', 'GRID', 'BACKGROUND_JOB', 'ABBIE_LOCAL'\n",
    "    ]\n",
    "    globals_to_save = {k: globals()[k] for k in variables_to_save}\n",
    "    globals_to_save['EXPLORE'] = EXPLORE\n",
    "    \n",
    "    if N_PARENT_JOBS < MAX_JOBS*2:\n",
    "        logging.warning(f\"To ensure a smooth start, N_PARENT_JOBS should be at least 2 x MAX_JOBS.\")\n",
    "        logging.warning(f\"N_PARENT_JOBS: {N_PARENT_JOBS}, MAX_JOBS: {MAX_JOBS}.\")\n",
    "            \n",
    "    with open(VARIABLES_JSON, 'w') as f: json.dump(globals_to_save, f, indent=4)\n",
    "    \n",
    "    \n",
    "def make_empty_all_scores_df():\n",
    "    \n",
    "    all_scores_df = pd.DataFrame(columns=['index', 'sequence', 'parent_index', \\\n",
    "                                          'interface_score', 'total_score', 'catalytic_score', 'efield_score', \\\n",
    "                                          'interface_potential', 'total_potential', 'catalytic_potential', 'efield_potential', \\\n",
    "                                          'relax_interface_score', 'relax_total_score', 'relax_catalytic_score', 'relax_efield_score' \\\n",
    "                                          'design_interface_score', 'design_total_score', 'design_catalytic_score', 'design_efield_score' \\\n",
    "                                          'generation', 'mutations', 'design_method', 'score_taken_from', 'blocked', \\\n",
    "                                          'cat_resi', 'cat_resn'])\n",
    "    \n",
    "    save_all_scores_df(all_scores_df)\n",
    "\n",
    "    return all_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c01cc",
   "metadata": {},
   "source": [
    "# RosettaMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dced448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RosettaMatch(EXPLORE=False, submit=False, bash=True):\n",
    "        \n",
    "    prepare_input_files()\n",
    "    \n",
    "    os.makedirs(FOLDER_MATCH, exist_ok=True)\n",
    "    if not os.path.isdir(f'{FOLDER_HOME}/{FOLDER_MATCH}/scripts'):\n",
    "        run_ESMfold_RosettaRelax(FOLDER_MATCH, all_scores_df=None, PreMatchRelax=True, EXPLORE=EXPLORE) \n",
    "    elif not os.path.isfile(f'{FOLDER_HOME}/{FOLDER_MATCH}/{WT}_Rosetta_Relax_{FOLDER_MATCH}_APO.pdb'):\n",
    "        print(f\"ESMfold and Relax of {FOLDER_MATCH} still running.\")\n",
    "    elif not os.path.isdir(f'{FOLDER_HOME}/{FOLDER_MATCH}/matches'):\n",
    "        run_Matcher()\n",
    "    else:\n",
    "        print(\"Matching is done\")\n",
    "            \n",
    "def run_Matcher():\n",
    "        \n",
    "    cmd = f\"\"\"       \n",
    "  \n",
    "cd {FOLDER_HOME}/{FOLDER_MATCH}\n",
    "\n",
    "echo C9 > {LIGAND}.central\n",
    "echo {\" \".join(MATCH.split(\",\"))} > {LIGAND}.pos\n",
    "\n",
    "{ROSETTA_PATH}/bin/gen_lig_grids.linuxgccrelease \\\n",
    "    -s                      {WT}_Rosetta_Relax_aligned_{FOLDER_MATCH}_APO.pdb ESMfold/{WT}_CPPTraj_Lig_{FOLDER_MATCH}.pdb \\\n",
    "    -extra_res_fa           {FOLDER_INPUT}/{LIGAND}.params \\\n",
    "    -grid_delta             0.5 \\\n",
    "    -grid_lig_cutoff        5.0 \\\n",
    "    -grid_bb_cutoff         2.25 \\\n",
    "    -grid_active_res_cutoff 15.0 \\\n",
    "    -overwrite \n",
    "\n",
    "mv {WT}_Rosetta_Relax_aligned_{FOLDER_MATCH}_APO.pdb_0.gridlig {WT}.gridlig\n",
    "rm {WT}_Rosetta_Relax_aligned_{FOLDER_MATCH}_APO.pdb_0.pos 2>1\n",
    "\n",
    "rm -r matches\n",
    "mkdir matches\n",
    "cd matches\n",
    "\n",
    "{ROSETTA_PATH}/bin/match.linuxgccrelease \\\n",
    "    -s                                        ../{WT}_Rosetta_Relax_aligned_{FOLDER_MATCH}_APO.pdb \\\n",
    "    -match:lig_name                           {LIGAND} \\\n",
    "    -extra_res_fa                             {FOLDER_INPUT}/{LIGAND}.params \\\n",
    "    -match:geometric_constraint_file          {FOLDER_INPUT}/{LIGAND}_enzdes.cst \\\n",
    "    -match::scaffold_active_site_residues     ../{LIGAND}.pos \\\n",
    "    -match:required_active_site_atom_names    ../{LIGAND}.central \\\n",
    "    -match:active_site_definition_by_gridlig  ../{WT}.gridlig  \\\n",
    "    -match:grid_boundary                      ../{WT}.gridlig  \\\n",
    "    -gridligpath                              ../{WT}.gridlig  \\\n",
    "    -overwrite  \\\n",
    "    -output_format PDB  \\\n",
    "    -output_matches_per_group 1  \\\n",
    "    -consolidate_matches true \n",
    "\"\"\" \n",
    "    with open(f'{FOLDER_HOME}/{FOLDER_MATCH}/scripts/RosettaMatch_{FOLDER_MATCH}.sh', 'w') as file: file.write(cmd)\n",
    "    logging.info(f\"Run Rosetta_Match for index {FOLDER_MATCH}.\")\n",
    "    submit_job(FOLDER_MATCH, job=\"RosettaMatch\", bash=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db4bf52",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_job(index, job, bash=False):        \n",
    "      \n",
    "    if GRID:\n",
    "        submission_script = f\"\"\"#!/bin/bash\n",
    "#$ -V\n",
    "#$ -cwd\n",
    "#$ -N {SUBMIT_PREFIX}_{job}_{index}\n",
    "#$ -hard -l mf=16G\n",
    "#$ -o {FOLDER_HOME}/{index}/scripts/AI_{job}_{index}.out\n",
    "#$ -e {FOLDER_HOME}/{index}/scripts/AI_{job}_{index}.err\n",
    "\"\"\"\n",
    "    if BLUEPEBBLE:\n",
    "        submission_script = f\"\"\"#!/bin/bash\n",
    "#SBATCH --account={BLUEPEBBLE_ACCOUNT}\n",
    "#SBATCH --partition=short\n",
    "#SBATCH --mem=40GB\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --time=2:00:00    \n",
    "#SBATCH --nodes=1          \n",
    "#SBATCH --job-name={SUBMIT_PREFIX}_{job}_{index}\n",
    "#SBATCH --output={FOLDER_HOME}/{index}/scripts/AI_{job}_{index}.out\n",
    "#SBATCH --error={FOLDER_HOME}/{index}/scripts/AI_{job}_{index}.err\n",
    "\"\"\"\n",
    "        \n",
    "    if BACKGROUND_JOB:\n",
    "        if not os.path.isfile(f'{FOLDER_HOME}/n_running_jobs.dat'):\n",
    "            with open(f'{FOLDER_HOME}/n_running_jobs.dat', 'w') as f: f.write('0')\n",
    "        with open(f'{FOLDER_HOME}/n_running_jobs.dat', 'r'): jobs = int(f.read())\n",
    "        with open(f'{FOLDER_HOME}/n_running_jobs.dat', 'w'): f.write(jobs+1)\n",
    "        submission_script = \"\"\n",
    "\n",
    "    if ABBIE_LOCAL:\n",
    "        submission_script = \"\"\n",
    "        \n",
    "    submission_script += f\"\"\"\n",
    "# Output folder\n",
    "cd {FOLDER_HOME}/{index}\n",
    "pwd\n",
    "bash {FOLDER_HOME}/{index}/scripts/{job}_{index}.sh\n",
    "\"\"\" \n",
    "    if BACKGROUND_JOB:\n",
    "        submission_script = f\"\"\"\n",
    "jobs=$(cat {FOLDER_HOME}/n_running_jobs.dat)\n",
    "jobs=$((jobs - 1))\n",
    "echo \"$jobs\" > {FOLDER_HOME}/n_running_jobs.dat\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    # Create the submission_script\n",
    "    with open(f'{FOLDER_HOME}/{index}/scripts/submit_{job}_{index}.sh', 'w') as file: file.write(submission_script)\n",
    "    \n",
    "    if bash:\n",
    "        #Bash the submission_script for testing\n",
    "        subprocess.run(f'bash {FOLDER_HOME}/{index}/scripts/submit_{job}_{index}.sh', shell=True, text=True)\n",
    "    else:\n",
    "        #Submit the submission_script\n",
    "        if GRID:\n",
    "            output = subprocess.check_output(f'qsub -l h=\"!bs-dsvr64&!bs-dsvr58\" -q regular.q \\\n",
    "                                             {FOLDER_HOME}/{index}/scripts/submit_{job}_{index}.sh', \\\n",
    "                                             shell=True, text=True)\n",
    "            logging.debug(output[:-1]) #remove newline at end of output\n",
    "            \n",
    "        if BLUEPEBBLE:\n",
    "            output = subprocess.check_output(f'sbatch {FOLDER_HOME}/{index}/scripts/submit_{job}_{index}.sh', \\\n",
    "                                             shell=True, text=True)\n",
    "            logging.debug(output[:-1]) #remove newline at end of output\n",
    "            \n",
    "        if BACKGROUND_JOB:\n",
    "\n",
    "            stdout_log_file_path = f'{FOLDER_HOME}/{index}/scripts/submit_{job}_{index}_stdout.log'\n",
    "            stderr_log_file_path = f'{FOLDER_HOME}/{index}/scripts/submit_{job}_{index}_stderr.log'\n",
    "\n",
    "            with open(stdout_log_file_path, 'w') as stdout_log_file, open(stderr_log_file_path, 'w') as stderr_log_file:\n",
    "                process = subprocess.Popen(f'bash {FOLDER_HOME}/{index}/scripts/submit_{job}_{index}.sh &', \n",
    "                                           shell=True, stdout=stdout_log_file, stderr=stderr_log_file)\n",
    "        \n",
    "        if ABBIE_LOCAL:\n",
    "\n",
    "            stdout_log_file_path = f'{FOLDER_HOME}/{index}/scripts/submit_{job}_{index}_stdout.log'\n",
    "            stderr_log_file_path = f'{FOLDER_HOME}/{index}/scripts/submit_{job}_{index}_stderr.log'\n",
    "\n",
    "            with open(stdout_log_file_path, 'w') as stdout_log_file, open(stderr_log_file_path, 'w') as stderr_log_file:\n",
    "                process = subprocess.Popen(f'bash {FOLDER_HOME}/{index}/scripts/submit_{job}_{index}.sh &', \n",
    "                                           shell=True, stdout=stdout_log_file, stderr=stderr_log_file)\n",
    "        \n",
    "def extract_sequence_from_pdb(pdb_path):\n",
    "    with open(pdb_path, \"r\") as pdb_file:\n",
    "        for record in SeqIO.parse(pdb_file, \"pdb-atom\"):\n",
    "            seq = str(record.seq)\n",
    "    return seq\n",
    "    \n",
    "def generate_remark_from_all_scores_df(all_scores_df, index):\n",
    "\n",
    "    remark = ''\n",
    "    cat_resns = str(all_scores_df.at[index, 'cat_resn']).split(';')\n",
    "    cat_resis = str(all_scores_df.at[index, 'cat_resi']).split(';')\n",
    "    \n",
    "    remarks = []\n",
    "\n",
    "    for cat_resi, cat_resn in zip(cat_resis, cat_resns):\n",
    "        remarks.append(f'REMARK 666 MATCH TEMPLATE X {LIGAND}    0 MATCH MOTIF A {cat_resn}{str(cat_resi).rjust(5)}  1  1')\n",
    "    return \"\\n\".join(remarks)\n",
    "\n",
    "def save_cat_res_into_all_scores_df(all_scores_df, index, PDB_file_path, from_parent_struct=False):\n",
    "    \n",
    "    '''Finds the indices and names of the catalytic residue from <PDB_file_path> \n",
    "       Saves indices and residues into <all_scores_df> in row <index> as lists.\n",
    "       To make sure these are saved and loaded as list, \";\".join() and .split(\";\") should be used\n",
    "       IF information is read from an input structure for design do not save cat_resn\n",
    "       Returns the updated all_scores_df'''\n",
    "      \n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    with open(PDB_file_path, 'r') as f: \n",
    "        PDB = f.readlines()\n",
    "    \n",
    "    remarks = [i for i in PDB if i[:10] == 'REMARK 666']\n",
    "\n",
    "    cat_resis = []\n",
    "    cat_resns = []\n",
    "\n",
    "    for remark in remarks:\n",
    "        cat_resis.append(str(int(remark[55:59])))\n",
    "\n",
    "    for cat_resi in cat_resis:\n",
    "        for line in PDB[len(remarks)+2:]:\n",
    "            atomtype = line[12:16]\n",
    "            if atomtype != \" CA \": continue\n",
    "            resi = str(int(line[22:26]))\n",
    "            resn = line[17:20]\n",
    "            if resi == cat_resi:\n",
    "                cat_resns.append(resn)\n",
    "                break\n",
    "\n",
    "    # all_scores_df['cat_resi'] = all_scores_df['cat_resi'].astype(object)\n",
    "    # all_scores_df['cat_resn'] = all_scores_df['cat_resn'].astype(object)\n",
    "    all_scores_df.at[index, 'cat_resi'] = \";\".join(cat_resis)\n",
    "    # Only save the cat_resn if this comes from the designed structure, not from the input structure for design\n",
    "    if not from_parent_struct:\n",
    "        all_scores_df.at[index, 'cat_resn'] = \";\".join(cat_resns)\n",
    "    \n",
    "    return all_scores_df \n",
    "\n",
    "def reset_to_after_parent_design():\n",
    "    \n",
    "    folders = []\n",
    "    \n",
    "    for folder_name in os.listdir(FOLDER_HOME):\n",
    "        if os.path.isdir(os.path.join(FOLDER_HOME, folder_name)) and folder_name.isdigit():\n",
    "            folders.append(int(folder_name))\n",
    "    \n",
    "    all_scores_df = make_empty_all_scores_df()\n",
    "        \n",
    "    PARENTS = [i for i in os.listdir(f'{FOLDER_HOME}/{FOLDER_PARENT}') if i[-4:] == \".pdb\"]\n",
    "    \n",
    "    for folder in sorted(folders):\n",
    "        \n",
    "        folder_path = os.path.join(FOLDER_HOME, str(folder))\n",
    "        \n",
    "        if folder >= N_PARENT_JOBS * len(PARENTS):\n",
    "            \n",
    "            #Remove non-parent designs\n",
    "            shutil.rmtree(folder_path)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #Remove Potentials\n",
    "            for item in os.listdir(folder_path):\n",
    "                if 'potential.dat' not in item: continue\n",
    "                item_path = os.path.join(folder_path, item)\n",
    "                os.remove(item_path)\n",
    "                print(item_path)\n",
    "                    \n",
    "            #Update Scorefile\n",
    "            new_index, all_scores_df = create_new_index(parent_index=\"Parent\", all_scores_df=all_scores_df)\n",
    "            all_scores_df['design_method'] = all_scores_df['design_method'].astype('object') \n",
    "            all_scores_df.at[new_index, 'design_method'] = \"RosettaDesign\"\n",
    "            all_scores_df['luca'] = all_scores_df['luca'].astype('object') \n",
    "            score_file_path = f\"{FOLDER_HOME}/{int(index)}/score_rosetta_design.sc\"\n",
    "            with open(score_file_path, 'r') as f: score = f.readlines()[2]\n",
    "            all_scores_df.at[new_index, 'luca'] = score.split()[-1][:-5]\n",
    "    \n",
    "            if new_index % 100 == 0: print(folder, new_index) \n",
    "\n",
    "    save_all_scores_df(all_scores_df)\n",
    "    \n",
    "def save_all_scores_df(all_scores_df):\n",
    "    \n",
    "    temp_fd, temp_path = tempfile.mkstemp(dir=FOLDER_HOME) # Create a temporary file\n",
    "\n",
    "    try:\n",
    "        all_scores_df.to_csv(temp_path, index=False)       # Save DataFrame to the temporary file\n",
    "        os.close(temp_fd)                                  # Close file descriptor\n",
    "        os.rename(temp_path, ALL_SCORES_CSV)               # Rename temporary file to final filename\n",
    "    except Exception as e:\n",
    "        os.close(temp_fd)                                  # Ensure file descriptor is closed in case of error\n",
    "        os.unlink(temp_path)                               # Remove the temporary file if an error occurs\n",
    "        raise e\n",
    "\n",
    "def get_best_structures():\n",
    "    # Remove all .pdb files in the current directory\n",
    "    pdb_files = glob.glob('*.pdb')\n",
    "    for file in pdb_files:\n",
    "        os.remove(file)\n",
    "\n",
    "    # Read the scores DataFrame\n",
    "    all_scores_df = pd.read_csv('/home/bunzelh/231126_Aizymes_RL/Design_MATCH_AI/all_scores.csv')\n",
    "\n",
    "    # Drop rows with NaN in 'total_score'\n",
    "    all_scores_df = all_scores_df.dropna(subset=['total_score'])\n",
    "\n",
    "    # Normalize the specified columns\n",
    "\n",
    "    ## Normalise efield score here??\n",
    "    for column in ['interface_score', 'total_score', 'catalytic_score']:\n",
    "        min_val = all_scores_df[column].min()\n",
    "        max_val = all_scores_df[column].max()\n",
    "        all_scores_df[column] = 1 - ((all_scores_df[column] - min_val) / (max_val - min_val))\n",
    "\n",
    "    # Calculate the geometric mean of the normalized values\n",
    "    all_scores_df['combined_score'] = all_scores_df[['interface_score', 'total_score', 'catalytic_score']].apply(\n",
    "        lambda row: np.prod(row)**(1/3), axis=1)\n",
    "\n",
    "    all_scores_df['replicate_sequences'] = 0  # Initialize to count duplicates\n",
    "    all_scores_df['replicate_sequences_combined_score'] = 0.0  # To store the average score\n",
    "    all_scores_df['replicate_sequences_combined_score_std'] = 0.0  # To store the standard deviation\n",
    "\n",
    "    # Loop to find duplicates, calculate average score, and standard deviation\n",
    "    for i, row in all_scores_df.iterrows():\n",
    "        duplicates = all_scores_df[all_scores_df['sequence'] == row['sequence']]\n",
    "        avg_score  = duplicates['total_score'].mean()\n",
    "        std_dev    = duplicates['total_score'].std()\n",
    "\n",
    "        all_scores_df.at[i, 'replicate_sequences']                    = len(duplicates)\n",
    "        all_scores_df.at[i, 'replicate_sequences_combined_score']     = avg_score\n",
    "        all_scores_df.at[i, 'replicate_sequences_combined_score_std'] = std_dev\n",
    "\n",
    "    # Remove replicates and keep only highest combined_score\n",
    "    all_scores_df.sort_values(by=['combined_score'], ascending=[False], inplace=True)\n",
    "    all_scores_df.drop_duplicates(subset=['sequence'], keep='first', inplace=True)\n",
    "\n",
    "    # Remove all structures with 'catalytic_score' < 0.95\n",
    "    all_scores_df = all_scores_df[all_scores_df['catalytic_score'] >= 0.95]\n",
    "\n",
    "    # Get the 10 rows with the highest geometric mean values\n",
    "    top10 = all_scores_df.nlargest(10, 'combined_score')[['index','luca','cat_resn','cat_resi','interface_score', \n",
    "                                                          'total_score', 'catalytic_score','combined_score','mutations',\n",
    "                                                          'replicate_sequences','replicate_sequences_combined_score',\n",
    "                                                          'replicate_sequences_combined_score_std']]\n",
    "\n",
    "    display(all_scores_df)\n",
    "    display(top10)\n",
    "\n",
    "    # Copy files based on the top10 'index'\n",
    "    for index, row in top10.iterrows():\n",
    "        geom_mean = \"{:.3f}\".format(row['combined_score'])\n",
    "        src_file = f\"{FOLDER_HOME}/{int(index)}/{WT}_Rosetta_Design_{int(index)}.pdb\"\n",
    "        dest_file = f\"{geom_mean}_{WT}_Rosetta_Design_{int(index)}.pdb\"\n",
    "        shutil.copy(src_file, dest_file)\n",
    "\n",
    "def run_command(command):\n",
    "    \"\"\"Wrapper to execute .py files in runntime with arguments\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        return result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Command '{e.cmd}' failed with return code {e.returncode}\")\n",
    "        logging.error(e.output)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while running command: {command}\")\n",
    "        raise\n",
    "\n",
    "def wait_for_file(file_path, timeout=5):\n",
    "    \"\"\"Wait for a file to exist and have a non-zero size.\"\"\"\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "            return True\n",
    "        time.sleep(0.1)  # Wait for 0.1 seconds before checking again\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d0d19",
   "metadata": {},
   "source": [
    "## Electric Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb417fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_AMBER_files(structure_filename : str):\n",
    "    '''Uses tleap to create a .parm7 and .rst7 file from a pdb. Requires ambertools and pdb-tools \n",
    "    (pip installable from  https://www.bonvinlab.org/pdb-tools/).\n",
    "    Also requires 5TS.prepi and 5TS.frcmod in the INPUT folder\n",
    "    TODO: Add script to generate these if not present'''\n",
    "\n",
    "    #delete everything after 'CONECT' - otherwise Amber tries to read it as extra residues\n",
    "    clean_pdb = run_command(['sed', '-n', '/CONECT/q;p', f\"{structure_filename}.pdb\"])\n",
    "    with open(f\"{structure_filename}_clean.pdb\", \"w\") as f:\n",
    "        f.write(clean_pdb)\n",
    "\n",
    "    # with open(f\"{structure_filename}_clean.pdb\", \"w\") as f:\n",
    "    #     subprocess.call(['sed', '-n', '/CONECT/q;p', f\"{structure_filename}.pdb\"], stdout=f)\n",
    "        \n",
    "    #remove hydrogens - requires pip install of https://www.bonvinlab.org/pdb-tools/\n",
    "    noHclean_pdb = run_command(['pdb_delelem', '-H', f\"{structure_filename}_clean.pdb\"])\n",
    "    with open(f\"{structure_filename}_noHclean.pdb\", \"w\") as f:\n",
    "        f.write(noHclean_pdb)\n",
    "\n",
    "    # with open(f\"{structure_filename}_noHclean.pdb\", \"w\") as f:\n",
    "    #     subprocess.call(['pdb_delelem', '-H', f\"{structure_filename}_clean.pdb\"], stdout=f)\n",
    "\n",
    "    os.remove(f\"{structure_filename}_clean.pdb\")\n",
    "\n",
    "    with open(\"tleap.in\", \"w\") as f:\n",
    "            f.write(f\"\"\"source leaprc.protein.ff19SB \n",
    "    source leaprc.gaff\n",
    "    loadamberprep   Input/5TS.prepi\n",
    "    loadamberparams Input/5TS.frcmod\n",
    "    mol = loadpdb {structure_filename}_noHclean.pdb\n",
    "    saveamberparm mol {structure_filename}.parm7 {structure_filename}.rst7\n",
    "    quit\n",
    "    \"\"\")\n",
    "            \n",
    "    run_command([\"tleap\", \"-s\", \"-f\", \"tleap.in\"])\n",
    "\n",
    "def calc_efields_score(pdb_path):\n",
    "    '''Executes the FieldTools.py script to calculate the electric field across the C-H bond of 5TS.\n",
    "    Requires a field_target.dat in the Input folder. Currently hard-coded based on 5TS\n",
    "    TODO: Make this function agnostic to contents of field_target'''\n",
    "    #from FieldTools import python_main    \n",
    "\n",
    "    structure_filename = os.path.splitext(pdb_path)[0]\n",
    "\n",
    "    generate_AMBER_files(structure_filename)\n",
    "\n",
    "    run_command([\"python\", \"FieldTools.py\", \n",
    "                \"-nc\", f\"{os.path.relpath(structure_filename)}.rst7\", \n",
    "                \"-parm\", f\"{os.path.relpath(structure_filename)}.parm7\", \n",
    "                \"-out\", f\"{os.path.relpath(structure_filename)}_fields.pkl\", \n",
    "                \"-target\", \"Input/field_target.dat\", \n",
    "                \"-solvent\", \"WAT\"])\n",
    "    \n",
    "    #python_main([\"FieldTools.py\", \"-nc\", f\"{os.path.relpath(output_filename)}.rst7\", \"-parm\", f\"{os.path.relpath(output_filename)}.parm7\", \"-out\", out_path, \"-target\", \"Input/field_target.dat\", \"-solvent\", \"WAT\"])\n",
    "\n",
    "    with open(f\"{structure_filename}_fields.pkl\", \"rb\") as f:\n",
    "        FIELDS = pkl.load(f)\n",
    "\n",
    "    bond_field = FIELDS[':5TS@C9_:5TS@H04']['Total']\n",
    "    all_fields = FIELDS[':5TS@C9_:5TS@H04']\n",
    "\n",
    "    return bond_field[0], all_fields\n",
    "\n",
    "# def update_efieldsdf(index, all_fields):\n",
    "#     if index == 0:       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2608bedf",
   "metadata": {},
   "source": [
    "# plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(combined_score_min=0, combined_score_max=1, combined_score_bin=0.01,\n",
    "                interface_score_min=0, interface_score_max=1, interface_score_bin=0.01,\n",
    "                total_score_min=0, total_score_max=1, total_score_bin=0.01,\n",
    "                catalytic_score_min=0, catalytic_score_max=1, catalytic_score_bin=0.01,\n",
    "                mut_min=0,mut_max=len(DESIGN.split(\",\"))+1):\n",
    "        \n",
    "    # Break because file does not exist\n",
    "    if not os.path.isfile(ALL_SCORES_CSV): return\n",
    "    \n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "    all_scores_df['sequence'] = all_scores_df['sequence'].astype(str)\n",
    "    all_scores_df['design_method'] = all_scores_df['design_method'].astype(str)\n",
    "    all_scores_df['score_taken_from'] = all_scores_df['score_taken_from'].astype(str)\n",
    "            \n",
    "    # Break because not enough data\n",
    "    if len(all_scores_df.dropna(subset=['total_score'])) < 3: return\n",
    "    \n",
    "    ### Calculate Generations and Scores    \n",
    "    G = nx.DiGraph()\n",
    "    ### generations = {} not needed\n",
    "    \n",
    "    for idx, row in all_scores_df.iterrows():\n",
    "        G.add_node(idx)\n",
    "        if row['parent_index'] != \"Parent\":\n",
    "            G.add_edge(int(float(row['parent_index'])), idx)        \n",
    "        \n",
    "    # Plot data\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(15, 6))\n",
    "    \n",
    "    all_scores_df = all_scores_df.dropna(subset=['total_score'])\n",
    "    catalytic_scores, total_scores, interface_scores, combined_scores = normalize_scores(all_scores_df, \n",
    "                                                                                         print_norm=True,\n",
    "                                                                                         norm_all=True)\n",
    "            \n",
    "    plot_combined_score(axs[0,0], combined_scores, \\\n",
    "                        combined_score_min, combined_score_max, combined_score_bin)\n",
    "    plot_interface_score(axs[0,1], interface_scores, \\\n",
    "                         interface_score_min, interface_score_max, interface_score_bin)\n",
    "    plot_total_score(axs[0,2], total_scores, \\\n",
    "                     total_score_min, total_score_max, total_score_bin)\n",
    "    plot_catalytic_score(axs[0,3], catalytic_scores, \\\n",
    "                         catalytic_score_min, catalytic_score_max, catalytic_score_bin)\n",
    "    \n",
    "    plot_boltzmann_histogram(axs[1,0], combined_scores, all_scores_df, \\\n",
    "                             combined_score_min, combined_score_max, combined_score_bin)\n",
    "    plot_combined_score_v_index(axs[1,1], combined_scores, all_scores_df)\n",
    "    plot_combined_score_v_generation(axs[1,2], combined_scores, all_scores_df)\n",
    "    plot_mutations_v_generation(axs[1,3], all_scores_df, mut_min, mut_max)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    #fig, ax = plt.subplots(1, 1, figsize=(15, 2.5))\n",
    "    #plot_tree(ax, combined_scores, all_scores_df, G)\n",
    "    #plt.show()\n",
    "\n",
    "def plot_combined_score(ax, combined_scores, score_min, score_max, score_bin):\n",
    "    \n",
    "    ax.hist(combined_scores, bins=np.arange(score_min,score_max+score_bin,score_bin))\n",
    "    ax.axvline(HIGHSCORE, color='b')\n",
    "    ax.axvline(NEG_BEST, color='r')\n",
    "    ax.set_xlim(score_min,score_max)\n",
    "    ax.set_title('Histogram of Score')\n",
    "    ax.set_xlabel('Combined Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "def plot_interface_score(ax, interface_scores, interface_score_min, interface_score_max, interface_score_bin):\n",
    "        \n",
    "    ax.hist(interface_scores, density=True,\n",
    "            bins=np.arange(interface_score_min,interface_score_max+interface_score_bin,interface_score_bin))\n",
    "    ax.set_xlim(interface_score_min,interface_score_max)\n",
    "    ax.set_title('Histogram of Interface Score')\n",
    "    ax.set_xlabel('Interface Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "def plot_total_score(ax, total_scores, total_score_min, total_score_max, total_score_bin):\n",
    "\n",
    "    ax.hist(total_scores, density=True,\n",
    "            bins=np.arange(total_score_min,total_score_max+total_score_bin,total_score_bin))\n",
    "    ax.set_xlim(total_score_min,total_score_max)\n",
    "    ax.set_title('Histogram of Total Score')\n",
    "    ax.set_xlabel('Total Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "def plot_catalytic_score(ax, catalytic_scores, total_score_min, total_score_max, total_score_bin):\n",
    "\n",
    "    ax.hist(catalytic_scores, density=True,\n",
    "            bins=np.arange(total_score_min,total_score_max+total_score_bin,total_score_bin))\n",
    "    ax.set_xlim(total_score_min,total_score_max)\n",
    "    ax.set_title('Histogram of Catalytic Score')\n",
    "    ax.set_xlabel('Catalytic Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "\n",
    "def plot_boltzmann_histogram(ax, combined_scores, all_scores_df, score_min, score_max, score_bin):\n",
    "\n",
    "    _, _, _, combined_potentials = normalize_scores(all_scores_df, print_norm=False, norm_all=False, extension=\"potential\")\n",
    "            \n",
    "    if isinstance(KBT_BOLTZMANN, (float, int)):\n",
    "        kbt_boltzmann = KBT_BOLTZMANN\n",
    "    else:\n",
    "        if len(KBT_BOLTZMANN) == 2:\n",
    "            kbt_boltzmann = KBT_BOLTZMANN[0] * 10 ** (- KBT_BOLTZMANN[1] * len(all_scores_df))\n",
    "    \n",
    "    boltzmann_factors = np.exp(combined_potentials / (kbt_boltzmann)) \n",
    "    probabilities     = boltzmann_factors / sum(boltzmann_factors) \n",
    "    \n",
    "    random_scores     = np.random.choice(combined_potentials, size=1000, replace=True)\n",
    "    boltzmann_scores  = np.random.choice(combined_potentials, size=1000, replace=True, p=probabilities)\n",
    "\n",
    "    # Plot the first histogram\n",
    "    ax.hist(random_scores, density=True, alpha=0.7, label='Random Sampling', \\\n",
    "            bins=np.arange(score_min,score_max+score_bin,score_bin))\n",
    "    ax.text(0.05, 0.95, \"normalized only to \\n this dataset\")\n",
    "    ax.set_xlabel('Potential')\n",
    "    ax.set_ylabel('Density (Normal)')\n",
    "    ax.set_title(f'kbT = {kbt_boltzmann:.1e}')\n",
    "    # Create a twin y-axis for the second histogram\n",
    "    ax_dup = ax.twinx()\n",
    "    ax_dup.hist(boltzmann_scores, density=True, alpha=0.7, color='orange', label='Boltzmann Sampling', \\\n",
    "                bins=np.arange(score_min,score_max+score_bin,score_bin))\n",
    "    ax.set_xlim(score_min,score_max)\n",
    "    ax_dup.set_ylabel('Density (Boltzmann)')\n",
    "    ax_dup.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "def plot_interface_score_v_total_score(ax, all_scores_df, \n",
    "                                       total_score_min, total_score_max, interface_score_min, interface_score_max):\n",
    "\n",
    "    ax.scatter(all_scores_df['total_score'], all_scores_df['interface_score'],\n",
    "            c=all_scores_df['index'], cmap='coolwarm_r', s=5)\n",
    "    correlation,_ = pearsonr(all_scores_df['total_score'], all_scores_df['interface_score'])\n",
    "    xmin = all_scores_df['total_score'].min()\n",
    "    xmax = all_scores_df['total_score'].max()\n",
    "    z = np.polyfit(all_scores_df['total_score'], all_scores_df['interface_score'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_trendline = np.linspace(xmin, xmax, 100) \n",
    "    ax.plot(x_trendline, p(x_trendline), \"k\")\n",
    "    ax.set_title(f'Pearson r: {correlation:.2f}')\n",
    "    ax.set_xlim(total_score_min,total_score_max)\n",
    "    ax.set_ylim(interface_score_min,interface_score_max)\n",
    "    ax.set_xlabel('Total Score')\n",
    "    ax.set_ylabel('Interface Score')\n",
    "\n",
    "def plot_combined_score_v_index(ax, combined_scores, all_scores_df):\n",
    "    \n",
    "    combined_scores = pd.Series(combined_scores)\n",
    "    moving_avg = combined_scores.rolling(window=20).mean()\n",
    "    ax.scatter(all_scores_df['index'], combined_scores, c='lightgrey', s=5) \n",
    "    ax.axhline(HIGHSCORE, color='b')\n",
    "    ax.axhline(NEG_BEST, color='r')\n",
    "    PARENTS = [i for i in os.listdir(f'{FOLDER_HOME}/{FOLDER_PARENT}') if i[-4:] == \".pdb\"]\n",
    "    ax.axvline(N_PARENT_JOBS*len(PARENTS), color='k')\n",
    "    ax.plot(range(len(moving_avg)),moving_avg,c=\"k\")\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_xlim(0,MAX_DESIGNS)\n",
    "    ax.set_title('Score vs Index')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Combined Score')    \n",
    "    \n",
    "def plot_combined_score_v_generation(ax, combined_scores, all_scores_df):\n",
    "    \n",
    "    all_scores_df['tmp'] = combined_scores\n",
    "    all_scores_df = all_scores_df.dropna(subset=['tmp'])\n",
    "    \n",
    "    max_gen = int(all_scores_df['generation'].max())\n",
    "    boxplot_data = [all_scores_df[all_scores_df['generation'] == gen]['tmp'] for gen in range(0,max_gen+1,1)]\n",
    "    ax.boxplot(boxplot_data, positions=range(len(boxplot_data)))\n",
    "    ax.axhline(HIGHSCORE, color='b')\n",
    "    ax.axhline(NEG_BEST, color='r')\n",
    "    ax.set_xticks(range(len(boxplot_data)))\n",
    "    ax.set_xticklabels(range(0,len(boxplot_data),1))\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_title('Combined Score vs Generations')\n",
    "    ax.set_xlabel('Generation')\n",
    "    ax.set_ylabel('Combined Score')\n",
    "    ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.7)\n",
    "    \n",
    "def plot_mutations_v_generation(ax, all_scores_df,  mut_min, mut_max):\n",
    "    \n",
    "    all_scores_df = all_scores_df.dropna(subset=['mutations'])\n",
    "    \n",
    "    max_gen = int(all_scores_df['generation'].max())\n",
    "    boxplot_data = [all_scores_df[all_scores_df['generation'] == gen]['mutations'] for gen in range(0,max_gen+1,1)]\n",
    "    ax.boxplot(boxplot_data, positions=range(len(boxplot_data)))\n",
    "    ax.axhline(len(DESIGN.split(\",\")), color='r')\n",
    "    ax.set_xticks(range(len(boxplot_data)))\n",
    "    ax.set_xticklabels(range(0,len(boxplot_data),1))\n",
    "    ax.set_ylim(mut_min,mut_max)\n",
    "    ax.set_title('Mutations vs Generations')\n",
    "    ax.set_xlabel('Generation')\n",
    "    ax.set_ylabel('Number of Mutations')\n",
    "    ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.7)\n",
    "    \n",
    "def plot_tree(ax, combined_scores, all_scores_df, G):\n",
    "    \n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "    _, _, _, combined_potentials = normalize_scores(all_scores_df, print_norm=False, norm_all=False, extension=\"potential\")\n",
    "\n",
    "    max_gen = int(all_scores_df['generation'].max())\n",
    "    \n",
    "    def set_node_positions(G, node, pos, x, y, counts):\n",
    "        pos[node] = (x, y)\n",
    "        neighbors = list(G.successors(node))\n",
    "        next_y = y - counts[node] / 2\n",
    "        for neighbor in neighbors:\n",
    "            set_node_positions(G, neighbor, pos, x + 1, next_y + counts[neighbor] / 2, counts)\n",
    "            next_y += counts[neighbor]\n",
    "            \n",
    "    def count_descendants(G, node, counts):\n",
    "        neighbors = list(G.successors(node))\n",
    "        count = 1\n",
    "        for neighbor in neighbors:\n",
    "            count += count_descendants(G, neighbor, counts)\n",
    "        counts[node] = count\n",
    "        return count\n",
    "\n",
    "    counts = {}\n",
    "    count_descendants(G, 0, counts)\n",
    "\n",
    "    pos = {}\n",
    "    set_node_positions(G, 0, pos, 0, 0, counts)\n",
    "    y_values = [y for x, y in pos.values()]\n",
    "    y_span = max(y_values) - min(y_values)  \n",
    "    \n",
    "    colors = combined_potentials\n",
    "    colors[0] = np.nan\n",
    "    normed_colors = [(x - np.nanmin(colors[1:])) / (np.nanmax(colors[1:]) - np.nanmin(colors[1:])) for x in colors]\n",
    "    normed_colors = np.nan_to_num(normed_colors, nan=0)\n",
    "    normed_colors = normed_colors**2\n",
    "\n",
    "    # Draw the graph with the positions set\n",
    "    #nx.draw(G, pos, ax=ax, sld', arrows=False, cmap=plt.cm.coolwarm_r)\n",
    "    for start, end in G.edges():\n",
    "        color = plt.cm.coolwarm_r(normed_colors[end])\n",
    "        if float(normed_colors[end]) == 0.0: color = [0., 0., 0., 1.]\n",
    "        linewidth = 0.1+2*normed_colors[end]\n",
    "        \n",
    "        x0, y0 = pos[start]\n",
    "        x1, y1 = pos[end]\n",
    "        ax.plot([y0, y1], [x0, x1], color=color, linewidth=linewidth)\n",
    "\n",
    "    # Adjust axis labels and ticks for the swapped axes\n",
    "    ax.axis('on')\n",
    "    ax.set_title(\"Colored by Potential\")\n",
    "    ax.set_xlabel(\"Variants\")\n",
    "    ax.set_ylabel(\"Generations\")\n",
    "    ax.set_yticks(range(max_gen+1))\n",
    "    ax.set_ylim(0,max_gen+0.25)\n",
    "    ax.set_yticklabels(range(max_gen+1))\n",
    "\n",
    "    ax.tick_params(axis='y', which='both', bottom=True, top=False, left=True, right=False,\n",
    "                   labelbottom=True, labelleft=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e1fb9",
   "metadata": {},
   "source": [
    "## Functions needed to run AIzyme algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c129bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AIzyme Functions loaded!\")\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2a4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
