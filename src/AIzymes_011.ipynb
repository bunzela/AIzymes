{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba281b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import statistics\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import esm\n",
    "#import mdtraj as md\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB import PDBParser\n",
    "from datetime import datetime\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.colors import Normalize\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from scipy.stats import gmean, pearsonr\n",
    "from scipy.optimize import curve_fit\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Setting initial options\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None \n",
    "matplotlib_axes_logger.setLevel('INFO')\n",
    "\n",
    "DESIGN_COUNT = {}\n",
    "\n",
    "if GRID:\n",
    "    USERNAME = os.getlogin()\n",
    "#     FOLDER_HOME = f'{os.getcwd()}/{DESIGN_FOLDER}'\n",
    "# if BLUEPEBBLE:\n",
    "#     FOLDER_HOME = f'{os.getcwd()}/{DESIGN_FOLDER}'\n",
    "# if BACKGROUND_JOB:\n",
    "#     FOLDER_HOME = f'{os.getcwd()}/{DESIGN_FOLDER}'\n",
    "\n",
    "FOLDER_HOME = f'{os.getcwd()}/{DESIGN_FOLDER}'\n",
    "\n",
    "os.makedirs(FOLDER_HOME, exist_ok=True)\n",
    "FOLDER_INPUT = f'{os.getcwd()}/Input'\n",
    "if not os.path.isdir(FOLDER_INPUT): print(\"ERROR! Input folder missing!\")\n",
    "LOG_FILE = f'{FOLDER_HOME}.log'\n",
    "ALL_SCORES_CSV = f'{FOLDER_HOME}/all_scores.csv'\n",
    "VARIABLES_JSON  = f'{FOLDER_HOME}/variables.json'\n",
    "\n",
    "# Configure logging file\n",
    "log_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "# Remove all handlers associated with the root logger\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Basic configuration for logging to a file\n",
    "logging.basicConfig(filename=LOG_FILE, level=logging.DEBUG, format=log_format, datefmt=date_format)\n",
    "#logging.basicConfig(filename=LOG_FILE, level=logging.INFO, format=log_format, datefmt=date_format)\n",
    "\n",
    "# Create a StreamHandler for console output\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(logging.Formatter(log_format, datefmt=date_format))\n",
    "\n",
    "# Add the console handler to the root logger\n",
    "logging.getLogger().addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200550d5",
   "metadata": {},
   "source": [
    "# main functions - running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e896b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def controller(RESET=False, EXPLORE=False, PROMPT=True, UNBLOCK_ALL=False, \n",
    "               PRINT_VAR=True, PLOT_DATA=True, \n",
    "               BLUEPEBBLE=False, GRID=True):\n",
    "    \n",
    "    # Main AI.zymes functions. Controls the whole design process\n",
    "\n",
    "    # Startup, will only be executed once in the beginning\n",
    "    setup_aizymes(RESET, EXPLORE, PROMPT) \n",
    "    \n",
    "    # Check if Startup is done, if done, read in all_scores_df\n",
    "    all_scores_df = startup_controller(UNBLOCK_ALL, \n",
    "                                       RESET,\n",
    "                                       PRINT_VAR=PRINT_VAR, \n",
    "                                       PLOT_DATA=PLOT_DATA)\n",
    "    \n",
    "    while not os.path.exists(os.path.join(FOLDER_HOME, str(MAX_DESIGNS))):\n",
    "\n",
    "        # Check how many jobs are currently running\n",
    "        # **\n",
    "        num_running_jobs = check_running_jobs()\n",
    "        \n",
    "        if num_running_jobs >= MAX_JOBS: \n",
    "            all_scores_df = update_scores(all_scores_df)\n",
    "            time.sleep(20)\n",
    "            \n",
    "        else:\n",
    "                        \n",
    "            # Update scores\n",
    "            all_scores_df = update_scores(all_scores_df)\n",
    "            \n",
    "            # Check if parent designs are done, if not, start design\n",
    "            parent_done, all_scores_df = start_parent_design(all_scores_df)\n",
    "\n",
    "            if parent_done:\n",
    "                # Boltzmann Selection\n",
    "                selected_index = boltzmann_selection(all_scores_df)\n",
    "\n",
    "                # Decide Fate of selected index\n",
    "                if selected_index is not None:\n",
    "                    all_scores_df = start_calculation(all_scores_df, selected_index)\n",
    "    \n",
    "        time.sleep(1)\n",
    "    \n",
    "    all_scores_df = update_scores(all_scores_df)\n",
    "    print(f\"Stopped because {os.path.join(FOLDER_HOME, str(MAX_DESIGNS))} exists.\")\n",
    "\n",
    "def check_running_jobs():\n",
    "    \n",
    "    if GRID:\n",
    "        jobs = subprocess.check_output([\"qstat\", \"-u\", USERNAME]).decode(\"utf-8\").split(\"\\n\")\n",
    "        jobs = [job for job in jobs if SUBMIT_PREFIX in job]\n",
    "        return len(jobs)\n",
    "        \n",
    "    if BLUEPEBBLE:\n",
    "        jobs = subprocess.check_output([\"squeue\",\"--me\"]).decode(\"utf-8\").split(\"\\n\")\n",
    "        jobs = [job for job in jobs if SUBMIT_PREFIX in job]\n",
    "        return len(jobs)\n",
    "        \n",
    "    if BACKGROUND_JOB:\n",
    "        with open(f'{FOLDER_HOME}/n_running_jobs.dat', 'r'): jobs = int(f.read())\n",
    "        return jobs\n",
    "    \n",
    "    if ABBIE_LOCAL:\n",
    "        return 0\n",
    "\n",
    "def update_potential(score_type:str, index:int, all_scores_df:pd.DataFrame):\n",
    "    '''Creates a <score_type>_potential.dat file in FOLDER_HOME/<index> \n",
    "    If latest score comes from Rosetta Relax - then the score for variant <index>\n",
    "    will be added to the <score_type>_potential.dat of the parent index \n",
    "    and the <score_type>_potential value of the dataframe for the parent \n",
    "    will be updated with the average of the parent and child scores. \n",
    "    Parameters:\n",
    "    - score_type(str): Type of score to update, one of these options: total, interface, catalytic, efield\n",
    "    - index (int): variant index to update\n",
    "    - all_scores_df (pd.DataFrame): scores dataframe\n",
    "    Returns: \n",
    "    - all_scores_df (pd.DataFrame): updated dataframe'''\n",
    "    \n",
    "    score = all_scores_df.at[index, f'{score_type}_score']\n",
    "\n",
    "    score_taken_from = all_scores_df.at[index, 'score_taken_from']\n",
    "\n",
    "    parent_index = all_scores_df.at[index, \"parent_index\"] #parent index is stored as a string because can be \"Parent\"\n",
    "\n",
    "    filename = f\"{FOLDER_HOME}/{index}/{score_type}_potential.dat\"\n",
    "\n",
    "    parent_filename = f\"{FOLDER_HOME}/{parent_index}/{score_type}_potential.dat\"\n",
    "\n",
    "    ## overwrites contents of filename - always overwrite the current index - only append for parent\n",
    "    with open(filename, \"w\") as f: \n",
    "            f.write(str(score))\n",
    "\n",
    "    all_scores_df.at[index, f'{score_type}_potential'] = score\n",
    "\n",
    "    if score_taken_from == \"Relax\" and parent_index != \"Parent\":\n",
    "        #Add new index scores to parent potential file (unless parent index is \"Parent\")\n",
    "\n",
    "        ## appends to parent_filename\n",
    "        with open(parent_filename, \"a\") as f: \n",
    "            f.write(f\"\\n{str(score)}\")\n",
    "        with open(parent_filename, \"r\") as f:\n",
    "            potentials = f.readlines()\n",
    "        \n",
    "        all_scores_df.at[parent_index, f'{score_type}_potential'] = np.average([float(i) for i in potentials])\n",
    "\n",
    "    all_scores_df = all_scores_df.dropna(subset=['index'])\n",
    "    \n",
    "    return all_scores_df\n",
    "        \n",
    "def update_scores(all_scores_df):\n",
    "    \n",
    "    for _, row in all_scores_df.iterrows():\n",
    "\n",
    "        index = int(row['index'])\n",
    "        parent_index = row['parent_index']\n",
    "          \n",
    "        # do NOT update score if score was taken from a relax file. Prevents repeated scoring!\n",
    "        if row['score_taken_from'] == 'Relax': continue\n",
    "            \n",
    "            \n",
    "        # default scorefile path\n",
    "        score_file_path = f\"{FOLDER_HOME}/{int(index)}/score_rosetta_relax.sc\"\n",
    "        \n",
    "        # default pdb file path\n",
    "        if row[\"design_method\"] == \"ProteinMPNN\":\n",
    "            pdb_path = f\"{FOLDER_HOME}/{int(index)}/{WT}_Rosetta_Relax_{int(index)}.pdb\"\n",
    "        else:\n",
    "            pdb_path = f\"{FOLDER_HOME}/{int(index)}/{WT}_Rosetta_Design_{int(index)}.pdb\"\n",
    "        \n",
    "        if not os.path.exists(score_file_path):\n",
    "\n",
    "            # change scorefile path if run is a RosettaDesign and if score_rosetta_relax.sc does not exist\n",
    "            if row['design_method'] == \"RosettaDesign\":\n",
    "                score_file_path = f\"{FOLDER_HOME}/{int(index)}/score_rosetta_design.sc\" \n",
    "\n",
    "                # do NOT update score if score was taken from a design file. Prevents repeated scoring!\n",
    "                if row['score_taken_from'] == 'Design': continue\n",
    "\n",
    "        # do NOT update score if score_file_path does not exist. Usually means job is not done.\n",
    "        if not os.access(score_file_path, os.R_OK) or not os.access(pdb_path, os.R_OK): continue\n",
    "  \n",
    "        if score_file_path == f\"{FOLDER_HOME}/{int(index)}/score_rosetta_relax.sc\":\n",
    "            all_scores_df.at[index, 'score_taken_from'] = 'Relax'\n",
    "        if score_file_path == f\"{FOLDER_HOME}/{int(index)}/score_rosetta_design.sc\":\n",
    "            all_scores_df.at[index, 'score_taken_from'] = 'Design'\n",
    "           \n",
    "        with open(score_file_path, \"r\") as f: scores = f.readlines()\n",
    "        \n",
    "        if len(scores) < 3: continue # if the timing is bad, the score file is not fully written. Check if len(scores) > 2!\n",
    "        \n",
    "        headers = scores[1].split()\n",
    "        scores  = scores[2].split()\n",
    "\n",
    "        catalytic_score = 0.0\n",
    "        interface_score = 0.0\n",
    "        for idx_headers, header in enumerate(headers):\n",
    "            if header == 'total_score':                total_score      = float(scores[idx_headers])\n",
    "            if header == 'interface_delta_X':          interface_score += float(scores[idx_headers])\n",
    "            if header in ['if_X_angle_constraint', \n",
    "                          'if_X_atom_pair_constraint', \n",
    "                          'if_X_dihedral_constraint']: interface_score -= float(scores[idx_headers])   \n",
    "            #Use 6-3-2 weighting when calculating the catalytic score\n",
    "            if header in ['atom_pair_constraint']:     catalytic_score += float(scores[idx_headers])       \n",
    "            if header in ['angle_constraint']:         catalytic_score += float(scores[idx_headers])       \n",
    "            if header in ['dihedral_constraint']:      catalytic_score += float(scores[idx_headers])  \n",
    "\n",
    "        efield_score, index_efields_dict = calc_efields_score(pdb_path)  \n",
    "\n",
    "        update_efieldsdf(index, index_efields_dict)              \n",
    "\n",
    "        # Update scores\n",
    "        all_scores_df.at[index, 'total_score']     = total_score\n",
    "        all_scores_df.at[index, 'interface_score'] = interface_score                \n",
    "        all_scores_df.at[index, 'catalytic_score'] = catalytic_score\n",
    "        all_scores_df.at[index, 'efield_score'] = efield_score\n",
    "        \n",
    "        # This is just for book keeping. AIzymes will always use the most up_to_date scores saved above\n",
    "        if score_file_path == f\"{FOLDER_HOME}/{int(index)}/score_rosetta_relax.sc\":\n",
    "            all_scores_df.at[index, 'relax_total_score']     = total_score\n",
    "            all_scores_df.at[index, 'relax_interface_score'] = interface_score                \n",
    "            all_scores_df.at[index, 'relax_catalytic_score'] = catalytic_score\n",
    "            all_scores_df.at[index, 'relax_efield_score'] = efield_score\n",
    "        if score_file_path == f\"{FOLDER_HOME}/{int(index)}/score_rosetta_design.sc\":\n",
    "            all_scores_df.at[index, 'design_total_score']     = total_score\n",
    "            all_scores_df.at[index, 'design_interface_score'] = interface_score                \n",
    "            all_scores_df.at[index, 'design_catalytic_score'] = catalytic_score\n",
    "            all_scores_df.at[index, 'design_efield_score'] = efield_score\n",
    "        \n",
    "        for score_type in ['total', 'interface', 'catalytic', 'efield']:     \n",
    "\n",
    "            all_scores_df = update_potential(score_type = score_type,\n",
    "                                             index= index, \n",
    "                                             all_scores_df = all_scores_df,)   \n",
    "\n",
    "        logging.info(f\"Updated scores and potentials of index {int(index)}.\")\n",
    "        if all_scores_df.at[index, 'score_taken_from'] == 'Relax' and all_scores_df.at[index, 'parent_index'] != \"Parent\":\n",
    "            logging.info(f\"Adjusted potentials of index parent {all_scores_df.at[index, 'parent_index']} (parent of index {int(index)}).\")\n",
    "        \n",
    "        #unblock index if relaxed file exists\n",
    "        if all_scores_df.at[int(index), \"blocked\"] == True:\n",
    "            if f\"{WT}_Rosetta_Relax_{int(index)}.pdb\" in os.listdir(os.path.join(FOLDER_HOME, str(int(index)))):\n",
    "                all_scores_df.at[index, \"blocked\"] = False\n",
    "                logging.debug(f\"Unblocked index {int(index)}.\")\n",
    "\n",
    "     \n",
    "        # Update catalytic residues\n",
    "        all_scores_df = save_cat_res_into_all_scores_df(all_scores_df, index, pdb_path)\n",
    "\n",
    "        # Update sequence and mutations\n",
    "        reference_sequence = extract_sequence_from_pdb(f\"{FOLDER_INPUT}/{WT}.pdb\")\n",
    "        current_sequence = extract_sequence_from_pdb(pdb_path)\n",
    "        mutations = sum(1 for a, b in zip(current_sequence, reference_sequence) if a != b)\n",
    "        all_scores_df['sequence'] = all_scores_df['sequence'].astype('object')\n",
    "        all_scores_df.at[index, 'sequence']  = current_sequence\n",
    "        all_scores_df.at[index, 'mutations'] = int(mutations)\n",
    "\n",
    "\n",
    "        if index % 1000 == 0:\n",
    "            save_all_scores_df(all_scores_df)\n",
    "    \n",
    "    save_all_scores_df(all_scores_df)\n",
    "\n",
    "    return all_scores_df\n",
    "\n",
    "def normalize_scores(unblocked_all_scores_df, print_norm=False, norm_all=False, extension=\"score\"):\n",
    "    \n",
    "    def neg_norm_array(array, score_type):\n",
    "\n",
    "        if len(array) > 1:  ##check that it's not only one value\n",
    "            \n",
    "            array    = -array\n",
    "            \n",
    "            if norm_all:\n",
    "                if print_norm:\n",
    "                    print(score_type,NORM[score_type],end=\" \")\n",
    "                array = (array-NORM[score_type][0])/(NORM[score_type][1]-NORM[score_type][0])\n",
    "                array[array < 0] = 0.0\n",
    "                if np.any(array > 1.0): print(\"\\nNORMALIZATION ERROR!\",score_type,\"has a value >1!\") \n",
    "            else:\n",
    "                if print_norm:\n",
    "                    print(score_type,[np.mean(array),np.std(array)],end=\" \")\n",
    "                # Normalize using mean and standard deviation\n",
    "                if np.std(array) == 0:\n",
    "                    array = np.where(np.isnan(array), array, 0.0)  # Handle case where all values are the same\n",
    "                else:\n",
    "                    array = (array - np.mean(array)) / np.std(array)\n",
    "\n",
    "            return array\n",
    "        \n",
    "        else:\n",
    "            # do not normalize if array only contains 1 value\n",
    "            return [1]\n",
    "         \n",
    "    catalytic_scores    = unblocked_all_scores_df[f\"catalytic_{extension}\"]\n",
    "    catalytic_scores    = neg_norm_array(catalytic_scores, f\"catalytic_{extension}\")   \n",
    "    \n",
    "    total_scores        = unblocked_all_scores_df[f\"total_{extension}\"]\n",
    "    total_scores        = neg_norm_array(total_scores, f\"total_{extension}\")   \n",
    "    \n",
    "    interface_scores    = unblocked_all_scores_df[f\"interface_{extension}\"]\n",
    "    interface_scores    = neg_norm_array(interface_scores, f\"interface_{extension}\")  \n",
    "    \n",
    "    efield_scores    = unblocked_all_scores_df[f\"efield_{extension}\"]   ### to be worked on\n",
    "    efield_scores    = neg_norm_array(-1*efield_scores, f\"efield_{extension}\")   ### to be worked on, with MINUS here\n",
    "    \n",
    "    if len(total_scores) == 0:\n",
    "        combined_scores = []\n",
    "    else:\n",
    "        combined_scores     = np.stack((total_scores, interface_scores, efield_scores))\n",
    "        combined_scores     = np.mean(combined_scores, axis=0)\n",
    "        \n",
    "          \n",
    "    if print_norm:\n",
    "        if combined_scores.size > 0:\n",
    "            print(\"HIGHSCORE:\",\"{:.2f}\".format(np.amax(combined_scores)),end=\" \")\n",
    "            print(\"Designs:\",len(combined_scores),end=\" \")\n",
    "            PARENTS = [i for i in os.listdir(f'{FOLDER_HOME}/{FOLDER_PARENT}') if i[-4:] == \".pdb\"]\n",
    "            print(\"Parents:\",len(PARENTS))\n",
    "        \n",
    "    return catalytic_scores, total_scores, interface_scores, efield_scores, combined_scores\n",
    "        \n",
    "def boltzmann_selection(all_scores_df):\n",
    "\n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "    parent_indices  = set(all_scores_df['parent_index'].astype(str).values)\n",
    "    all_scores_df = all_scores_df[all_scores_df[\"blocked\"] == False] # Remove blocked indices\n",
    "       \n",
    "    all_scores_df = all_scores_df.dropna(subset=['total_score'])     # Remove indices without score (design running)\n",
    "    \n",
    "    # Drop catalytic scroes > mean + 1 std\n",
    "    mean_catalytic_score = all_scores_df['catalytic_score'].mean()\n",
    "    std_catalytic_score = all_scores_df['catalytic_score'].std()\n",
    "    if len(all_scores_df) > 10:\n",
    "        all_scores_df = all_scores_df[all_scores_df['catalytic_score'] < mean_catalytic_score + std_catalytic_score]\n",
    "    \n",
    "    # If there are structures that ran through RosettaRelax but have never been used for design, run 1 design (exclude PMPNN as it is always relaxed)\n",
    "    relaxed_indices = all_scores_df[(all_scores_df['score_taken_from'] == 'Relax') & (all_scores_df['design_method'] != 'ProteinMPNN')]\n",
    "    relaxed_indices = [str(i) for i in relaxed_indices.index]\n",
    "    filtered_indices = [index for index in relaxed_indices if index not in parent_indices]\n",
    "\n",
    "\n",
    "    if len(filtered_indices) >= 1:\n",
    "        selected_index = filtered_indices[0]\n",
    "        logging.info(f\"{selected_index} selected because its relaxed but nothing was designed from it.\")\n",
    "        return int(selected_index)\n",
    "                \n",
    "    # Do Boltzmann Selection if some scores exist\n",
    "    _, _, _, _, combined_potentials = normalize_scores(all_scores_df, norm_all=False, \\\n",
    "                                                    extension=\"potential\", print_norm = False) \n",
    "        \n",
    "    if len(combined_potentials) > 0:\n",
    "        \n",
    "        if isinstance(KBT_BOLTZMANN, (float, int)):\n",
    "            kbt_boltzmann = KBT_BOLTZMANN\n",
    "        elif len(KBT_BOLTZMANN) > 2:\n",
    "            logging.error(f\"KBT_BOLTZMANN must either be a single value or list of two values.\")\n",
    "            logging.error(f\"KBT_BOLTZMANN is {KBT_BOLTZMANN}\")\n",
    "        else:\n",
    "            # Ramp down kbT_boltzmann over time (i.e., with increaseing indices)\n",
    "            # datapoints = legth of all_scores_df - number of parents generated\n",
    "            num_pdb_files = len([file for file in os.listdir(f'{FOLDER_HOME}/{FOLDER_PARENT}') if file.endswith('.pdb')])\n",
    "            datapoints = max(all_scores_df['index'].max() +1 - num_pdb_files*N_PARENT_JOBS, 0)\n",
    "            kbt_boltzmann = max(KBT_BOLTZMANN[0] * np.exp(-KBT_BOLTZMANN[1]*datapoints), 0.05)\n",
    "        boltzmann_factors = np.exp(combined_potentials / (kbt_boltzmann))\n",
    "        probabilities = boltzmann_factors / sum(boltzmann_factors)\n",
    "        \n",
    "        #selected_index = int(np.random.choice(np.array(all_scores_df[\"index\"].tolist()), p=probabilities))\n",
    "        if len(all_scores_df[\"index\"] > 0):\n",
    "            selected_index = int(np.random.choice(all_scores_df[\"index\"].to_numpy(), p=probabilities))\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        selected_index = 0\n",
    "\n",
    "    return selected_index\n",
    "\n",
    "def start_parent_design(all_scores_df):\n",
    "\n",
    "    number_of_indices = len(all_scores_df)\n",
    "    PARENTS = [i for i in os.listdir(f'{FOLDER_HOME}/{FOLDER_PARENT}') if i[-4:] == \".pdb\"]\n",
    "    \n",
    "    if number_of_indices < N_PARENT_JOBS * len(PARENTS):\n",
    "        \n",
    "        parent_done = False\n",
    "        selected_index = int(number_of_indices / N_PARENT_JOBS)\n",
    "        parent = PARENTS[selected_index][:-4]\n",
    "        \n",
    "        new_index, all_scores_df = create_new_index(parent_index=\"Parent\", all_scores_df=all_scores_df)\n",
    "        all_scores_df['design_method'] = all_scores_df['design_method'].astype('object') #?\n",
    "        all_scores_df.at[new_index, 'design_method'] = \"RosettaDesign\"\n",
    "        all_scores_df['luca'] = all_scores_df['luca'].astype('object') #?\n",
    "        all_scores_df.at[new_index, 'luca'] = parent\n",
    "        \n",
    "        #Add cat res to new entry\n",
    "        all_scores_df = save_cat_res_into_all_scores_df(all_scores_df, new_index, \n",
    "                                                        f'{FOLDER_HOME}/{FOLDER_PARENT}/{PARENTS[selected_index]}',\n",
    "                                                        from_parent_struct=True)\n",
    "        \n",
    "        run_RosettaDesign(parent_index=parent, new_index=new_index, all_scores_df=all_scores_df, parent_done=parent_done)                      \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        parent_done = True\n",
    "    \n",
    "    save_all_scores_df(all_scores_df)\n",
    "    return parent_done, all_scores_df\n",
    "\n",
    "# Decides what to do with selected index\n",
    "def start_calculation(all_scores_df, selected_index):\n",
    "    \n",
    "    logging.debug(f\"Starting new calculation for index {selected_index}.\")\n",
    "\n",
    "    blocked = False\n",
    "    if all_scores_df.at[selected_index, \"blocked\"] == True:\n",
    "        blocked = True\n",
    "        \n",
    "    relaxed = False\n",
    "    if f\"{WT}_Rosetta_Relax_{selected_index}.pdb\" in os.listdir(os.path.join(FOLDER_HOME, str(selected_index))):\n",
    "        relaxed = True\n",
    "\n",
    "    # Check if ESMfold_Rosetta_Relax is done\n",
    "    if relaxed:\n",
    "\n",
    "        # ESMfold_Rosetta_Relax is done, create a new index\n",
    "        new_index, all_scores_df = create_new_index(parent_index=selected_index, all_scores_df=all_scores_df)\n",
    "\n",
    "        #####\n",
    "        # Here, we can add an AI to decide on the next steps\n",
    "        #####\n",
    "\n",
    "        # Run Rosetta_Design with new_index\n",
    "        rand = random.random()\n",
    "        if rand < ProteinMPNN_PROB:  \n",
    "            all_scores_df.at[new_index, 'design_method'] = \"ProteinMPNN\"\n",
    "            run_ProteinMPNN(parent_index=selected_index, new_index=new_index, all_scores_df=all_scores_df) \n",
    "        elif rand < ProteinMPNN_PROB + LMPNN_PROB:\n",
    "            all_scores_df.at[new_index, 'design_method'] = \"LigandMPNN\"\n",
    "            run_LigandMPNN(parent_index=selected_index, new_index=new_index, all_scores_df=all_scores_df) \n",
    "        else:                    \n",
    "            all_scores_df.at[new_index, 'design_method'] = \"RosettaDesign\"\n",
    "            run_RosettaDesign(parent_index=selected_index, new_index=new_index, all_scores_df=all_scores_df) \n",
    "        save_all_scores_df(all_scores_df)\n",
    "        \n",
    "    else:\n",
    "        # ESMfold_Rosetta_Relax is not done, check if Index is blocked\n",
    "        if blocked:\n",
    "            # Blocked --> ESMfold_Rosetta_Relax is still running, do not do antyting. This shouldn't happen!\n",
    "            logging.error(f\"Index {selected_index} is being worked on. Skipping index.\")\n",
    "            logging.error(f\"Note: This should not happen! Check blocking and Boltzman selection.\")\n",
    "        else:\n",
    "            # Not blocked --> submit ESMfold_Rosetta_Relax and block index\n",
    "            logging.info(f\"Index {selected_index} has no relaxed structure, starting ESMfold_Rosetta_Relax.\")\n",
    "            submitted_status = run_ESMfold_RosettaRelax(index=selected_index, all_scores_df=all_scores_df, \\\n",
    "                                                        OnlyRelax=True, EXPLORE=EXPLORE)\n",
    "            \n",
    "            #Check if submission executed correctly before blocking\n",
    "            if submitted_status:\n",
    "                all_scores_df.at[selected_index, \"blocked\"] = True\n",
    "\n",
    "        save_all_scores_df(all_scores_df)\n",
    "        \n",
    "    return all_scores_df\n",
    "\n",
    "def create_new_index(parent_index, all_scores_df):\n",
    "    \n",
    "    # Create a new line with the next index and parent_index\n",
    "    new_index = len(all_scores_df)\n",
    "    \n",
    "    # Append the new line to the DataFrame and save to  all_scores_df.csv\n",
    "    if isinstance(KBT_BOLTZMANN, (float, int)):\n",
    "        kbt_boltzmann = KBT_BOLTZMANN\n",
    "    elif len(KBT_BOLTZMANN) == 2:\n",
    "        num_pdb_files = len([file for file in os.listdir(f'{FOLDER_HOME}/{FOLDER_PARENT}') if file.endswith('.pdb')])\n",
    "        datapoints = max(all_scores_df['index'].max() +1 - num_pdb_files*N_PARENT_JOBS, 0)\n",
    "        kbt_boltzmann = max(KBT_BOLTZMANN[0] * np.exp(-KBT_BOLTZMANN[1]*datapoints), 0.05)\n",
    "    if parent_index == 'Parent':\n",
    "        generation = 0\n",
    "        luca = \"x\"\n",
    "    else:\n",
    "        generation = all_scores_df['generation'][int(parent_index)]+1\n",
    "        luca       = all_scores_df['luca'][int(parent_index)]\n",
    "        \n",
    "    # all_scores_df = all_scores_df.append({'index': new_index, \n",
    "    #                                       'parent_index': parent_index,\n",
    "    #                                       'kbt_boltzmann': kbt_boltzmann,\n",
    "    #                                       'generation': generation,\n",
    "    #                                       'luca': luca,\n",
    "    #                                       'blocked': False,\n",
    "    #                                       }, ignore_index=True)\n",
    "    \n",
    "    # why do this in two steps?\n",
    "    new_index_df = pd.DataFrame({'index': int(new_index), \n",
    "                                'parent_index': parent_index,\n",
    "                                'kbt_boltzmann': kbt_boltzmann,\n",
    "                                'generation': generation,\n",
    "                                'luca': luca,\n",
    "                                'blocked': False,\n",
    "                                }, index = [0])\n",
    "    all_scores_df = pd.concat([all_scores_df, new_index_df], ignore_index=True)\n",
    "\n",
    "    save_all_scores_df(all_scores_df)\n",
    "\n",
    "    # Create the folders for the new index\n",
    "    os.makedirs(f\"{FOLDER_HOME}/{new_index}/scripts\", exist_ok=True)\n",
    "           \n",
    "    logging.debug(f\"Child index {new_index} created for {parent_index}.\")\n",
    "    \n",
    "    return new_index, all_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152761b",
   "metadata": {},
   "source": [
    "# main functions - design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5725702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ESMfold_RosettaRelax(index, all_scores_df, OnlyRelax=False, ProteinMPNN=False, PreMatchRelax=False,\n",
    "                             ProteinMPNN_parent_index=0, cmd=\"\", bash=False, EXPLORE=False):\n",
    "    \n",
    "    # Giving the ESMfold algorihm the needed inputs\n",
    "    output_file = f'{FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_output_{index}.pdb'\n",
    "\n",
    "    protein_mpnn_seq_file = f'{FOLDER_HOME}/{index}/ProteinMPNN/{WT}_{index}.seq'\n",
    "    ligand_mpnn_seq_file = f'{FOLDER_HOME}/{index}/LigandMPNN/{WT}_{index}.seq'\n",
    "    esmfold_seq_file = f'{FOLDER_HOME}/{index}/ESMfold/{WT}_{index}.seq'\n",
    "\n",
    "    if ProteinMPNN:\n",
    "        if os.path.exists(protein_mpnn_seq_file):\n",
    "            sequence_file = protein_mpnn_seq_file\n",
    "        elif os.path.exists(ligand_mpnn_seq_file):\n",
    "            sequence_file = ligand_mpnn_seq_file\n",
    "        else:\n",
    "            logging.error(f\"Neither {protein_mpnn_seq_file} nor {ligand_mpnn_seq_file} exists, but ProteinMPNN was True.\")\n",
    "            return\n",
    "    else:\n",
    "        sequence_file = esmfold_seq_file\n",
    "\n",
    "        \n",
    "    # Make directories\n",
    "    os.makedirs(f\"{FOLDER_HOME}/{index}/ESMfold\", exist_ok=True)\n",
    "    os.makedirs(f\"{FOLDER_HOME}/{index}/scripts\", exist_ok=True)\n",
    "\n",
    "        \n",
    "    # Options for EXPLORE, accelerated script for testing\n",
    "    ex = \"-ex1 -ex2\"\n",
    "    if EXPLORE: ex = \"\"\n",
    "        \n",
    "    # Get Name of parent PDB\n",
    "    if OnlyRelax: \n",
    "        PDBFile = f\"{FOLDER_HOME}/{index}/{WT}_Rosetta_Design_{index}.pdb\"\n",
    "    elif PreMatchRelax: \n",
    "        PDBFile = f\"{FOLDER_INPUT}/{WT}.pdb\"\n",
    "    elif ProteinMPNN:\n",
    "        PDBFile = f\"{FOLDER_HOME}/{ProteinMPNN_parent_index}/{WT}_Rosetta_Relax_{ProteinMPNN_parent_index}.pdb\"\n",
    "    else:\n",
    "        print(\"I don't know what you want me to do\")\n",
    "        return\n",
    "    if not os.path.isfile(PDBFile):\n",
    "        logging.error(f\"{PDBFile} not present!\")\n",
    "        return False\n",
    "\n",
    "    # Make sequence file\n",
    "    if OnlyRelax or PreMatchRelax: \n",
    "        seq = extract_sequence_from_pdb(PDBFile)\n",
    "        with open(f\"{FOLDER_HOME}/{index}/ESMfold/{WT}_{index}.seq\",\"w\") as f: f.write(seq)\n",
    "                    \n",
    "    # Get the pdb file from the last step and strip away ligand and hydrogens \n",
    "    cpptraj = f'''parm    {PDBFile}\n",
    "trajin  {PDBFile}\n",
    "strip   :{LIGAND}\n",
    "strip   !@C,N,O,CA\n",
    "trajout {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Apo_{index}.pdb\n",
    "'''\n",
    "    with open(f'{FOLDER_HOME}/{index}/ESMfold/CPPTraj_Apo_{index}.in','w') as f: f.write(cpptraj)\n",
    "\n",
    "    # Get the pdb file from the last step and strip away everything except the ligand\n",
    "    cpptraj = f'''parm    {PDBFile}\n",
    "trajin  {PDBFile}\n",
    "strip   !:{LIGAND}\n",
    "trajout {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Lig_{index}.pdb\n",
    "'''\n",
    "    with open(f'{FOLDER_HOME}/{index}/ESMfold/CPPTraj_Lig_{index}.in','w') as f: f.write(cpptraj)\n",
    "\n",
    "    # Get the ESMfold pdb file and strip away all hydrogens\n",
    "    cpptraj = f'''parm    {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_output_{index}.pdb\n",
    "trajin  {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_output_{index}.pdb\n",
    "strip   !@C,N,O,CA\n",
    "trajout {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_no_hydrogens_{index}.pdb\n",
    "'''\n",
    "    with open(f'{FOLDER_HOME}/{index}/ESMfold/CPPTraj_no_hydrogens_{index}.in','w') as f: f.write(cpptraj)\n",
    "\n",
    "    # Align substrate and ESM prediction of scaffold without hydrogens\n",
    "    cpptraj = f'''parm    {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_no_hydrogens_{index}.pdb\n",
    "reference {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Apo_{index}.pdb [apo]\n",
    "trajin    {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_no_hydrogens_{index}.pdb\n",
    "rmsd      @CA ref [apo]\n",
    "trajout   {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_aligned_{index}.pdb noter\n",
    "'''\n",
    "    with open(f'{FOLDER_HOME}/{index}/ESMfold/CPPTraj_aligned_{index}.in','w') as f: f.write(cpptraj) \n",
    "              \n",
    "    if GRID:           extension = \"linuxgccrelease\"\n",
    "    if BLUEPEBBLE:     extension = \"serialization.linuxgccrelease\"\n",
    "    if BACKGROUND_JOB: extension = \"serialization.linuxgccrelease\"\n",
    "    if ABBIE_LOCAL:    \n",
    "        extension = \"linuxgccrelease\"\n",
    "        bash_args = \"OMP_NUM_THREADS=1\"\n",
    "    else:\n",
    "        bash_args = \"\"\n",
    " \n",
    "    cmd += f\"\"\"\n",
    "    \n",
    "{bash_args} python {FOLDER_HOME}/ESMfold.py {output_file} {sequence_file}\n",
    "\n",
    "sed -i '/PARENT N\\/A/d' {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_output_{index}.pdb\n",
    "cpptraj -i {FOLDER_HOME}/{index}/ESMfold/CPPTraj_Apo_{index}.in           &> \\\n",
    "           {FOLDER_HOME}/{index}/ESMfold/CPPTraj_Apo_{index}.out\n",
    "cpptraj -i {FOLDER_HOME}/{index}/ESMfold/CPPTraj_Lig_{index}.in           &> \\\n",
    "           {FOLDER_HOME}/{index}/ESMfold/CPPTraj_Lig_{index}.out\n",
    "cpptraj -i {FOLDER_HOME}/{index}/ESMfold/CPPTraj_no_hydrogens_{index}.in  &> \\\n",
    "           {FOLDER_HOME}/{index}/ESMfold/CPPTraj_no_hydrogens_{index}.out\n",
    "cpptraj -i {FOLDER_HOME}/{index}/ESMfold/CPPTraj_aligned_{index}.in       &> \\\n",
    "           {FOLDER_HOME}/{index}/ESMfold/CPPTraj_aligned_{index}.out\n",
    "\n",
    "# Assemble the final protein\n",
    "sed -i '/END/d' {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_aligned_{index}.pdb\n",
    "# Return HETATM to ligand output and remove TER\n",
    "sed -i -e 's/^ATOM  /HETATM/' -e '/^TER/d' {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Lig_{index}.pdb\n",
    "\"\"\"\n",
    "    \n",
    "    input_extension_relax = \"\"\n",
    "    if PreMatchRelax:\n",
    "        extension_relax = \"_APO\"\n",
    "        ## No ligand necessary so just use the aligned pdb from ESMfold\n",
    "        cmd += f\"\"\"\n",
    "cp {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_aligned_{index}.pdb \\\n",
    "   {FOLDER_HOME}/{index}/{WT}_ESMfold_{index}{extension_relax}.pdb\n",
    "\"\"\"  \n",
    "\n",
    "    else:\n",
    "        extension_relax = \"\"\n",
    "        remark = generate_remark_from_all_scores_df(all_scores_df, index)\n",
    "        with open(f'{FOLDER_HOME}/{index}/{WT}_ESMfold_{index}.pdb', 'w') as f: f.write(remark+\"\\n\")\n",
    "        cmd += f\"\"\"\n",
    "cat {FOLDER_HOME}/{index}/ESMfold/{WT}_ESMfold_aligned_{index}.pdb >> {FOLDER_HOME}/{index}/{WT}_ESMfold_{index}.pdb\n",
    "cat {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Lig_{index}.pdb     >> {FOLDER_HOME}/{index}/{WT}_ESMfold_{index}.pdb\n",
    "sed -i '/TER/d' {FOLDER_HOME}/{index}/{WT}_ESMfold_{index}.pdb\n",
    "\"\"\"\n",
    "        \n",
    "    cmd += f\"\"\"\n",
    "# Run Rosetta Relax\n",
    "{ROSETTA_PATH}/bin/rosetta_scripts.{extension} \\\n",
    "                -s                                        {FOLDER_HOME}/{index}/{WT}_ESMfold_{index}{extension_relax}.pdb \\\n",
    "                -extra_res_fa                             {FOLDER_INPUT}/{LIGAND}.params \\\n",
    "                -parser:protocol                          {FOLDER_HOME}/{index}/scripts/Rosetta_Relax_{index}.xml \\\n",
    "                -out:file:scorefile                       {FOLDER_HOME}/{index}/score_rosetta_relax.sc \\\n",
    "                -nstruct                                  1 \\\n",
    "                -ignore_zero_occupancy                    false \\\n",
    "                -corrections::beta_nov16                  true \\\n",
    "                -run:preserve_header                      true \\\n",
    "                -overwrite {ex}\n",
    "\n",
    "# Rename the output file\n",
    "mv {WT}_ESMfold_{index}{extension_relax}_0001.pdb {WT}_Rosetta_Relax_{index}{extension_relax}.pdb\n",
    "sed -i '/        H  /d' {WT}_Rosetta_Relax_{index}{extension_relax}.pdb\n",
    "\"\"\"\n",
    "    \n",
    "    if PreMatchRelax:\n",
    "        extension_relax = \"_APO\"\n",
    "        \n",
    "        cmd += f\"\"\"\n",
    "# Align relaxed ESM prediction of scaffold without hydrogens\n",
    "cpptraj -i {FOLDER_HOME}/{index}/ESMfold/{WT}_Rosetta_Relax_aligned_{index}{extension_relax}.in           &> \\\n",
    "           {FOLDER_HOME}/{index}/ESMfold/{WT}_Rosetta_Relax_aligned_{index}{extension_relax}.out\n",
    "sed -i '/END/d' {FOLDER_HOME}/{index}/{WT}_Rosetta_Relax_aligned_{index}{extension_relax}.pdb\n",
    "\"\"\"  \n",
    "        \n",
    "        cpptraj = f'''parm    {FOLDER_HOME}/{index}/{WT}_Rosetta_Relax_{index}{extension_relax}.pdb [protein]\n",
    "parm      {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Apo_{index}.pdb [reference]\n",
    "reference {FOLDER_HOME}/{index}/ESMfold/{WT}_CPPTraj_Apo_{index}.pdb parm [reference] [apo]\n",
    "trajin    {FOLDER_HOME}/{index}/{WT}_Rosetta_Relax_{index}{extension_relax}.pdb parm [protein]\n",
    "rmsd      @CA ref [apo]\n",
    "trajout   {FOLDER_HOME}/{index}/{WT}_Rosetta_Relax_aligned_{index}{extension_relax}.pdb noter\n",
    "'''\n",
    "        with open(f'{FOLDER_HOME}/{index}/ESMfold/{WT}_Rosetta_Relax_aligned_{index}{extension_relax}.in','w') as f: \n",
    "            f.write(cpptraj) \n",
    "    \n",
    "        # Create the Rosetta_Relax.xml file\n",
    "    repeats = \"3\"\n",
    "    if EXPLORE: repeats = \"1\"\n",
    "    Rosetta_Relax_xml = f\"\"\"\n",
    "<ROSETTASCRIPTS>\n",
    "\n",
    "    <SCOREFXNS>\n",
    "    \n",
    "        <ScoreFunction name      = \"score\"                   weights = \"beta_nov16\" >\n",
    "            <Reweight scoretype  = \"atom_pair_constraint\"    weight  = \"4\" />\n",
    "            <Reweight scoretype  = \"angle_constraint\"        weight  = \"2\" />\n",
    "            <Reweight scoretype  = \"dihedral_constraint\"     weight  = \"1\" />\n",
    "        </ScoreFunction> \n",
    "        \n",
    "        <ScoreFunction name      = \"score_final\"             weights = \"beta_nov16\" >\n",
    "            <Reweight scoretype  = \"atom_pair_constraint\"    weight  = \"4\" />\n",
    "            <Reweight scoretype  = \"angle_constraint\"        weight  = \"2\" />\n",
    "            <Reweight scoretype  = \"dihedral_constraint\"     weight  = \"1\" />\n",
    "        </ScoreFunction>\n",
    "        \n",
    "    </SCOREFXNS>\n",
    "       \n",
    "    <MOVERS>\n",
    "                                  \n",
    "        <FastRelax  name=\"mv_relax\" disable_design=\"false\" repeats=\"{repeats}\" /> \n",
    "\"\"\"\n",
    "    if not PreMatchRelax: Rosetta_Relax_xml += f\"\"\"\n",
    "        <AddOrRemoveMatchCsts     name=\"mv_add_cst\" \n",
    "                                  cst_instruction=\"add_new\" \n",
    "                                  cstfile=\"{FOLDER_INPUT}/{LIGAND}_enzdes.cst\" />\n",
    "\n",
    "\"\"\"\n",
    "    Rosetta_Relax_xml += f\"\"\"\n",
    "\n",
    "        <InterfaceScoreCalculator   name                   = \"mv_inter\" \n",
    "                                    chains                 = \"X\" \n",
    "                                    scorefxn               = \"score_final\" />\n",
    "    </MOVERS>\n",
    "    \n",
    "    <PROTOCOLS>  \n",
    "\n",
    "        <Add mover_name=\"mv_relax\" />\n",
    "\"\"\"\n",
    "    if not PreMatchRelax: Rosetta_Relax_xml += f\"\"\"                                  \n",
    "        <Add mover_name=\"mv_add_cst\" />       \n",
    "        <Add mover_name=\"mv_inter\" />\n",
    "\"\"\"\n",
    "    Rosetta_Relax_xml += f\"\"\"\n",
    "    </PROTOCOLS>\n",
    "    \n",
    "</ROSETTASCRIPTS>\n",
    "\"\"\"\n",
    "    # Write the Rosetta_Relax.xml to a file\n",
    "    with open(f'{FOLDER_HOME}/{index}/scripts/Rosetta_Relax_{index}.xml', 'w') as f:\n",
    "        f.writelines(Rosetta_Relax_xml)      \n",
    "        \n",
    "    if OnlyRelax or PreMatchRelax: \n",
    "        with open(f'{FOLDER_HOME}/{index}/scripts/ESMfold_Rosetta_Relax_{index}.sh', 'w') as file:\n",
    "            file.write(cmd)\n",
    "        logging.info(f\"Run ESMfold & Rosetta_Relax for index {index}.\")\n",
    "        submit_job(index=index, job=\"ESMfold_Rosetta_Relax\", bash=bash)\n",
    "        \n",
    "    if ProteinMPNN:\n",
    "        with open(f'{FOLDER_HOME}/{index}/scripts/ProteinMPNN_ESMfold_Rosetta_Relax_{index}.sh', 'w') as file:\n",
    "            file.write(cmd)\n",
    "        logging.info(f\"Run Ligand/ProteinMPNN for index {index} based on index {ProteinMPNN_parent_index}.\")\n",
    "        submit_job(index=index, job=\"ProteinMPNN_ESMfold_Rosetta_Relax\", bash=bash)\n",
    "\n",
    "    return True\n",
    "        \n",
    "def run_RosettaDesign(parent_index, new_index, all_scores_df, parent_done=True):\n",
    "\n",
    "    # Options for EXPLORE, accelerated script for testing\n",
    "    ex = \"-ex1 -ex2\"\n",
    "    if EXPLORE: ex = \"\"\n",
    "\n",
    "    if GRID:           extension = \"linuxgccrelease\"\n",
    "    if BLUEPEBBLE:     extension = \"serialization.linuxgccrelease\"\n",
    "    if BACKGROUND_JOB: extension = \"serialization.linuxgccrelease\"\n",
    "\n",
    "    if ABBIE_LOCAL:    extension = \"linuxgccrelease\"\n",
    "\n",
    "    if parent_done:\n",
    "        PDB_input  = f'{FOLDER_HOME}/{parent_index}/{WT}_Rosetta_Relax_{parent_index}.pdb'\n",
    "        PDB_output = f'{WT}_Rosetta_Relax_{parent_index}_0001.pdb'\n",
    "    else:\n",
    "        PDB_input  = f'{FOLDER_HOME}/{FOLDER_PARENT}/{parent_index}.pdb'\n",
    "        PDB_output = f'{parent_index}_0001.pdb'\n",
    "        \n",
    "    all_scores_df = save_cat_res_into_all_scores_df(all_scores_df, new_index, PDB_input, from_parent_struct=True)\n",
    "    \n",
    "    cmd = f\"\"\"{ROSETTA_PATH}/bin/rosetta_scripts.{extension}\\\n",
    "    -s                                        {PDB_input} \\\n",
    "    -in:file:native                           {PDB_input} \\\n",
    "    -run:preserve_header                      true \\\n",
    "    -extra_res_fa                             {FOLDER_INPUT}/{LIGAND}.params \\\n",
    "    -parser:protocol                          {FOLDER_HOME}/{new_index}/scripts/Rosetta_Design_{new_index}.xml \\\n",
    "    -out:file:scorefile                       {FOLDER_HOME}/{new_index}/score_rosetta_design.sc \\\n",
    "    -nstruct                                  1  \\\n",
    "    -ignore_zero_occupancy                    false  \\\n",
    "    -corrections::beta_nov16                  true \\\n",
    "    -overwrite {ex}\n",
    "    \n",
    "mv {PDB_output} {WT}_Rosetta_Design_{new_index}.pdb \n",
    "\"\"\"\n",
    "    # Write the shell command to a file\n",
    "    with open(f'{FOLDER_HOME}/{new_index}/scripts/Rosetta_Design_{new_index}.sh','w') as file: file.write(cmd)\n",
    "                \n",
    "    # Create XML script for Rosetta Design  \n",
    "    repeats = \"3\"\n",
    "    if EXPLORE: repeats = \"1\"\n",
    "        \n",
    "    Rosetta_Design_xml = f\"\"\"\n",
    "<ROSETTASCRIPTS>\n",
    "\n",
    "    <SCOREFXNS>\n",
    "\n",
    "        <ScoreFunction            name=\"score\"                           weights=\"beta_nov16\" >  \n",
    "            <Reweight             scoretype=\"atom_pair_constraint\"       weight=\"4\" />\n",
    "            <Reweight             scoretype=\"angle_constraint\"           weight=\"2\" />    \n",
    "            <Reweight             scoretype=\"dihedral_constraint\"        weight=\"1\" />        \n",
    "            <Reweight             scoretype=\"res_type_constraint\"        weight=\"1\" />              \n",
    "        </ScoreFunction>\n",
    "       \n",
    "        <ScoreFunction            name=\"score_unconst\"                   weights=\"beta_nov16\" >        \n",
    "            <Reweight             scoretype=\"atom_pair_constraint\"       weight=\"0\" />\n",
    "            <Reweight             scoretype=\"dihedral_constraint\"        weight=\"0\" />\n",
    "            <Reweight             scoretype=\"angle_constraint\"           weight=\"0\" />              \n",
    "        </ScoreFunction>\n",
    "\n",
    "        <ScoreFunction            name=\"score_final\"                     weights=\"beta_nov16\" >    \n",
    "            <Reweight             scoretype=\"atom_pair_constraint\"       weight=\"4\" />\n",
    "            <Reweight             scoretype=\"angle_constraint\"           weight=\"2\" />    \n",
    "            <Reweight             scoretype=\"dihedral_constraint\"        weight=\"1\" />               \n",
    "        </ScoreFunction>\n",
    "   \n",
    "   </SCOREFXNS>\n",
    "   \n",
    "    <RESIDUE_SELECTORS>\n",
    "   \n",
    "        <Index                    name=\"sel_design\"\n",
    "                                  resnums=\"{DESIGN}\" />\n",
    "\n",
    "        <Index                    name=\"sel_repack\"\n",
    "                                  resnums=\"{REPACK}\" />\n",
    "\"\"\"\n",
    "    \n",
    "    # Add residue number constraints from REMARK (via all_scores_df['cat_resi'])\n",
    "    cat_resis = all_scores_df.at[new_index, 'cat_resi'].split(';')\n",
    "    for idx, cat_resi in enumerate(cat_resis): \n",
    "        \n",
    "        Rosetta_Design_xml += f\"\"\"\n",
    "        <Index                    name=\"sel_cat_{idx}\"\n",
    "                                  resnums=\"{int(cat_resi)}\" />\n",
    "\"\"\"\n",
    "        \n",
    "    Rosetta_Design_xml += f\"\"\"\n",
    "        <Or                       name=\"sel_desrep\"\n",
    "                                  selectors=\"sel_design,sel_repack\" />\n",
    "\n",
    "        <Not                      name=\"sel_nothing\"\n",
    "                                  selector=\"sel_desrep\" />\n",
    "    </RESIDUE_SELECTORS>\n",
    "   \n",
    "    <TASKOPERATIONS>\n",
    "   \n",
    "        <OperateOnResidueSubset   name=\"tsk_design\"                      selector=\"sel_design\" >\n",
    "                                  <RestrictAbsentCanonicalAASRLT         aas=\"GPAVLIMFYWHKRQNEDST\" />\n",
    "        </OperateOnResidueSubset>\n",
    "\"\"\"\n",
    "    \n",
    "    # Add residue identity constraints from constraint file\n",
    "    with open(f'{FOLDER_HOME}/cst.dat', 'r') as f: cat_resns = f.read()\n",
    "    cat_resns = cat_resns.split(\";\")\n",
    "    \n",
    "    for idx, cat_resn in enumerate(cat_resns): \n",
    "        Rosetta_Design_xml += f\"\"\"\n",
    "        <OperateOnResidueSubset   name=\"tsk_cat_{idx}\"                   selector=\"sel_cat_{idx}\" >\n",
    "                                  <RestrictAbsentCanonicalAASRLT         aas=\"{cat_resn}\" />\n",
    "        </OperateOnResidueSubset>\n",
    "\"\"\"\n",
    "    \n",
    "    tsk_cat = []\n",
    "    for idx, cat_res in enumerate(cat_resns): \n",
    "        tsk_cat += [f\"tsk_cat_{idx}\"]\n",
    "    tsk_cat = \",\".join(tsk_cat)\n",
    "        \n",
    "    Rosetta_Design_xml += f\"\"\"\n",
    "       \n",
    "        <OperateOnResidueSubset   name=\"tsk_repack\"                      selector=\"sel_repack\" >\n",
    "                                  <RestrictToRepackingRLT />\n",
    "        </OperateOnResidueSubset>\n",
    "       \n",
    "        <OperateOnResidueSubset   name=\"tsk_nothing\"                     selector=\"sel_nothing\" >\n",
    "                                  <PreventRepackingRLT />\n",
    "        </OperateOnResidueSubset>\n",
    "       \n",
    "    </TASKOPERATIONS>\n",
    "\n",
    "    <FILTERS>\n",
    "   \n",
    "        <HbondsToResidue          name=\"flt_hbonds\"\n",
    "                                  scorefxn=\"score\"\n",
    "                                  partners=\"1\"\n",
    "                                  residue=\"1X\"\n",
    "                                  backbone=\"true\"\n",
    "                                  sidechain=\"true\"\n",
    "                                  from_other_chains=\"true\"\n",
    "                                  from_same_chain=\"false\"\n",
    "                                  confidence=\"0\" />\n",
    "    </FILTERS>\n",
    "   \n",
    "    <MOVERS>\n",
    "       \n",
    "        <FavorSequenceProfile     name=\"mv_native\"\n",
    "                                  weight=\"{CST_WEIGHT}\"\n",
    "                                  use_native=\"true\"\n",
    "                                  matrix=\"IDENTITY\"\n",
    "                                  scorefxns=\"score\" />  \n",
    "                               \n",
    "        <AddOrRemoveMatchCsts     name=\"mv_add_cst\"\n",
    "                                  cst_instruction=\"add_new\"\n",
    "                                  cstfile=\"{FOLDER_INPUT}/{LIGAND}_enzdes.cst\" />\n",
    "\n",
    "        <FastDesign               name                   = \"mv_design\"\n",
    "                                  disable_design         = \"false\"\n",
    "                                  task_operations        = \"tsk_design,tsk_repack,tsk_nothing,{tsk_cat}\"\n",
    "                                  repeats                = \"{repeats}\"\n",
    "                                  ramp_down_constraints  = \"false\"\n",
    "                                  scorefxn               = \"score\" />\n",
    "        \n",
    "        <FastDesign               name                   = \"mv_design_no_native\"\n",
    "                                  disable_design         = \"false\"\n",
    "                                  task_operations        = \"tsk_design,tsk_repack,tsk_nothing,{tsk_cat}\"\n",
    "                                  repeats                = \"1\"\n",
    "                                  ramp_down_constraints  = \"false\"\n",
    "                                  scorefxn               = \"score\" />\n",
    "                                  \n",
    "        <FastRelax                name                   = \"mv_relax\"\n",
    "                                  disable_design         = \"true\"\n",
    "                                  task_operations        = \"tsk_design,tsk_repack,tsk_nothing,{tsk_cat}\"\n",
    "                                  repeats                = \"1\"\n",
    "                                  ramp_down_constraints  = \"false\"\n",
    "                                  scorefxn               = \"score_unconst\" />  \n",
    "                                  \n",
    "        <InterfaceScoreCalculator name                   = \"mv_inter\"\n",
    "                                  chains                 = \"X\"\n",
    "                                  scorefxn               = \"score_final\" />\n",
    "                                 \n",
    "    </MOVERS>\n",
    "\n",
    "    <PROTOCOLS>\n",
    "        <Add mover_name=\"mv_add_cst\" />\n",
    "        <Add mover_name=\"mv_design_no_native\" />\n",
    "        <Add mover_name=\"mv_native\" />\n",
    "        <Add mover_name=\"mv_design\" />\n",
    "        <Add mover_name=\"mv_relax\" />\n",
    "        <Add mover_name=\"mv_inter\" />\n",
    "    </PROTOCOLS>\n",
    "   \n",
    "</ROSETTASCRIPTS>\n",
    "\n",
    "\"\"\"\n",
    "    # Write the XML script to a file\n",
    "    with open(f'{FOLDER_HOME}/{new_index}/scripts/Rosetta_Design_{new_index}.xml', 'w') as f:\n",
    "        f.writelines(Rosetta_Design_xml)               \n",
    "        \n",
    "    # Submit the job using the submit_job function\n",
    "    logging.info(f\"Run RosettaDesign for index {new_index} based on index {parent_index}.\")\n",
    "    submit_job(index=new_index, job=\"Rosetta_Design\", ram=2)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def run_ProteinMPNN(parent_index, new_index, all_scores_df):\n",
    "    \"\"\"\n",
    "    Executes the ProteinMPNN pipeline for a given protein structure and generates\n",
    "    new protein sequences with potentially higher functional scores.\n",
    "\n",
    "    Parameters:\n",
    "    - parent_index (str): The index of the parent protein variant.\n",
    "    - new_index (str): The index assigned to the new protein variant.\n",
    "    - all_scores_df (DataFrame): A DataFrame containing information for protein variants.\n",
    "    - --------------------------------GLOBAL variables used ----------------------------\n",
    "    - FOLDER_HOME (str): The base directory where ProteinMPNN and related files are located.\n",
    "    - WT (str): The wild type or reference protein identifier.\n",
    "    - DESIGN (str): A string representing positions and types of amino acids to design with RosettaDesign.\n",
    "    - ProteinMPNN_T (float): The sampling temperature for ProteinMPNN.\n",
    "    - EXPLORE (bool): Flag to indicate whether exploration mode is enabled.\n",
    "    - PMPNN_BIAS (float): The bias value for ProteinMPNN parent sequence retention.\n",
    "\n",
    "    Returns:\n",
    "    None: The function primarily works by side effects, finally producing the highes scoring sequence \n",
    "    in the specified directories.\n",
    "    \n",
    "    Note:\n",
    "    This function assumes the ProteinMPNN toolkit is available and properly set up in the specified location.\n",
    "    It involves multiple subprocess calls to Python scripts for processing protein structures and generating new sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Testing PMPNN_submit.py\n",
    "    cmd = f'python {os.getcwd()}/PMPNN_submit.py --index {parent_index} --new_index {new_index} --home_folder {FOLDER_HOME}'\n",
    "    with open(f'{FOLDER_HOME}/{new_index}/scripts/PMPNN_ESM_Relax_{new_index}.sh','w') as file: file.write(cmd)\n",
    "    submit_job(index=new_index, job=\"PMPNN_ESM_Relax\")\n",
    "    return\n",
    "\n",
    "    # Ensure ProteinMPNN is available\n",
    "    if not os.path.exists(f'{FOLDER_HOME}/../ProteinMPNN'):\n",
    "        logging.error(f\"ProteinMPNN not installed in {FOLDER_HOME}/../ProteinMPNN.\")\n",
    "        logging.error(\"Install using: git clone https://github.com/dauparas/ProteinMPNN.git\")\n",
    "        return\n",
    "\n",
    "    # Prepare file paths\n",
    "    pdb_file = f\"{FOLDER_HOME}/{parent_index}/{WT}_Rosetta_Relax_{parent_index}.pdb\"\n",
    "    if not os.path.isfile(pdb_file):\n",
    "        logging.error(f\"{pdb_file} not present!\")\n",
    "        return\n",
    "\n",
    "    protein_mpnn_folder = f\"{FOLDER_HOME}/{new_index}/ProteinMPNN\"\n",
    "    os.makedirs(protein_mpnn_folder, exist_ok=True)\n",
    "    shutil.copy(pdb_file, os.path.join(protein_mpnn_folder, f\"{WT}_Rosetta_Relax_{parent_index}.pdb\"))\n",
    "\n",
    "    seq = extract_sequence_from_pdb(pdb_file)\n",
    "    with open(os.path.join(protein_mpnn_folder, f\"Rosetta_Relax_{parent_index}.seq\"), \"w\") as f:\n",
    "        f.write(seq)\n",
    "    \n",
    "\n",
    "    # Run ProteinMPNN steps using subprocess after creating the bias file\n",
    "    helper_scripts_path = f\"{FOLDER_HOME}/../ProteinMPNN/helper_scripts\"\n",
    "    protein_mpnn_path = f\"{FOLDER_HOME}/../ProteinMPNN\"\n",
    "\n",
    "    # Prepare input JSON for bias dictionary creation\n",
    "    input_json = {\"name\": f\"{WT}_Rosetta_Relax_{parent_index}\", \"seq_chain_A\": seq}\n",
    "\n",
    "    # Create bias dictionary\n",
    "    mpnn_alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    mpnn_alphabet_dict = {aa: idx for idx, aa in enumerate(mpnn_alphabet)}\n",
    "    \n",
    "    bias_dict = {}\n",
    "    for chain_key, sequence in input_json.items():\n",
    "        if chain_key.startswith('seq_chain_'):\n",
    "            chain = chain_key[-1]\n",
    "            chain_length = len(sequence)\n",
    "            bias_per_residue = np.zeros([chain_length, 21])  # 21 for each amino acid in the alphabet\n",
    "\n",
    "            # Apply a positive bias for the amino acid at each position\n",
    "            for idx, aa in enumerate(sequence):\n",
    "                if aa in mpnn_alphabet_dict:  # Ensure the amino acid is in the defined alphabet\n",
    "                    aa_index = mpnn_alphabet_dict[aa]\n",
    "                    bias_per_residue[idx, aa_index] = PMPNN_BIAS  # Use the global bias variable\n",
    "\n",
    "            bias_dict[input_json[\"name\"]] = {chain: bias_per_residue.tolist()}\n",
    "\n",
    "    # Write the bias dictionary to a JSON file\n",
    "    bias_json_path = os.path.join(protein_mpnn_folder, \"bias_by_res.jsonl\")\n",
    "    with open(bias_json_path, 'w') as f:\n",
    "        json.dump(bias_dict, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "    # Parse multiple chains\n",
    "    run_command([\n",
    "        \"python\", os.path.join(helper_scripts_path, \"parse_multiple_chains.py\"),\n",
    "        \"--input_path\", protein_mpnn_folder,\n",
    "        \"--output_path\", os.path.join(protein_mpnn_folder, \"parsed_chains.jsonl\")\n",
    "    ])\n",
    "\n",
    "    # Assign fixed chains\n",
    "    run_command([\n",
    "        \"python\", os.path.join(helper_scripts_path, \"assign_fixed_chains.py\"),\n",
    "        \"--input_path\", os.path.join(protein_mpnn_folder, \"parsed_chains.jsonl\"),\n",
    "        \"--output_path\", os.path.join(protein_mpnn_folder, \"assigned_chains.jsonl\"),\n",
    "        \"--chain_list\", 'A'\n",
    "    ])\n",
    "\n",
    "    # Make fixed positions dict\n",
    "    run_command([\n",
    "        \"python\", os.path.join(helper_scripts_path, \"make_fixed_positions_dict.py\"),\n",
    "        \"--input_path\", os.path.join(protein_mpnn_folder, \"parsed_chains.jsonl\"),\n",
    "        \"--output_path\", os.path.join(protein_mpnn_folder, \"fixed_positions.jsonl\"),\n",
    "        \"--chain_list\", 'A',\n",
    "        \"--position_list\", \" \".join(DESIGN.split(\",\"))\n",
    "    ])\n",
    "\n",
    "    # Protein MPNN run\n",
    "    run_command([\n",
    "        \"python\", os.path.join(protein_mpnn_path, \"protein_mpnn_run.py\"),\n",
    "        \"--jsonl_path\", os.path.join(protein_mpnn_folder, \"parsed_chains.jsonl\"),\n",
    "        \"--chain_id_jsonl\", os.path.join(protein_mpnn_folder, \"assigned_chains.jsonl\"),\n",
    "        \"--fixed_positions_jsonl\", os.path.join(protein_mpnn_folder, \"fixed_positions.jsonl\"),\n",
    "        \"--bias_by_res_jsonl\", os.path.join(protein_mpnn_folder, \"bias_by_res.jsonl\"),\n",
    "        \"--out_folder\", protein_mpnn_folder,\n",
    "        \"--num_seq_per_target\", \"100\",\n",
    "        \"--sampling_temp\", ProteinMPNN_T,\n",
    "        \"--seed\", \"37\",\n",
    "        \"--batch_size\", \"1\"\n",
    "    ])\n",
    "    \n",
    "\n",
    "    # Find highest scoring sequence\n",
    "    highest_scoring_sequence = find_highest_scoring_sequence(protein_mpnn_folder, parent_index, input_sequence_path=\n",
    "                                                             f\"{FOLDER_HOME}/input_sequence_with_X_as_wildecard.seq\")\n",
    "\n",
    "    # Save highest scoring sequence and prepare for ESMfold\n",
    "    with open(os.path.join(protein_mpnn_folder, f\"{WT}_{new_index}.seq\"), \"w\") as f:\n",
    "        f.write(highest_scoring_sequence)\n",
    "    \n",
    "    if highest_scoring_sequence:\n",
    "        logging.info(f\"Ran ProteinMPNN for index {parent_index} and found a new sequence with index {new_index}.\")\n",
    "    else:\n",
    "        logging.error(f\"Failed to find a new sequnce for index {parent_index} with ProteinMPNN.\")\n",
    "    \n",
    "    all_scores_df = save_cat_res_into_all_scores_df(all_scores_df, new_index, pdb_file, from_parent_struct=False)\n",
    "        \n",
    "    # Run ESMfold Relax with the ProteinMPNN Flag\n",
    "    run_ESMfold_RosettaRelax(index=new_index, all_scores_df=all_scores_df, OnlyRelax=False, \\\n",
    "                             ProteinMPNN=True, ProteinMPNN_parent_index=parent_index, EXPLORE=EXPLORE)\n",
    "\n",
    "def find_highest_scoring_sequence(folder_path, parent_index, input_sequence_path):\n",
    "    \"\"\"\n",
    "    Identifies the highest scoring protein sequence from a set of generated PNPNN sequences,\n",
    "    excluding the parent and WT sequence (except wildcard positions specified by DESIGN).\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path to the directory containing sequence files (/ProteinMPNN).\n",
    "    - parent_index (str): The index of the parent protein sequence.\n",
    "    - input_sequence_path (str): The path to a file containing the input sequence pattern,\n",
    "      where 'X' represents wildcard positions that can match any character.\n",
    "    - -------------------------------- GLOBAL variables used ----------------------------\n",
    "    - WT (str): The wild type or reference protein identifier.\n",
    "      \n",
    "\n",
    "    Returns:\n",
    "    - highest_scoring_sequence (str): The protein sequence with the highest score \n",
    "      that does not match the parent and WT.\n",
    "    \n",
    "    Note:\n",
    "    This function parses .fa files to find sequences and their scores, and applies\n",
    "    a regex pattern derived from the input sequence to filter sequences.\n",
    "    It assumes the presence of 'global_score' within the sequence descriptor lines\n",
    "    in the .fa file for scoring.\n",
    "    \"\"\"\n",
    "    # Construct the file path for the sequence data\n",
    "    file_path = f'{folder_path}/seqs/{WT}_Rosetta_Relax_{parent_index}.fa'\n",
    "    parent_seq_file = f'{folder_path}/Rosetta_Relax_{parent_index}.seq'\n",
    "    \n",
    "    # Read the parent sequence from its file\n",
    "    with open(parent_seq_file, 'r') as file:\n",
    "        parent_sequence = file.readline().strip()\n",
    "\n",
    "    # Read the input sequence pattern and prepare it for regex matching\n",
    "    with open(input_sequence_path, 'r') as file:\n",
    "        input_sequence = file.readline().strip()\n",
    "    pattern = re.sub('X', '.', input_sequence)  # Replace 'X' with regex wildcard '.'\n",
    "\n",
    "    highest_score = 0\n",
    "    highest_scoring_sequence = ''\n",
    "\n",
    "    # Process the sequence file to find the highest scoring sequence\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                score_match = re.search('global_score=(\\d+\\.\\d+)', line)\n",
    "                if score_match:\n",
    "                    score = float(score_match.group(1))\n",
    "                    sequence = next(file, '').strip()  # Read the next line for the sequence\n",
    "                    \n",
    "                    # Check if the score is higher, the sequence is different from the parent,\n",
    "                    # and does not match the input sequence pattern\n",
    "                    if score > highest_score and sequence != parent_sequence and not re.match(pattern, sequence):\n",
    "                        highest_score = score\n",
    "                        highest_scoring_sequence = sequence\n",
    "\n",
    "    # Return the highest scoring sequence found\n",
    "    return highest_scoring_sequence\n",
    "\n",
    "def find_highest_confidence_sequence(fa_file_path, output_seq_file_path):\n",
    "    \"\"\"\n",
    "    Parses a .fa file to find the sequence with the highest overall confidence and writes it to a .seq file.\n",
    "\n",
    "    Parameters:\n",
    "    - fa_file_path (str): Path to the .fa file generated by LigandMPNN.\n",
    "    - output_seq_file_path (str): Path where the .seq file should be saved.\n",
    "    \"\"\"\n",
    "    highest_confidence = 0.0\n",
    "    highest_confidence_sequence = None\n",
    "    current_confidence = 0.0\n",
    "\n",
    "    with open(fa_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                # Extract overall confidence from the header line\n",
    "                match = re.search('overall_confidence=([0-9.]+)', line)\n",
    "                if match:\n",
    "                    current_confidence = float(match.group(1))\n",
    "            else:\n",
    "                # Sequence line\n",
    "                current_sequence = line.strip()\n",
    "                if current_confidence > highest_confidence:\n",
    "                    highest_confidence = current_confidence\n",
    "                    highest_confidence_sequence = current_sequence\n",
    "\n",
    "    # Write the highest confidence sequence to a .seq file\n",
    "    if highest_confidence_sequence:\n",
    "        with open(output_seq_file_path, 'w') as output_file:\n",
    "            output_file.write(highest_confidence_sequence)\n",
    "        logging.info(f\"Extracted sequence with highest confidence: {highest_confidence} to {output_seq_file_path}\")\n",
    "    else:\n",
    "        logging.error(f\"No sequence found with overall confidence for {fa_file_path}.\")\n",
    "\n",
    "def run_LigandMPNN(parent_index, new_index, all_scores_df):\n",
    "    \"\"\"\n",
    "    Executes the LigandMPNN pipeline for a given protein-ligand structure and generates\n",
    "    new protein sequences with potentially higher functional scores considering the ligand context.\n",
    "\n",
    "    Parameters:\n",
    "    - parent_index (str): The index of the parent protein variant.\n",
    "    - new_index (str): The index assigned to the new protein variant.\n",
    "    - all_scores_df (DataFrame): A DataFrame containing information for protein variants.\n",
    "    \"\"\"\n",
    "    # Ensure LigandMPNN is available\n",
    "    if not os.path.exists(f'{FOLDER_HOME}/../LigandMPNN'):\n",
    "        logging.error(f\"LigandMPNN not installed in {FOLDER_HOME}/LigandMPNN.\")\n",
    "        logging.error(\"Install using: git clone https://github.com/dauparas/LigandMPNN.git\")\n",
    "        return\n",
    "    ligand_mpnn_path = f\"{FOLDER_HOME}/../LigandMPNN\"\n",
    "\n",
    "    # Prepare file paths\n",
    "    pdb_file = f\"{FOLDER_HOME}/{parent_index}/{WT}_Rosetta_Relax_{parent_index}.pdb\"\n",
    "    if not os.path.isfile(pdb_file):\n",
    "        logging.error(f\"{pdb_file} not present!\")\n",
    "        return\n",
    "\n",
    "    ligand_mpnn_folder = f\"{FOLDER_HOME}/{new_index}/LigandMPNN\"\n",
    "    os.makedirs(ligand_mpnn_folder, exist_ok=True)\n",
    "    shutil.copy(pdb_file, os.path.join(ligand_mpnn_folder, f\"{WT}_Rosetta_Relax_{parent_index}.pdb\"))\n",
    "\n",
    "    # Extract catalytic residue information\n",
    "    cat_resi = int(all_scores_df.at[parent_index, 'cat_resi'])\n",
    "    fixed_residues = f\"A{cat_resi}\"\n",
    "\n",
    "    # Run LigandMPNN\n",
    "    run_command([\n",
    "        \"python\", os.path.join(ligand_mpnn_path, \"run.py\"),\n",
    "        \"--model_type\", \"ligand_mpnn\",\n",
    "        \"--temperature\", LMPNN_T,\n",
    "        \"--seed\", \"37\",\n",
    "        \"--pdb_path\", os.path.join(ligand_mpnn_folder, f\"{WT}_Rosetta_Relax_{parent_index}.pdb\"),\n",
    "        \"--out_folder\", ligand_mpnn_folder,\n",
    "        #\"--pack_side_chains\", \"1\",\n",
    "        \"--number_of_packs_per_design\", \"4\",\n",
    "        \"--fixed_residues\", fixed_residues\n",
    "    ], cwd=ligand_mpnn_path)\n",
    "\n",
    "    find_highest_confidence_sequence(f\"{FOLDER_HOME}/{new_index}/LigandMPNN/seqs/{WT}_Rosetta_Relax_{parent_index}.fa\",\n",
    "                                    f\"{FOLDER_HOME}/{new_index}/LigandMPNN/{WT}_{new_index}.seq\")\n",
    "\n",
    "    # Update all_scores_df\n",
    "\n",
    "    logging.info(f\"Ran LigandMPNN for index {parent_index} and generated index {new_index}.\")\n",
    "\n",
    "    all_scores_df = save_cat_res_into_all_scores_df(all_scores_df, new_index, pdb_file, from_parent_struct=False)\n",
    "\n",
    "    run_ESMfold_RosettaRelax(index=new_index, all_scores_df=all_scores_df, OnlyRelax=False, \\\n",
    "                             ProteinMPNN=True, ProteinMPNN_parent_index=parent_index, EXPLORE=False)\n",
    "\n",
    "    # Save updates to all_scores_df\n",
    "    #save_all_scores_df(all_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e6193",
   "metadata": {},
   "source": [
    "# main functions - startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd671a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def startup_controller(UNBLOCK_ALL, RESET, PRINT_VAR=True, PLOT_DATA=True):\n",
    "\n",
    "    # Execute setup if variables file not found\n",
    "    if not os.path.isfile(VARIABLES_JSON): setup_aizymes(RESET) #? why is this called again?\n",
    "    \n",
    "    # Creat all input files, not needed here but does not harm\n",
    "    prepare_input_files()\n",
    "    \n",
    "    if PRINT_VAR:\n",
    "        if os.path.isfile(VARIABLES_JSON):\n",
    "            with open(VARIABLES_JSON, 'r') as f: \n",
    "                globals_dict = json.load(f)\n",
    "            if globals_dict['DESIGN_FOLDER'] == DESIGN_FOLDER:\n",
    "                for k, v in globals_dict.items():\n",
    "                    globals()[k] = v\n",
    "                    print(k.ljust(16), ':', v)\n",
    "            else:\n",
    "                print(\"WRONG DESIGN FOLDER!\")\n",
    "                sys.exit()\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    if PLOT_DATA:\n",
    "        plot_scores()\n",
    "        \n",
    "    # Read in current databases of AIzymes\n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "\n",
    "    if UNBLOCK_ALL: \n",
    "        all_scores_df[\"blocked\"] = False\n",
    "   \n",
    "    return all_scores_df \n",
    "\n",
    "def prepare_input_files():\n",
    "                      \n",
    "    # Create the ESMfold.py script\n",
    "    ESMfold_python_script = \"\"\"import sys\n",
    "from transformers import AutoTokenizer, EsmForProteinFolding, EsmConfig\n",
    "import torch\n",
    "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
    "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
    "\n",
    "# Set PyTorch to use only one thread\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "output_file = sys.argv[1]\n",
    "sequence_file = sys.argv[2]\n",
    "\n",
    "with open(sequence_file) as f: sequence=f.read()\n",
    "\n",
    "def convert_outputs_to_pdb(outputs):\n",
    "    final_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
    "    outputs = {k: v.to(\"cpu\").numpy() for k, v in outputs.items()}\n",
    "    final_atom_positions = final_atom_positions.cpu().numpy()\n",
    "    final_atom_mask = outputs[\"atom37_atom_exists\"]\n",
    "    pdbs = []\n",
    "    for i in range(outputs[\"aatype\"].shape[0]):\n",
    "        aa = outputs[\"aatype\"][i]\n",
    "        pred_pos = final_atom_positions[i]\n",
    "        mask = final_atom_mask[i]\n",
    "        resid = outputs[\"residue_index\"][i] + 1\n",
    "        pred = OFProtein(\n",
    "            aatype=aa,\n",
    "            atom_positions=pred_pos,\n",
    "            atom_mask=mask,\n",
    "            residue_index=resid,\n",
    "            b_factors=outputs[\"plddt\"][i],\n",
    "            chain_index=outputs[\"chain_index\"][i] if \"chain_index\" in outputs else None,\n",
    "        )\n",
    "        pdbs.append(to_pdb(pred))\n",
    "    return pdbs\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\", low_cpu_mem_usage=True)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "model.trunk.set_chunk_size(64)\n",
    "tokenized_input = tokenizer([sequence], return_tensors=\"pt\", add_special_tokens=False)['input_ids']\n",
    "with torch.no_grad(): output = model(tokenized_input)\n",
    "pdb = convert_outputs_to_pdb(output)\n",
    "with open(output_file, \"w\") as f: f.write(\"\".join(pdb))\n",
    "\"\"\"\n",
    "\n",
    "    # Write the ESMfold.py to a file\n",
    "    with open(f\"{FOLDER_HOME}/ESMfold.py\", \"w\") as f: \n",
    "        f.write(ESMfold_python_script)\n",
    "\n",
    "    # Save input sequence with X as wildcard\n",
    "    if not os.path.isfile(f\"{FOLDER_INPUT}/{WT}.pdb\"):\n",
    "        print(f\"Error, scaffold protein structure {FOLDER_INPUT}/{WT}.pdb is missing!\")\n",
    "        sys.exit()        \n",
    "    seq = extract_sequence_from_pdb(f\"{FOLDER_INPUT}/{WT}.pdb\")\n",
    "    design_positions = [int(x) for x in DESIGN.split(',')]\n",
    "    # Replace seq with X at design positions. Note: Subtract 1 from each position to convert to Python's 0-based indexing\n",
    "    seq = ''.join('X' if (i+1) in design_positions else amino_acid for i, amino_acid in enumerate(seq))\n",
    "    with open(f'{FOLDER_HOME}/input_sequence_with_X_as_wildecard.seq', 'w') as f:\n",
    "        f.writelines(seq)    \n",
    "    \n",
    "    # Get the Constraint Residues from enzdes constraints file\n",
    "    with open(f'{FOLDER_INPUT}/{LIGAND}_enzdes.cst', 'r') as f:\n",
    "        cst = f.readlines()    \n",
    "    cst = [i.split()[-1] for i in cst if \"TEMPLATE::   ATOM_MAP: 2 res\" in i]\n",
    "    cst = \";\".join(cst)\n",
    "    with open(f'{FOLDER_HOME}/cst.dat', 'w') as f:\n",
    "        f.write(cst)    \n",
    "    \n",
    "def setup_aizymes(RESET=False, EXPLORE=False, PROMPT=True):\n",
    "    \n",
    "    \n",
    "    # Check if setup needs to run\n",
    "    if not os.path.isfile(VARIABLES_JSON):\n",
    "        if PROMPT:\n",
    "            if not input(f'''Do you want to start AIzymes? [y/n]\n",
    "\n",
    "    ''') == 'y':\n",
    "                return #stops the start up. Although VARIABLES_JSON is missing, user elected not to set up AIzymes\n",
    "\n",
    "\n",
    "    else:\n",
    "        if RESET:\n",
    "            if not input(f'''Do you really want to restart AIzymes from scratch? \n",
    "    This will delete all existing files in {FOLDER_HOME} [y/n]\n",
    "\n",
    "    ''') == 'y':\n",
    "                return #stops the start up. Although VARIABLES_JSON exists and RESET set, user canceled\n",
    "        else:\n",
    "            return #stop startup. VARIABLES_JSON exists and RESET not set by user\n",
    "\n",
    "    with open(LOG_FILE, 'w'): pass  #resets logfile\n",
    "    logging.info(f\"Running AI.zymes setup.\")\n",
    "    logging.info(f\"Content of {FOLDER_HOME} deleted.\")\n",
    "    logging.info(f\"Happy AI.zymeing! :)\")\n",
    "   \n",
    "    if os.path.exists(FOLDER_HOME):\n",
    "        for item in os.listdir(FOLDER_HOME):\n",
    "            if item == FOLDER_MATCH: continue\n",
    "            item = f'{FOLDER_HOME}/{item}'\n",
    "            if os.path.isfile(item): \n",
    "                os.remove(item)\n",
    "            elif os.path.isdir(item):\n",
    "                shutil.rmtree(item)\n",
    "    os.makedirs(FOLDER_HOME, exist_ok=True)\n",
    "\n",
    "    prepare_input_files()\n",
    "        \n",
    "    #make empyt all_scores_df\n",
    "    all_scores_df = make_empty_all_scores_df()\n",
    "\n",
    "    # create empty blocked.dat\n",
    "    #np.savetxt(BLOCKED_DAT, np.array([], dtype=int), fmt='%d')\n",
    "\n",
    "    # Save global varliables\n",
    "    variables_to_save = [\n",
    "        'DESIGN_FOLDER', 'FOLDER_MATCH', 'MAX_JOBS', 'N_PARENT_JOBS', 'MAX_DESIGNS', 'KBT_BOLTZMANN', 'CST_WEIGHT',\n",
    "        'ProteinMPNN_PROB', 'LMPNN_PROB', 'WT', 'LIGAND', 'ROSETTA_PATH', 'REPACK', 'DESIGN', 'MATCH', 'FOLDER_PARENT',\n",
    "        'ProteinMPNN_T', 'PMPNN_BIAS', 'LMPNN_T', 'SUBMIT_PREFIX', 'BLUEPEBBLE', 'GRID', 'BACKGROUND_JOB', 'ABBIE_LOCAL', 'FIELD_TOOLS'\n",
    "    ]\n",
    "    globals_to_save = {k: globals()[k] for k in variables_to_save}\n",
    "    globals_to_save['EXPLORE'] = EXPLORE\n",
    "    \n",
    "    if N_PARENT_JOBS < MAX_JOBS*2:\n",
    "        logging.warning(f\"To ensure a smooth start, N_PARENT_JOBS should be at least 2 x MAX_JOBS.\")\n",
    "        logging.warning(f\"N_PARENT_JOBS: {N_PARENT_JOBS}, MAX_JOBS: {MAX_JOBS}.\")\n",
    "            \n",
    "    with open(VARIABLES_JSON, 'w') as f: json.dump(globals_to_save, f, indent=4)\n",
    "    \n",
    "    \n",
    "def make_empty_all_scores_df():\n",
    "    \n",
    "    all_scores_df = pd.DataFrame(columns=['index', 'sequence', 'parent_index', \\\n",
    "                                          'interface_score', 'total_score', 'catalytic_score', 'efield_score', \\\n",
    "                                          'interface_potential', 'total_potential', 'catalytic_potential', 'efield_potential', \\\n",
    "                                          'relax_interface_score', 'relax_total_score', 'relax_catalytic_score', 'relax_efield_score', \\\n",
    "                                          'design_interface_score', 'design_total_score', 'design_catalytic_score', 'design_efield_score', \\\n",
    "                                          'generation', 'mutations', 'design_method', 'score_taken_from', 'blocked', \\\n",
    "                                          'cat_resi', 'cat_resn'])\n",
    "    \n",
    "    save_all_scores_df(all_scores_df)\n",
    "\n",
    "    return all_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c01cc",
   "metadata": {},
   "source": [
    "# RosettaMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dced448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RosettaMatch(EXPLORE=False, submit=False, bash=True):\n",
    "        \n",
    "    prepare_input_files()\n",
    "    \n",
    "    os.makedirs(FOLDER_MATCH, exist_ok=True)\n",
    "    if not os.path.isdir(f'{FOLDER_HOME}/{FOLDER_MATCH}/scripts'):\n",
    "        run_ESMfold_RosettaRelax(FOLDER_MATCH, all_scores_df=None, PreMatchRelax=True, EXPLORE=EXPLORE) \n",
    "    elif not os.path.isfile(f'{FOLDER_HOME}/{FOLDER_MATCH}/{WT}_Rosetta_Relax_{FOLDER_MATCH}_APO.pdb'):\n",
    "        print(f\"ESMfold and Relax of {FOLDER_MATCH} still running.\")\n",
    "    elif not os.path.isdir(f'{FOLDER_HOME}/{FOLDER_MATCH}/matches'):\n",
    "        run_Matcher()\n",
    "    else:\n",
    "        print(\"Matching is done\")\n",
    "            \n",
    "def run_Matcher():\n",
    "        \n",
    "    cmd = f\"\"\"       \n",
    "  \n",
    "cd {FOLDER_HOME}/{FOLDER_MATCH}\n",
    "\n",
    "echo C9 > {LIGAND}.central\n",
    "echo {\" \".join(MATCH.split(\",\"))} > {LIGAND}.pos\n",
    "\n",
    "{ROSETTA_PATH}/bin/gen_lig_grids.linuxgccrelease \\\n",
    "    -s                      {WT}_Rosetta_Relax_aligned_{FOLDER_MATCH}_APO.pdb ESMfold/{WT}_CPPTraj_Lig_{FOLDER_MATCH}.pdb \\\n",
    "    -extra_res_fa           {FOLDER_INPUT}/{LIGAND}.params \\\n",
    "    -grid_delta             0.5 \\\n",
    "    -grid_lig_cutoff        5.0 \\\n",
    "    -grid_bb_cutoff         2.25 \\\n",
    "    -grid_active_res_cutoff 15.0 \\\n",
    "    -overwrite \n",
    "\n",
    "mv {WT}_Rosetta_Relax_aligned_{FOLDER_MATCH}_APO.pdb_0.gridlig {WT}.gridlig\n",
    "rm {WT}_Rosetta_Relax_aligned_{FOLDER_MATCH}_APO.pdb_0.pos 2>1\n",
    "\n",
    "rm -r matches\n",
    "mkdir matches\n",
    "cd matches\n",
    "\n",
    "{ROSETTA_PATH}/bin/match.linuxgccrelease \\\n",
    "    -s                                        ../{WT}_Rosetta_Relax_aligned_{FOLDER_MATCH}_APO.pdb \\\n",
    "    -match:lig_name                           {LIGAND} \\\n",
    "    -extra_res_fa                             {FOLDER_INPUT}/{LIGAND}.params \\\n",
    "    -match:geometric_constraint_file          {FOLDER_INPUT}/{LIGAND}_enzdes.cst \\\n",
    "    -match::scaffold_active_site_residues     ../{LIGAND}.pos \\\n",
    "    -match:required_active_site_atom_names    ../{LIGAND}.central \\\n",
    "    -match:active_site_definition_by_gridlig  ../{WT}.gridlig  \\\n",
    "    -match:grid_boundary                      ../{WT}.gridlig  \\\n",
    "    -gridligpath                              ../{WT}.gridlig  \\\n",
    "    -overwrite  \\\n",
    "    -output_format PDB  \\\n",
    "    -output_matches_per_group 1  \\\n",
    "    -consolidate_matches true \n",
    "\"\"\" \n",
    "    with open(f'{FOLDER_HOME}/{FOLDER_MATCH}/scripts/RosettaMatch_{FOLDER_MATCH}.sh', 'w') as file: file.write(cmd)\n",
    "    logging.info(f\"Run Rosetta_Match for index {FOLDER_MATCH}.\")\n",
    "    submit_job(FOLDER_MATCH, job=\"RosettaMatch\", bash=False)\n",
    "\n",
    "def separate_matches():\n",
    "    parent_folder = f'{FOLDER_HOME}/{FOLDER_PARENT}'\n",
    "    pdb_files = [f for f in os.listdir(parent_folder) if f.endswith('.pdb')]\n",
    "\n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_path = os.path.join(parent_folder, pdb_file)\n",
    "        with open(pdb_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                if line.startswith('REMARK 666 MATCH TEMPLATE'):\n",
    "                    parts = line.split()\n",
    "                    catalytic_residue_index = parts[11]  # Extract the catalytic residue index\n",
    "                    break\n",
    "\n",
    "        # Create the destination folder if it doesn't exist\n",
    "        dest_folder = os.path.join(parent_folder, catalytic_residue_index)\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "\n",
    "        # Copy the PDB file to the destination folder instead of moving it\n",
    "        shutil.copy(pdb_path, os.path.join(dest_folder, pdb_file))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db4bf52",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_job(index, job, bash=False, ram=16):        \n",
    "      \n",
    "    if GRID:\n",
    "        submission_script = f\"\"\"#!/bin/bash\n",
    "#$ -V\n",
    "#$ -cwd\n",
    "#$ -N {SUBMIT_PREFIX}_{job}_{index}\n",
    "#$ -hard -l mf={ram}G\n",
    "#$ -o {FOLDER_HOME}/{index}/scripts/AI_{job}_{index}.out\n",
    "#$ -e {FOLDER_HOME}/{index}/scripts/AI_{job}_{index}.err\n",
    "\"\"\"\n",
    "    if BLUEPEBBLE:\n",
    "        submission_script = f\"\"\"#!/bin/bash\n",
    "#SBATCH --account={BLUEPEBBLE_ACCOUNT}\n",
    "#SBATCH --partition=short\n",
    "#SBATCH --mem=40GB\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --time=2:00:00    \n",
    "#SBATCH --nodes=1          \n",
    "#SBATCH --job-name={SUBMIT_PREFIX}_{job}_{index}\n",
    "#SBATCH --output={FOLDER_HOME}/{index}/scripts/AI_{job}_{index}.out\n",
    "#SBATCH --error={FOLDER_HOME}/{index}/scripts/AI_{job}_{index}.err\n",
    "\"\"\"\n",
    "        \n",
    "    if BACKGROUND_JOB:\n",
    "        if not os.path.isfile(f'{FOLDER_HOME}/n_running_jobs.dat'):\n",
    "            with open(f'{FOLDER_HOME}/n_running_jobs.dat', 'w') as f: f.write('0')\n",
    "        with open(f'{FOLDER_HOME}/n_running_jobs.dat', 'r'): jobs = int(f.read())\n",
    "        with open(f'{FOLDER_HOME}/n_running_jobs.dat', 'w'): f.write(jobs+1)\n",
    "        submission_script = \"\"\n",
    "\n",
    "    if ABBIE_LOCAL:\n",
    "        submission_script = \"\"\n",
    "        \n",
    "    submission_script += f\"\"\"\n",
    "# Output folder\n",
    "cd {FOLDER_HOME}/{index}\n",
    "pwd\n",
    "bash {FOLDER_HOME}/{index}/scripts/{job}_{index}.sh\n",
    "\"\"\" \n",
    "    if BACKGROUND_JOB:\n",
    "        submission_script = f\"\"\"\n",
    "jobs=$(cat {FOLDER_HOME}/n_running_jobs.dat)\n",
    "jobs=$((jobs - 1))\n",
    "echo \"$jobs\" > {FOLDER_HOME}/n_running_jobs.dat\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    # Create the submission_script\n",
    "    with open(f'{FOLDER_HOME}/{index}/scripts/submit_{job}_{index}.sh', 'w') as file: file.write(submission_script)\n",
    "    \n",
    "    if bash:\n",
    "        #Bash the submission_script for testing\n",
    "        subprocess.run(f'bash {FOLDER_HOME}/{index}/scripts/submit_{job}_{index}.sh', shell=True, text=True)\n",
    "    else:\n",
    "        #Submit the submission_script\n",
    "        if GRID:\n",
    "            if \"ESM\" in job:\n",
    "                \n",
    "                output = subprocess.check_output(\n",
    "    (f'qsub -l h=\"!bs-dsvr64&!bs-dsvr58&!bs-dsvr42&'\n",
    "     f'!bs-grid64&!bs-grid65&!bs-grid66&!bs-grid67&'\n",
    "     f'!bs-grid68&!bs-grid69&!bs-grid70&!bs-grid71&'\n",
    "     f'!bs-grid72&!bs-grid73&!bs-grid74&!bs-grid75&'\n",
    "     f'!bs-grid76&!bs-grid77&!bs-grid78&!bs-headnode04&'\n",
    "     f'!bs-stellcontrol05&!bs-stellsubmit05\" -q regular.q '\n",
    "     f'{FOLDER_HOME}/{index}/scripts/submit_{job}_{index}.sh'),\n",
    "    shell=True, text=True\n",
    "    )\n",
    "            else:\n",
    "                output = subprocess.check_output(f'qsub -q regular.q \\\n",
    "                                                {FOLDER_HOME}/{index}/scripts/submit_{job}_{index}.sh', \\\n",
    "                                                shell=True, text=True)\n",
    "            logging.debug(output[:-1]) #remove newline at end of output\n",
    "            \n",
    "        if BLUEPEBBLE:\n",
    "            output = subprocess.check_output(f'sbatch {FOLDER_HOME}/{index}/scripts/submit_{job}_{index}.sh', \\\n",
    "                                             shell=True, text=True)\n",
    "            logging.debug(output[:-1]) #remove newline at end of output\n",
    "            \n",
    "        if BACKGROUND_JOB:\n",
    "\n",
    "            stdout_log_file_path = f'{FOLDER_HOME}/{index}/scripts/submit_{job}_{index}_stdout.log'\n",
    "            stderr_log_file_path = f'{FOLDER_HOME}/{index}/scripts/submit_{job}_{index}_stderr.log'\n",
    "\n",
    "            with open(stdout_log_file_path, 'w') as stdout_log_file, open(stderr_log_file_path, 'w') as stderr_log_file:\n",
    "                process = subprocess.Popen(f'bash {FOLDER_HOME}/{index}/scripts/submit_{job}_{index}.sh &', \n",
    "                                           shell=True, stdout=stdout_log_file, stderr=stderr_log_file)\n",
    "        \n",
    "        if ABBIE_LOCAL:\n",
    "\n",
    "            stdout_log_file_path = f'{FOLDER_HOME}/{index}/scripts/submit_{job}_{index}_stdout.log'\n",
    "            stderr_log_file_path = f'{FOLDER_HOME}/{index}/scripts/submit_{job}_{index}_stderr.log'\n",
    "\n",
    "            with open(stdout_log_file_path, 'w') as stdout_log_file, open(stderr_log_file_path, 'w') as stderr_log_file:\n",
    "                process = subprocess.Popen(f'bash {FOLDER_HOME}/{index}/scripts/submit_{job}_{index}.sh &', \n",
    "                                           shell=True, stdout=stdout_log_file, stderr=stderr_log_file)\n",
    "        \n",
    "def extract_sequence_from_pdb(pdb_path):\n",
    "    with open(pdb_path, \"r\") as pdb_file:\n",
    "        for record in SeqIO.parse(pdb_file, \"pdb-atom\"):\n",
    "            seq = str(record.seq)\n",
    "    return seq\n",
    "    \n",
    "def generate_remark_from_all_scores_df(all_scores_df, index):\n",
    "\n",
    "    remark = ''\n",
    "    cat_resns = str(all_scores_df.at[index, 'cat_resn']).split(';')\n",
    "    # Making sure resi is converted to int to avoid crash in Relax\n",
    "    cat_resis = [int(float(x)) for x in str(all_scores_df.at[index, 'cat_resi']).split(';')]\n",
    "    \n",
    "    remarks = []\n",
    "\n",
    "    for cat_resi, cat_resn in zip(cat_resis, cat_resns):\n",
    "        remarks.append(f'REMARK 666 MATCH TEMPLATE X {LIGAND}    0 MATCH MOTIF A {cat_resn}{str(cat_resi).rjust(5)}  1  1')\n",
    "    return \"\\n\".join(remarks)\n",
    "\n",
    "def save_cat_res_into_all_scores_df(all_scores_df, index, PDB_file_path, from_parent_struct=False):\n",
    "    \n",
    "    '''Finds the indices and names of the catalytic residue from <PDB_file_path> \n",
    "       Saves indices and residues into <all_scores_df> in row <index> as lists.\n",
    "       To make sure these are saved and loaded as list, \";\".join() and .split(\";\") should be used\n",
    "       IF information is read from an input structure for design do not save cat_resn\n",
    "       Returns the updated all_scores_df'''\n",
    "      \n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    with open(PDB_file_path, 'r') as f: \n",
    "        PDB = f.readlines()\n",
    "    \n",
    "    remarks = [i for i in PDB if i[:10] == 'REMARK 666']\n",
    "\n",
    "    cat_resis = []\n",
    "    cat_resns = []\n",
    "\n",
    "    for remark in remarks:\n",
    "        cat_resis.append(str(int(remark[55:59])))\n",
    "\n",
    "    for cat_resi in cat_resis:\n",
    "        for line in PDB[len(remarks)+2:]:\n",
    "            atomtype = line[12:16]\n",
    "            if atomtype != \" CA \": continue\n",
    "            resi = str(int(line[22:26]))\n",
    "            resn = line[17:20]\n",
    "            if resi == cat_resi:\n",
    "                cat_resns.append(resn)\n",
    "                break\n",
    "\n",
    "    # all_scores_df['cat_resi'] = all_scores_df['cat_resi'].astype(object)\n",
    "    # all_scores_df['cat_resn'] = all_scores_df['cat_resn'].astype(object)\n",
    "    all_scores_df.at[index, 'cat_resi'] = \";\".join(cat_resis)\n",
    "    # Only save the cat_resn if this comes from the designed structure, not from the input structure for design\n",
    "    if not from_parent_struct:\n",
    "        all_scores_df.at[index, 'cat_resn'] = \";\".join(cat_resns)\n",
    "    \n",
    "    return all_scores_df \n",
    "\n",
    "def reset_to_after_parent_design():\n",
    "    \n",
    "    folders = []\n",
    "    \n",
    "    for folder_name in os.listdir(FOLDER_HOME):\n",
    "        if os.path.isdir(os.path.join(FOLDER_HOME, folder_name)) and folder_name.isdigit():\n",
    "            folders.append(int(folder_name))\n",
    "    \n",
    "    all_scores_df = make_empty_all_scores_df()\n",
    "        \n",
    "    PARENTS = [i for i in os.listdir(f'{FOLDER_HOME}/{FOLDER_PARENT}') if i[-4:] == \".pdb\"]\n",
    "    \n",
    "    for folder in sorted(folders):\n",
    "        \n",
    "        folder_path = os.path.join(FOLDER_HOME, str(folder))\n",
    "        \n",
    "        if folder >= N_PARENT_JOBS * len(PARENTS):\n",
    "            \n",
    "            #Remove non-parent designs\n",
    "            shutil.rmtree(folder_path)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #Remove Potentials\n",
    "            for item in os.listdir(folder_path):\n",
    "                if 'potential.dat' not in item: continue\n",
    "                item_path = os.path.join(folder_path, item)\n",
    "                os.remove(item_path)\n",
    "                print(item_path)\n",
    "                    \n",
    "            #Update Scorefile\n",
    "            new_index, all_scores_df = create_new_index(parent_index=\"Parent\", all_scores_df=all_scores_df)\n",
    "            all_scores_df['design_method'] = all_scores_df['design_method'].astype('object') \n",
    "            all_scores_df.at[new_index, 'design_method'] = \"RosettaDesign\"\n",
    "            all_scores_df['luca'] = all_scores_df['luca'].astype('object') \n",
    "            score_file_path = f\"{FOLDER_HOME}/{int(index)}/score_rosetta_design.sc\"\n",
    "            with open(score_file_path, 'r') as f: score = f.readlines()[2]\n",
    "            all_scores_df.at[new_index, 'luca'] = score.split()[-1][:-5]\n",
    "    \n",
    "            if new_index % 100 == 0: print(folder, new_index) \n",
    "\n",
    "    save_all_scores_df(all_scores_df)\n",
    "\n",
    "def reset_to_after_index(index):\n",
    "    '''This function resets the run back to a chosen index. It removes all later entries from the all_scores.csv and the home dir.\n",
    "    index: The last index to keep, after which everything will be deleted.'''\n",
    "    \n",
    "    folders = []\n",
    "    \n",
    "    for folder_name in os.listdir(FOLDER_HOME):\n",
    "        if os.path.isdir(os.path.join(FOLDER_HOME, folder_name)) and folder_name.isdigit():\n",
    "            folders.append(int(folder_name))\n",
    "    \n",
    "    # Load the existing all_scores_df\n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "    \n",
    "    # Filter out rows with index greater than the specified index\n",
    "    all_scores_df = all_scores_df[all_scores_df['index'] <= index]\n",
    "    \n",
    "    # Save the updated all_scores_df\n",
    "    save_all_scores_df(all_scores_df)\n",
    "    \n",
    "    for folder in sorted(folders):\n",
    "        if folder > index:\n",
    "            folder_path = os.path.join(FOLDER_HOME, str(folder))\n",
    "            shutil.rmtree(folder_path)\n",
    "    \n",
    "    print(f\"Reset completed. All entries and folders after index {index} have been removed.\")\n",
    "\n",
    "    \n",
    "def save_all_scores_df(all_scores_df):\n",
    "    \n",
    "    temp_fd, temp_path = tempfile.mkstemp(dir=FOLDER_HOME) # Create a temporary file\n",
    "\n",
    "    try:\n",
    "        all_scores_df.to_csv(temp_path, index=False)       # Save DataFrame to the temporary file\n",
    "        os.close(temp_fd)                                  # Close file descriptor\n",
    "        os.rename(temp_path, ALL_SCORES_CSV)               # Rename temporary file to final filename\n",
    "    except Exception as e:\n",
    "        os.close(temp_fd)                                  # Ensure file descriptor is closed in case of error\n",
    "        os.unlink(temp_path)                               # Remove the temporary file if an error occurs\n",
    "        raise e\n",
    "\n",
    "def get_best_structures(save_structures = False, seq_per_active_site = 100):\n",
    "    print(save_structures)\n",
    "    # Read the scores DataFrame\n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "    all_scores_df_og = all_scores_df.copy()\n",
    "    # Drop rows with NaN in 'total_score'\n",
    "    all_scores_df = all_scores_df.dropna(subset=['total_score'])\n",
    "\n",
    "    # Calculate the combined scores using the normalize_scores function\n",
    "    _, _, _, _, combined_scores = normalize_scores(all_scores_df, print_norm=False, norm_all=True)\n",
    "\n",
    "    all_scores_df['combined_score'] = combined_scores\n",
    "\n",
    "    all_scores_df['replicate_sequences'] = 0  # Initialize to count duplicates\n",
    "    all_scores_df['replicate_sequences_combined_score'] = 0.0  # To store the average score\n",
    "    all_scores_df['replicate_sequences_combined_score_std'] = 0.0  # To store the standard deviation\n",
    "\n",
    "    # Loop to find duplicates, calculate average score, and standard deviation\n",
    "    for i, row in all_scores_df.iterrows():\n",
    "        duplicates = all_scores_df[all_scores_df['sequence'] == row['sequence']]\n",
    "        avg_score = duplicates['combined_score'].mean()\n",
    "        std_dev = duplicates['combined_score'].std()\n",
    "\n",
    "        all_scores_df.at[i, 'replicate_sequences'] = len(duplicates)\n",
    "        all_scores_df.at[i, 'replicate_sequences_combined_score'] = avg_score\n",
    "        all_scores_df.at[i, 'replicate_sequences_combined_score_std'] = std_dev\n",
    "\n",
    "    # Remove replicates and keep only highest combined_score\n",
    "    all_scores_df.sort_values(by=['combined_score'], ascending=[False], inplace=True)\n",
    "    all_scores_df.drop_duplicates(subset=['sequence'], keep='first', inplace=True)\n",
    "\n",
    "    # Define Design group\n",
    "    def get_design_sequence(sequence, design_positions):\n",
    "        return ''.join(sequence[pos - 1] for pos in design_positions)\n",
    "\n",
    "    design_positions = [int(pos) for pos in DESIGN.split(',')]\n",
    "    #manually define AS here\n",
    "    #design_positions = [95, 84, 65, 15, 97, 116, 99, 58, 18, 54, 14, 55, 26, 114, 38, 112, 30, 36, 82, 98, 63]\n",
    "    all_scores_df['design_group'] = all_scores_df['sequence'].apply(lambda seq: get_design_sequence(seq, design_positions))\n",
    "\n",
    "    # Use the standard deviation selection for catalytic score\n",
    "    mean_catalytic_score = all_scores_df['catalytic_score'].mean()\n",
    "    std_catalytic_score = all_scores_df['catalytic_score'].std()\n",
    "    all_scores_df = all_scores_df[all_scores_df['catalytic_score'] < mean_catalytic_score + std_catalytic_score]\n",
    "\n",
    "    # Get the best variants while respecting the seq_per_active_site limit\n",
    "    top_variants = []\n",
    "    group_counts = {}\n",
    "\n",
    "    for _, row in all_scores_df.iterrows():\n",
    "        group = row['design_group']\n",
    "        if group not in group_counts:\n",
    "            group_counts[group] = 0\n",
    "        if group_counts[group] < seq_per_active_site:\n",
    "            top_variants.append(row)\n",
    "            group_counts[group] += 1\n",
    "        if len(top_variants) >= 100:\n",
    "            break\n",
    "\n",
    "    top100 = pd.DataFrame(top_variants)\n",
    "\n",
    "    selected_indices = np.array(top100['index'].tolist(), dtype=int)\n",
    "    # print(selected_indices)\n",
    "    # print(top100)\n",
    "\n",
    "    # Print average scores for top 100 and all data points\n",
    "    score_types = ['combined_score', 'total_score', 'interface_score', 'efield_score', 'catalytic_score', 'mutations']\n",
    "    print_average_scores(all_scores_df, top100, score_types)\n",
    "\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    best_structures_folder = os.path.join(FOLDER_HOME, 'best_structures')\n",
    "    os.makedirs(best_structures_folder, exist_ok=True)\n",
    "\n",
    "    # Copy files based on the top100 'index'\n",
    "    print(best_structures_folder)\n",
    "    if save_structures:\n",
    "        print(\"Saving...\")\n",
    "        for index, row in top100.iterrows():\n",
    "            geom_mean = \"{:.3f}\".format(row['combined_score'])\n",
    "            relax_file = f\"{FOLDER_HOME}/{int(index)}/{WT}_Rosetta_Relax_{int(index)}.pdb\"\n",
    "            design_file = f\"{FOLDER_HOME}/{int(index)}/{WT}_Rosetta_Design_{int(index)}.pdb\"\n",
    "            if os.path.isfile(relax_file):\n",
    "                src_file = relax_file\n",
    "            else:\n",
    "                src_file = design_file\n",
    "            dest_file = os.path.join(best_structures_folder, f\"{geom_mean}_{WT}_Rosetta_{os.path.basename(src_file)}\")\n",
    "            shutil.copy(src_file, dest_file)\n",
    "        print(\"Saved strucutures to: \", best_structures_folder)\n",
    "            \n",
    "    # Plot sorted total score, interface score, and efield score distributions\n",
    "    def plot_elbow_curve(scores_dict, title, top_indices):\n",
    "        sorted_scores = sorted(scores_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        indices, scores = zip(*sorted_scores)\n",
    "        colors = ['orange' if idx in top_indices else 'blue' for idx in indices]\n",
    "        alphas = [1.0 if idx in top_indices else 0.05 for idx in indices]\n",
    "        plt.scatter(range(len(scores)), scores, color=colors, alpha=alphas, s=1)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Variants')\n",
    "        plt.ylabel('Score')\n",
    "        plt.show()\n",
    "\n",
    "    top100_indices = set(top100['index'])\n",
    "\n",
    "    plot_elbow_curve(all_scores_df.set_index('index')['total_score'].to_dict(), 'Total Score Elbow Curve', top100_indices)\n",
    "    plot_elbow_curve(all_scores_df.set_index('index')['interface_score'].to_dict(), 'Interface Score Elbow Curve', top100_indices)\n",
    "    plot_elbow_curve(all_scores_df.set_index('index')['efield_score'].to_dict(), 'Efield Score Elbow Curve', top100_indices)\n",
    "    plot_elbow_curve(all_scores_df.set_index('index')['catalytic_score'].to_dict(), 'Catalytic Score Elbow Curve', top100_indices)\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "\n",
    "def trace_mutation_tree(all_scores_df, index):\n",
    "    mutations = []\n",
    "    offspring_counts = []\n",
    "    combined_scores = []\n",
    "    total_scores = []\n",
    "    interface_scores = []\n",
    "    efield_scores = []\n",
    "    generations = []\n",
    "\n",
    "    all_scores_df = all_scores_df.dropna(subset=['total_score'])\n",
    "    \n",
    "    # Calculate combined scores using normalized scores\n",
    "    _, _, _, _, combined_scores_normalized = normalize_scores(all_scores_df, print_norm=True, norm_all=True)\n",
    "    \n",
    "    # Add combined scores to the DataFrame\n",
    "    all_scores_df['combined_score'] = combined_scores_normalized\n",
    "\n",
    "    # Cast index column to int\n",
    "    all_scores_df['index'] = all_scores_df['index'].astype(int)\n",
    "    all_scores_df['parent_index'] = all_scores_df['parent_index'].apply(lambda x: int(x) if x != \"Parent\" else x)\n",
    "\n",
    "    def get_mutations(parent_seq, child_seq):\n",
    "        return [f\"{p}{i+1}{c}\" for i, (p, c) in enumerate(zip(parent_seq, child_seq)) if p != c]\n",
    "\n",
    "    def count_offspring(all_scores_df, parent_index):\n",
    "        children = all_scores_df[all_scores_df['parent_index'] == parent_index]\n",
    "        count = len(children)\n",
    "        for child_index in children['index']:\n",
    "            count += count_offspring(all_scores_df, child_index)\n",
    "        return count\n",
    "\n",
    "    total_variants = len(all_scores_df)\n",
    "    total_mutations = int(all_scores_df.loc[all_scores_df['index'] == index, 'mutations'].values[0])\n",
    "    current_index = index\n",
    "    accumulated_mutations = 0\n",
    "\n",
    "    while current_index in all_scores_df['index'].values:\n",
    "        current_row = all_scores_df[all_scores_df['index'] == current_index].iloc[0]\n",
    "        parent_index = current_row['parent_index']\n",
    "        \n",
    "        if parent_index in all_scores_df['index'].values:\n",
    "            parent_row = all_scores_df[all_scores_df['index'] == parent_index].iloc[0]\n",
    "            parent_seq = parent_row['sequence']\n",
    "            child_seq = current_row['sequence']\n",
    "            mutation = get_mutations(parent_seq, child_seq)\n",
    "            offspring_count = count_offspring(all_scores_df, parent_index)\n",
    "            \n",
    "            mutations.append(mutation)\n",
    "            offspring_counts.append(offspring_count)\n",
    "            generations.append(current_row['generation'])\n",
    "            \n",
    "            # Store actual scores\n",
    "            combined_scores.append(current_row['combined_score'])\n",
    "            total_scores.append(current_row['total_score'])\n",
    "            interface_scores.append(current_row['interface_score'])\n",
    "            efield_scores.append(current_row['efield_score'])\n",
    "        \n",
    "        current_index = parent_index\n",
    "\n",
    "    # Plot the actual scores\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    def plot_scores(ax, scores, title):\n",
    "        ax.plot(generations[::-1], scores[::-1], marker='o', linestyle='-', color='b')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Generation')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.grid(True, linestyle='--', which='major', color='grey', alpha=0.7)\n",
    "\n",
    "    plot_scores(axs[0, 0], combined_scores, 'Combined Score vs Generations')\n",
    "    plot_scores(axs[0, 1], total_scores, 'Total Score vs Generations')\n",
    "    plot_scores(axs[1, 0], interface_scores, 'Interface Score vs Generations')\n",
    "    plot_scores(axs[1, 1], efield_scores, 'Efield Score vs Generations')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return mutations[::-1], offspring_counts[::-1], combined_scores[::-1], total_scores[::-1], interface_scores[::-1], efield_scores[::-1]\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "def print_average_scores(all_scores_df, top100, score_types):\n",
    "    print(\"\\nSummary of Average Scores:\")\n",
    "    print(f\"{'Score Type':<20} {'Average of All':<20} {'Average of Top 100':<20}\")\n",
    "    print(\"=\"*60)\n",
    "    for score_type in score_types:\n",
    "        avg_all = all_scores_df[score_type].mean()\n",
    "        avg_top100 = top100[score_type].mean()\n",
    "        print(f\"{score_type.replace('_', ' ').title():<20} {avg_all:<20.4f} {avg_top100:<20.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "def run_command(command, cwd=None, capture_output=False):\n",
    "    \"\"\"Wrapper to execute .py files in runtime with arguments, and print error messages if they occur.\n",
    "    \n",
    "    Parameters:\n",
    "    - command: The command to run as a list of strings.\n",
    "    - cwd: Optional; The directory to execute the command in.\n",
    "    - capture_output: Optional; If True, capture stdout and stderr. Defaults to False (This is to conserve memory).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If capture_output is True, capture stdout and stderr\n",
    "        if capture_output:\n",
    "            result = subprocess.run(command, capture_output=True, text=True, check=True, cwd=cwd)\n",
    "        else:\n",
    "            # If capture_output is False, suppress all output by redirecting to os.devnull\n",
    "            with open(os.devnull, 'w') as fnull:\n",
    "                result = subprocess.run(command, stdout=fnull, stderr=fnull, text=True, check=True, cwd=cwd)\n",
    "        return result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Command '{e.cmd}' failed with return code {e.returncode}\")\n",
    "        logging.error(e.stderr)\n",
    "        #maybe rerun command here in case of efields\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while running command: {command}\")\n",
    "        raise\n",
    "\n",
    "def wait_for_file(file_path, timeout=5):\n",
    "    \"\"\"Wait for a file to exist and have a non-zero size.\"\"\"\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "            return True\n",
    "        time.sleep(0.1)  # Wait for 0.1 seconds before checking again\n",
    "    return False\n",
    "\n",
    "def hamming_distance(seq1, seq2):\n",
    "    \"\"\"Calculate the Hamming distance between two strings.\"\"\"\n",
    "    if not isinstance(seq2, str):\n",
    "        return None\n",
    "    if len(seq1) != len(seq2):\n",
    "        raise ValueError(\"Sequences must be of equal length\")\n",
    "    return sum(ch1 != ch2 for ch1, ch2 in zip(seq1, seq2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d0d19",
   "metadata": {},
   "source": [
    "## Electric Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb417fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_AMBER_files(structure_filename : str):\n",
    "    '''Uses tleap to create a .parm7 and .rst7 file from a pdb. Requires ambertools and pdb-tools \n",
    "    (pip installable from  https://www.bonvinlab.org/pdb-tools/).\n",
    "    Also requires 5TS.prepi and 5TS.frcmod in the INPUT folder\n",
    "    TODO: Add script to generate these if not present.\n",
    "    \n",
    "    Parameters:\n",
    "    - structure_filename (str): The path to the pdb file to analyse without the file extension.'''\n",
    "\n",
    "    #delete everything after 'CONECT' - otherwise Amber tries to read it as extra residues\n",
    "    clean_pdb = run_command(['sed', '-n', '/CONECT/q;p', f\"{structure_filename}.pdb\"], capture_output=True)\n",
    "    with open(f\"{structure_filename}_clean.pdb\", \"w\") as f:\n",
    "        f.write(clean_pdb)\n",
    "\n",
    "    # with open(f\"{structure_filename}_clean.pdb\", \"w\") as f:\n",
    "    #     subprocess.call(['sed', '-n', '/CONECT/q;p', f\"{structure_filename}.pdb\"], stdout=f)\n",
    "        \n",
    "    #remove hydrogens - requires pip install of https://www.bonvinlab.org/pdb-tools/\n",
    "    noHclean_pdb = run_command(['pdb_delelem', '-H', f\"{structure_filename}_clean.pdb\"], capture_output=True)\n",
    "    with open(f\"{structure_filename}_noHclean.pdb\", \"w\") as f:\n",
    "        f.write(noHclean_pdb)\n",
    "\n",
    "    # with open(f\"{structure_filename}_noHclean.pdb\", \"w\") as f:\n",
    "    #     subprocess.call(['pdb_delelem', '-H', f\"{structure_filename}_clean.pdb\"], stdout=f)\n",
    "\n",
    "    os.remove(f\"{structure_filename}_clean.pdb\")\n",
    "\n",
    "    with open(\"tleap.in\", \"w\") as f:\n",
    "            f.write(f\"\"\"source leaprc.protein.ff19SB \n",
    "    source leaprc.gaff\n",
    "    loadamberprep   Input/5TS.prepi\n",
    "    loadamberparams Input/5TS.frcmod\n",
    "    mol = loadpdb {structure_filename}_noHclean.pdb\n",
    "    saveamberparm mol {structure_filename}.parm7 {structure_filename}.rst7\n",
    "    quit\n",
    "    \"\"\")\n",
    "            \n",
    "    run_command([\"tleap\", \"-s\", \"-f\", \"tleap.in\"])\n",
    "\n",
    "def calc_efields_score(pdb_path):\n",
    "    '''Executes the FieldTools.py script to calculate the electric field across the C-H bond of 5TS.\n",
    "    Requires a field_target.dat in the Input folder. Currently hard-coded based on 5TS\n",
    "    TODO: Make this function agnostic to contents of field_target\n",
    "    \n",
    "    Parameters:\n",
    "    - pdb_path (str): The path to the pdb structure to analyse - either from design or relax.\n",
    "    \n",
    "    Returns:\n",
    "    - bond_field (float): The total electric field across the 5TS@C9_:5TS@H04 bond in MV/cm. (Currently hard coded to these atoms)\n",
    "    - all_fields (dict): The components of the electric field across the 5TS@C9_:5TS@H04 bond per residue.'''\n",
    "\n",
    "    structure_filename = os.path.splitext(pdb_path)[0]\n",
    "\n",
    "    generate_AMBER_files(structure_filename)\n",
    "\n",
    "    run_command([\"python\", f\"{FIELD_TOOLS}\", \n",
    "                \"-nc\", f\"{os.path.relpath(structure_filename)}.rst7\", \n",
    "                \"-parm\", f\"{os.path.relpath(structure_filename)}.parm7\", \n",
    "                \"-out\", f\"{os.path.relpath(structure_filename)}_fields.pkl\", \n",
    "                \"-target\", \"Input/field_target.dat\", \n",
    "                \"-solvent\", \"WAT\"])\n",
    "    \n",
    "\n",
    "    with open(f\"{structure_filename}_fields.pkl\", \"rb\") as f:\n",
    "        FIELDS = pkl.load(f)\n",
    "\n",
    "    bond_field = FIELDS[':5TS@C9_:5TS@H04']['Total']\n",
    "    all_fields = FIELDS[':5TS@C9_:5TS@H04']\n",
    "\n",
    "    return bond_field[0], all_fields\n",
    "\n",
    "def update_efieldsdf(index:int, index_efields_dict:dict):\n",
    "    '''Adds a new row to \"{FOLDER_HOME}/electric_fields.csv\" containing the electric fields \n",
    "    generated by FieldTools.py for all residues in the protein'''\n",
    "\n",
    "    no_residues = len(index_efields_dict)-4\n",
    "\n",
    "    gen_headers = [\"Total\",\"Protein\",\"Solvent\",\"WAT\"]\n",
    "    resi_headers = [f\"RESI_{idx}\" for idx in range(1,no_residues+1)]\n",
    "    headers = gen_headers + resi_headers\n",
    "\n",
    "    fields_list = [field[0] for field in index_efields_dict.values()]\n",
    "\n",
    "    if not os.path.isfile(f\"{FOLDER_HOME}/electric_fields.csv\"):       \n",
    "        fields_df = pd.DataFrame([fields_list], columns=headers, index=[index])\n",
    "        fields_df.to_csv(f\"{FOLDER_HOME}/electric_fields.csv\") \n",
    "\n",
    "    else:\n",
    "        fields_df = pd.read_csv(f\"{FOLDER_HOME}/electric_fields.csv\", index_col=0)\n",
    "        new_row_df = pd.DataFrame([fields_list], columns=headers, index=[index])\n",
    "        fields_df = pd.concat([fields_df, new_row_df])\n",
    "        fields_df.sort_index(inplace=True)\n",
    "        fields_df.to_csv(f\"{FOLDER_HOME}/electric_fields.csv\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2608bedf",
   "metadata": {},
   "source": [
    "# plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(combined_score_min=0, combined_score_max=1, combined_score_bin=0.01,\n",
    "                interface_score_min=0, interface_score_max=1, interface_score_bin=0.01,\n",
    "                total_score_min=0, total_score_max=1, total_score_bin=0.01,\n",
    "                catalytic_score_min=0, catalytic_score_max=1, catalytic_score_bin=0.01,\n",
    "                mut_min=0,mut_max=len(DESIGN.split(\",\"))+1):\n",
    "        \n",
    "    # Break because file does not exist\n",
    "    if not os.path.isfile(ALL_SCORES_CSV): return\n",
    "    \n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "    all_scores_df['sequence'] = all_scores_df['sequence'].astype(str)\n",
    "    all_scores_df['design_method'] = all_scores_df['design_method'].astype(str)\n",
    "    all_scores_df['score_taken_from'] = all_scores_df['score_taken_from'].astype(str)\n",
    "            \n",
    "    # Break because not enough data\n",
    "    if len(all_scores_df.dropna(subset=['total_score'])) < 3: return\n",
    "    \n",
    "\n",
    "    if (ProteinMPNN_PROB > 0 or LMPNN_PROB > 0):\n",
    "        #first not nan sequence from all_scores_df\n",
    "        mut_max = len(all_scores_df[all_scores_df['sequence'] != 'nan']['sequence'].iloc[0])   \n",
    "        \n",
    "    # Plot data\n",
    "    fig, axs = plt.subplots(3, 4, figsize=(15, 9))\n",
    "    \n",
    "    all_scores_df = all_scores_df.dropna(subset=['total_score'])\n",
    "    catalytic_scores, total_scores, interface_scores, efield_scores, combined_scores = normalize_scores(all_scores_df, \n",
    "                                                                                         print_norm=True,\n",
    "                                                                                         norm_all=True)\n",
    "            \n",
    "    plot_combined_score(axs[0,0], combined_scores, \\\n",
    "                        combined_score_min, combined_score_max, combined_score_bin)\n",
    "    plot_interface_score(axs[0,1], interface_scores, \\\n",
    "                         interface_score_min, interface_score_max, interface_score_bin)\n",
    "    plot_total_score(axs[0,2], total_scores, \\\n",
    "                     total_score_min, total_score_max, total_score_bin)\n",
    "    plot_catalytic_score(axs[0,3], catalytic_scores, \\\n",
    "                         catalytic_score_min, catalytic_score_max, catalytic_score_bin)\n",
    "    plot_efield_score(axs[2,0], efield_scores, \\\n",
    "                        catalytic_score_min, catalytic_score_max, catalytic_score_bin)\n",
    "    \n",
    "    plot_boltzmann_histogram(axs[1,0], combined_scores, all_scores_df, \\\n",
    "                             combined_score_min, combined_score_max, combined_score_bin)\n",
    "    plot_combined_score_v_index(axs[1,1], combined_scores, all_scores_df)\n",
    "    plot_combined_score_v_generation_violin(axs[1,2], combined_scores, all_scores_df)\n",
    "    plot_mutations_v_generation_violin(axs[1,3], all_scores_df, mut_min, mut_max)\n",
    "    plot_score_v_generation_violin(axs[2,1], 'total_score', all_scores_df)\n",
    "    plot_score_v_generation_violin(axs[2,2], 'interface_score', all_scores_df)\n",
    "    plot_score_v_generation_violin(axs[2,3], 'efield_score', all_scores_df)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(15, 2.5))\n",
    "    # plot_tree(ax, combined_scores, all_scores_df, G)\n",
    "    # plt.show()\n",
    "\n",
    "def plot_summary():\n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 2, figsize=(20, 23))\n",
    "\n",
    "    # Normalize scores\n",
    "    catalytic_scores, total_scores, interface_scores, efield_scores, combined_scores = normalize_scores(all_scores_df, \n",
    "                                                                                         print_norm=True,\n",
    "                                                                                         norm_all=True)\n",
    "    \n",
    "    # Plot interface vs total score colored by generation\n",
    "    plot_interface_v_total_score_generation(axs[0,0], total_scores, interface_scores, all_scores_df['generation'])\n",
    "\n",
    "    # Plot stacked histogram of interface scores by generation\n",
    "    plot_stacked_histogram_by_generation(axs[0, 1], all_scores_df)\n",
    "\n",
    "    # Plot stacked histogram of interface scores by catalytic residue index\n",
    "        # Create a consistent color map for catalytic residues\n",
    "    all_scores_df['cat_resi'] = pd.to_numeric(all_scores_df['cat_resi'], errors='coerce')\n",
    "    unique_cat_resi = all_scores_df['cat_resi'].dropna().unique()\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_cat_resi)))\n",
    "    color_map = {resi: colors[i] for i, resi in enumerate(unique_cat_resi)}\n",
    "    plot_stacked_histogram_by_cat_resi(axs[1, 0], all_scores_df, color_map=color_map)\n",
    "\n",
    "    # Plot stacked histogram of interface scores by catalytic residue index (excluding generation 0)\n",
    "    all_scores_df_cleaned = all_scores_df[all_scores_df['generation'] != 0]  # Exclude generation 0\n",
    "    plot_stacked_histogram_by_cat_resi(axs[1, 1], all_scores_df_cleaned, color_map=color_map)\n",
    "\n",
    "    # Plot interface vs total score colored by catalytic residue index\n",
    "    plot_interface_v_total_score_cat_resi(axs[2,0], total_scores, interface_scores, all_scores_df['cat_resi'])\n",
    "\n",
    "    # Plot interface vs total score colored by catalytic residue name\n",
    "    legend_elements = plot_interface_v_total_score_cat_resn(axs[2, 1], total_scores, interface_scores, all_scores_df['cat_resn'])\n",
    "    \n",
    "    axs[0,0].set_title('Total Scores vs Interface Scores by Generation')\n",
    "    axs[0,1].set_title('Stacked Histogram of Interface Scores by Generation')\n",
    "    axs[1,0].set_title('Stacked Histogram of Interface Scores by Catalytic Residue Index')\n",
    "    axs[1,1].set_title('Stacked Histogram of Interface Scores by Catalytic Residue Index (Excluding Generation 0)')\n",
    "    axs[2,0].set_title('Total Scores vs Interface Scores by Catalytic Residue Index')\n",
    "    axs[2,1].set_title('Total Scores vs Interface Scores by Catalytic Residue Name')\n",
    "\n",
    "    # Adjust legends\n",
    "    # For cat_resi\n",
    "    handles_cat_resi, labels_cat_resi = axs[1,1].get_legend_handles_labels()\n",
    "    fig.legend(handles_cat_resi, labels_cat_resi, loc='upper right', bbox_to_anchor=(0.95, 0.65), title=\"Catalytic Residue Index\")\n",
    "\n",
    "    # For generation\n",
    "    handles_generation, labels_generation = axs[0,1].get_legend_handles_labels()\n",
    "    fig.legend(handles_generation, labels_generation, loc='upper right', bbox_to_anchor=(0.95, 0.98), title=\"Generation\")\n",
    "\n",
    "    # For cat_resn\n",
    "    handles_cat_resn, labels_cat_resn = axs[2,1].get_legend_handles_labels()\n",
    "    fig.legend(handles = legend_elements, loc='upper right', bbox_to_anchor=(0.95, 0.31), title=\"Catalytic Residue\")\n",
    "\n",
    "    # Adjust layout to make space for the legends on the right\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "def plot_interface_v_total_score_selection(ax, total_scores, interface_scores, selected_indices):\n",
    "    \"\"\"\n",
    "    Plots a scatter plot of total_scores vs interface_scores and highlights the points\n",
    "    corresponding to the selected indices.\n",
    "\n",
    "    Parameters:\n",
    "    - ax (matplotlib.axes.Axes): The Axes object to plot on.\n",
    "    - total_scores (list or np.array): The total scores of the structures.\n",
    "    - interface_scores (list or np.array): The interface scores of the structures.\n",
    "    - selected_indices (list of int): Indices of the points to highlight.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a mask for selected indices\n",
    "    mask = np.ones(len(total_scores), dtype=bool)  # Initialize mask to include all points\n",
    "    mask[selected_indices] = False  # Exclude selected indices\n",
    "    \n",
    "    # Plot all points excluding the selected ones on the given Axes object\n",
    "    ax.scatter(total_scores[mask], interface_scores[mask], color='gray', alpha=0.3, label='All Points', s=1)\n",
    "    \n",
    "    # Highlight selected points on the given Axes object\n",
    "    ax.scatter(total_scores[selected_indices], interface_scores[selected_indices], color='red', alpha=0.4, label='Selected Points', s=1)\n",
    "    \n",
    "    ax.set_title('Total Scores vs Interface Scores')\n",
    "    ax.set_xlabel('Total Score')\n",
    "    ax.set_ylabel('Interface Score')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "def plot_interface_v_total_score_cat_resi(ax, total_scores, interface_scores, cat_resi):\n",
    "    from matplotlib.lines import Line2D\n",
    "    \"\"\"\n",
    "    Plots a scatter plot of total_scores vs interface_scores and colors the points\n",
    "    according to the catalytic residue number (cat_resi) for all data points, using categorical coloring.\n",
    "    Adds a legend to represent each unique catalytic residue number with its corresponding color.\n",
    "\n",
    "    Parameters:\n",
    "    - ax (matplotlib.axes.Axes): The Axes object to plot on.\n",
    "    - total_scores (list or np.array): The total scores of the structures.\n",
    "    - interface_scores (list or np.array): The interface scores of the structures.\n",
    "    - cat_resi (pd.Series or np.array): Catalytic residue numbers for all data points.\n",
    "    \"\"\"\n",
    "    # Ensure cat_resi is a pandas Series for easier handling and remove NaN values\n",
    "    if not isinstance(cat_resi, pd.Series):\n",
    "        cat_resi = pd.Series(cat_resi)\n",
    "    cat_resi = cat_resi.dropna()  # Drop NaN values\n",
    "    \n",
    "    # Proceed with the rest of the function after removing NaN values\n",
    "    unique_cat_resi = cat_resi.unique()\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_cat_resi)))\n",
    "\n",
    "    color_map = {resi: colors[i] for i, resi in enumerate(unique_cat_resi)}\n",
    "    cat_resi_colors = cat_resi.map(color_map).values\n",
    "\n",
    "    scatter = ax.scatter(total_scores[cat_resi.index], interface_scores[cat_resi.index], c=cat_resi_colors, alpha=0.4, s=2)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'Cat Resi {resi}',\n",
    "                              markerfacecolor=color_map[resi], markersize=10) for resi in unique_cat_resi]\n",
    "    #ax.legend(handles=legend_elements, title=\"Catalytic Residue\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    ax.set_title('Total Scores vs Interface Scores')\n",
    "    ax.set_xlabel('Total Score')\n",
    "    ax.set_ylabel('Interface Score')\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "def plot_interface_v_total_score_cat_resn(ax, total_scores, interface_scores, cat_resn):\n",
    "    from matplotlib.lines import Line2D\n",
    "    \"\"\"\n",
    "    Plots a scatter plot of total_scores vs interface_scores and colors the points\n",
    "    according to the catalytic residue name (cat_resn) for all data points, using categorical coloring.\n",
    "    Adds a legend to represent each unique catalytic residue name with its corresponding color.\n",
    "\n",
    "    Parameters:\n",
    "    - ax (matplotlib.axes.Axes): The Axes object to plot on.\n",
    "    - total_scores (list or np.array): The total scores of the structures.\n",
    "    - interface_scores (list or np.array): The interface scores of the structures.\n",
    "    - cat_resn (pd.Series or np.array): Catalytic residue names for all data points.\n",
    "    \"\"\"\n",
    "    # Ensure cat_resn is a pandas Series for easier handling and remove NaN values\n",
    "    if not isinstance(cat_resn, pd.Series):\n",
    "        cat_resn = pd.Series(cat_resn)\n",
    "    cat_resn = cat_resn.dropna()  # Drop NaN values\n",
    "    \n",
    "    # Proceed with the rest of the function after removing NaN values\n",
    "    unique_cat_resn = cat_resn.unique()\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_cat_resn)))\n",
    "\n",
    "    color_map = {resn: colors[i] for i, resn in enumerate(unique_cat_resn)}\n",
    "    cat_resn_colors = cat_resn.map(color_map).values\n",
    "\n",
    "    scatter = ax.scatter(total_scores[cat_resn.index], interface_scores[cat_resn.index], c=cat_resn_colors, alpha=0.4, s=2)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'Cat Resn {resn}',\n",
    "                              markerfacecolor=color_map[resn], markersize=10) for resn in unique_cat_resn]\n",
    "    #ax.legend(handles=legend_elements, title=\"Catalytic Residue\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    ax.set_title('Total Scores vs Interface Scores')\n",
    "    ax.set_xlabel('Total Score')\n",
    "    ax.set_ylabel('Interface Score')\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    return legend_elements\n",
    "\n",
    "def plot_interface_v_total_score_generation(ax, total_scores, interface_scores, generation):\n",
    "    \"\"\"\n",
    "    Plots a scatter plot of total_scores vs interface_scores and colors the points\n",
    "    according to the generation for all data points, using categorical coloring.\n",
    "    Adds a legend to represent each unique generation with its corresponding color.\n",
    "\n",
    "    Parameters:\n",
    "    - ax (matplotlib.axes.Axes): The Axes object to plot on.\n",
    "    - total_scores (list or np.array): The total scores of the structures.\n",
    "    - interface_scores (list or np.array): The interface scores of the structures.\n",
    "    - generation (pd.Series or np.array): Generation numbers for all data points.\n",
    "    \"\"\"\n",
    "    if not isinstance(generation, pd.Series):\n",
    "        generation = pd.Series(generation)\n",
    "    generation = generation.dropna()  # Drop NaN values\n",
    "    \n",
    "    unique_generations = generation.unique()\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_generations)))\n",
    "\n",
    "    color_map = {gen: colors[i] for i, gen in enumerate(unique_generations)}\n",
    "    \n",
    "    # Loop through each generation to plot, adjusting alpha for generation 0\n",
    "    for gen in unique_generations:\n",
    "        gen_mask = generation == gen\n",
    "        alpha_value = 0.2 if gen == 0 else 0.8  # More transparent for generation 0\n",
    "        ax.scatter(total_scores[generation.index][gen_mask], interface_scores[generation.index][gen_mask], \n",
    "                   c=[color_map[gen]], alpha=alpha_value, s=2, label=f'Generation {gen}' if gen == 0 else None)\n",
    "    #ax.legend(handles=legend_elements, title=\"Generation\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    ax.set_title('Total vs Interface Scores - Generation', fontsize=18)\n",
    "    ax.set_xlabel('Total Score', fontsize=16)\n",
    "    ax.set_ylabel('Interface Score', fontsize=16)\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "def plot_stacked_histogram_by_cat_resi(ax, all_scores_df, color_map=None, show_legend=False):\n",
    "    \"\"\"\n",
    "    Plots a stacked bar plot of interface scores colored by cat_resi on the given Axes object,\n",
    "    where each bar's segments represent counts of different cat_resi values in that bin.\n",
    "\n",
    "    Parameters:\n",
    "    - ax (matplotlib.axes.Axes): The Axes object to plot on.\n",
    "    - all_scores_df (pd.DataFrame): DataFrame containing 'cat_resi' and 'interface_score' columns.\n",
    "    - color_map (dict): Optional; A dictionary mapping catalytic residue indices to colors.\n",
    "    - show_legend (bool): Optional; Whether to show the legend. Defaults to False.\n",
    "    \"\"\"\n",
    "    # Drop rows with NaN in 'interface_score'\n",
    "    all_scores_df_cleaned = all_scores_df.dropna(subset=['interface_score'])\n",
    "\n",
    "    # Ensure cat_resi is numeric and drop NaN values\n",
    "    all_scores_df_cleaned['cat_resi'] = pd.to_numeric(all_scores_df_cleaned['cat_resi'], errors='coerce').dropna()\n",
    "    unique_cat_resi = all_scores_df_cleaned['cat_resi'].unique()\n",
    "\n",
    "    # Use the provided color_map or generate a new one\n",
    "    if color_map is None:\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, len(unique_cat_resi)))  # Generate colors for unique cat_resi values\n",
    "        color_map = {resi: colors[i] for i, resi in enumerate(unique_cat_resi)}  # Map cat_resi to colors\n",
    "\n",
    "    # Define bins for the histogram\n",
    "    bins = np.linspace(all_scores_df_cleaned['interface_score'].min(), all_scores_df_cleaned['interface_score'].max(), 21)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "    # Calculate counts for each cat_resi in each bin\n",
    "    counts = {resi: np.histogram(all_scores_df_cleaned[all_scores_df_cleaned['cat_resi'] == resi]['interface_score'], bins=bins)[0] for resi in unique_cat_resi}\n",
    "\n",
    "    # Plot stacked bars for each bin\n",
    "    bottom = np.zeros(len(bin_centers))\n",
    "    for resi in unique_cat_resi:\n",
    "        ax.bar(bin_centers, counts[resi], bottom=bottom, width=np.diff(bins), label=f'Cat Resi {resi}', color=color_map[resi], align='center')\n",
    "        bottom += counts[resi]\n",
    "\n",
    "    # Create a custom legend\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'Cat Resi {resi}',\n",
    "                              markerfacecolor=color_map[resi], markersize=10) for resi in unique_cat_resi]\n",
    "    if show_legend:\n",
    "        ax.legend(handles=legend_elements, title=\"Catalytic Residue\")\n",
    "\n",
    "    ax.set_title('Stacked Histogram of Interface Scores')\n",
    "    ax.set_xlabel('Interface Score')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "    ax.set_xlim(-32.5, -13.5)\n",
    "\n",
    "def plot_stacked_histogram_by_cat_resn(ax, all_scores_df):\n",
    "    \"\"\"\n",
    "    Plots a stacked bar plot of interface scores colored by cat_resn on the given Axes object,\n",
    "    where each bar's segments represent counts of different cat_resn values in that bin.\n",
    "\n",
    "    Parameters:\n",
    "    - ax (matplotlib.axes.Axes): The Axes object to plot on.\n",
    "    - all_scores_df (pd.DataFrame): DataFrame containing 'cat_resn' and 'interface_score' columns.\n",
    "    \"\"\"\n",
    "    # Drop rows with NaN in 'interface_score'\n",
    "    all_scores_df_cleaned = all_scores_df.dropna(subset=['interface_score'])\n",
    "\n",
    "    # Ensure cat_resn is a string and drop NaN values\n",
    "    all_scores_df_cleaned['cat_resn'] = all_scores_df_cleaned['cat_resn'].astype(str).dropna()\n",
    "    unique_cat_resn = all_scores_df_cleaned['cat_resn'].unique()\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_cat_resn)))  # Generate colors for unique cat_resn values\n",
    "\n",
    "    color_map = {resn: colors[i] for i, resn in enumerate(unique_cat_resn)}  # Map cat_resn to colors\n",
    "\n",
    "    # Define bins for the histogram\n",
    "    bins = np.linspace(all_scores_df_cleaned['interface_score'].min(), all_scores_df_cleaned['interface_score'].max(), 21)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "    # Calculate counts for each cat_resn in each bin\n",
    "    counts = {resn: np.histogram(all_scores_df_cleaned[all_scores_df_cleaned['cat_resn'] == resn]['interface_score'], bins=bins)[0] for resn in unique_cat_resn}\n",
    "\n",
    "    # Plot stacked bars for each bin\n",
    "    bottom = np.zeros(len(bin_centers))\n",
    "    for resn in unique_cat_resn:\n",
    "        ax.bar(bin_centers, counts[resn], bottom=bottom, width=np.diff(bins), label=f'Cat Resn {resn}', color=color_map[resn], align='center')\n",
    "        bottom += counts[resn]\n",
    "    \n",
    "    ax.set_xlim(-32.5, -13.5)\n",
    "\n",
    "    # Create a custom legend\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'Cat Resn {resn}',\n",
    "                              markerfacecolor=color_map[resn], markersize=10) for resn in unique_cat_resn]\n",
    "    #ax.legend(handles=legend_elements, title=\"Catalytic Residue\")\n",
    "\n",
    "    ax.set_title('Stacked Histogram of Interface Scores by Catalytic Residue')\n",
    "    ax.set_xlabel('Interface Score')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "def plot_stacked_histogram_by_generation(ax, all_scores_df):\n",
    "    \"\"\"\n",
    "    Plots a stacked bar plot of interface scores colored by generation on the given Axes object,\n",
    "    where each bar's segments represent counts of different generation values in that bin.\n",
    "\n",
    "    Parameters:\n",
    "    - ax (matplotlib.axes.Axes): The Axes object to plot on.\n",
    "    - all_scores_df (pd.DataFrame): DataFrame containing 'generation' and 'interface_score' columns.\n",
    "    \"\"\"\n",
    "    all_scores_df_cleaned = all_scores_df.dropna(subset=['interface_score', 'generation'])\n",
    "\n",
    "    all_scores_df_cleaned['generation'] = pd.to_numeric(all_scores_df_cleaned['generation'], errors='coerce').dropna()\n",
    "    unique_generations = all_scores_df_cleaned['generation'].unique()\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_generations)))\n",
    "\n",
    "    color_map = {gen: colors[i] for i, gen in enumerate(unique_generations)}\n",
    "\n",
    "    bins = np.linspace(all_scores_df_cleaned['interface_score'].min(), all_scores_df_cleaned['interface_score'].max(), 21)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "    counts = {gen: np.histogram(all_scores_df_cleaned[all_scores_df_cleaned['generation'] == gen]['interface_score'], bins=bins)[0] for gen in unique_generations}\n",
    "\n",
    "    bottom = np.zeros(len(bin_centers))\n",
    "    for gen in unique_generations:\n",
    "        ax.bar(bin_centers, counts[gen], bottom=bottom, width=np.diff(bins), label=f'Generation {gen}', color=color_map[gen], align='center')\n",
    "        bottom += counts[gen]\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'Generation {gen}',\n",
    "                              markerfacecolor=color_map[gen], markersize=10) for gen in unique_generations]\n",
    "    #ax.legend(handles=legend_elements, title=\"Generation\")\n",
    "\n",
    "    ax.set_title('Stacked Histogram of Interface Scores by Generation')\n",
    "    ax.set_xlabel('Interface Score')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "    ax.set_xlim(-32.5, -13.5)\n",
    "\n",
    "def plot_combined_score(ax, combined_scores, score_min, score_max, score_bin):\n",
    "    \n",
    "    ax.hist(combined_scores, bins=np.arange(score_min,score_max+score_bin,score_bin))\n",
    "    ax.axvline(HIGHSCORE, color='b')\n",
    "    ax.axvline(NEG_BEST, color='r')\n",
    "    ax.set_xlim(score_min,score_max)\n",
    "    ax.set_title('Histogram of Score')\n",
    "    ax.set_xlabel('Combined Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "def plot_interface_score(ax, interface_scores, interface_score_min, interface_score_max, interface_score_bin):\n",
    "        \n",
    "    ax.hist(interface_scores, density=True,\n",
    "            bins=np.arange(interface_score_min,interface_score_max+interface_score_bin,interface_score_bin))\n",
    "    ax.set_xlim(interface_score_min,interface_score_max)\n",
    "    ax.set_title('Histogram of Interface Score')\n",
    "    ax.set_xlabel('Interface Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "def plot_total_score(ax, total_scores, total_score_min, total_score_max, total_score_bin):\n",
    "\n",
    "    ax.hist(total_scores, density=True,\n",
    "            bins=np.arange(total_score_min,total_score_max+total_score_bin,total_score_bin))\n",
    "    ax.set_xlim(total_score_min,total_score_max)\n",
    "    ax.set_title('Histogram of Total Score')\n",
    "    ax.set_xlabel('Total Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "def plot_catalytic_score(ax, catalytic_scores, total_score_min, total_score_max, total_score_bin):\n",
    "\n",
    "    ax.hist(catalytic_scores, density=True,\n",
    "            bins=np.arange(total_score_min,total_score_max+total_score_bin,total_score_bin))\n",
    "    ax.set_xlim(total_score_min,total_score_max)\n",
    "    ax.set_title('Histogram of Catalytic Score')\n",
    "    ax.set_xlabel('Catalytic Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "def plot_efield_score(ax, efield_scores, total_score_min, total_score_max, total_score_bin):\n",
    "\n",
    "    ax.hist(efield_scores, density=True,\n",
    "            bins=np.arange(total_score_min,total_score_max+total_score_bin,total_score_bin))\n",
    "    ax.set_xlim(total_score_min,total_score_max)\n",
    "    ax.set_title('Histogram of Efield Score')\n",
    "    ax.set_xlabel('Efield Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "def plot_boltzmann_histogram(ax, combined_scores, all_scores_df, score_min, score_max, score_bin):\n",
    "\n",
    "    _, _, _, _, combined_potentials = normalize_scores(all_scores_df, print_norm=False, norm_all=False, extension=\"score\")\n",
    "            \n",
    "    if isinstance(KBT_BOLTZMANN, (float, int)):\n",
    "        kbt_boltzmann = KBT_BOLTZMANN\n",
    "    else:\n",
    "        if len(KBT_BOLTZMANN) == 2:\n",
    "            kbt_boltzmann = max(KBT_BOLTZMANN[0] * np.exp(-KBT_BOLTZMANN[1]*all_scores_df['index'].max()), 0.05)\n",
    "    boltzmann_factors = np.exp(combined_potentials / (kbt_boltzmann)) \n",
    "    print(f\"Min/Max boltzmann factors: {min(boltzmann_factors)}, {max(boltzmann_factors)}\")\n",
    "    probabilities     = boltzmann_factors / sum(boltzmann_factors) \n",
    "    \n",
    "    random_scores     = np.random.choice(combined_potentials, size=10000, replace=True)\n",
    "    boltzmann_scores  = np.random.choice(combined_potentials, size=10000, replace=True, p=probabilities)\n",
    "\n",
    "\n",
    "    # Plot the first histogram\n",
    "    ax.hist(random_scores, density=True, alpha=0.7, label='Random Sampling', \\\n",
    "            bins=np.arange(score_min-2,score_max+1+score_bin,score_bin))\n",
    "    ax.text(0.05, 0.95, \"normalized only to \\n this dataset\")\n",
    "    ax.set_xlabel('Potential')\n",
    "    ax.set_ylabel('Density (Normal)')\n",
    "    ax.set_title(f'kbT = {kbt_boltzmann:.1e}')\n",
    "    # Create a twin y-axis for the second histogram\n",
    "    ax_dup = ax.twinx()\n",
    "    ax_dup.hist(boltzmann_scores, density=True, alpha=0.7, color='orange', label='Boltzmann Sampling', \\\n",
    "                bins=np.arange(score_min-2,score_max+1+score_bin,score_bin))\n",
    "    ax.set_xlim(score_min-2,score_max+1)\n",
    "    ax_dup.set_ylabel('Density (Boltzmann)')\n",
    "    ax_dup.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "def plot_interface_score_v_total_score(ax, all_scores_df, \n",
    "                                       total_score_min, total_score_max, interface_score_min, interface_score_max):\n",
    "\n",
    "    ax.scatter(all_scores_df['total_score'], all_scores_df['interface_score'],\n",
    "            c=all_scores_df['index'], cmap='coolwarm_r', s=5)\n",
    "    correlation,_ = pearsonr(all_scores_df['total_score'], all_scores_df['interface_score'])\n",
    "    xmin = all_scores_df['total_score'].min()\n",
    "    xmax = all_scores_df['total_score'].max()\n",
    "    z = np.polyfit(all_scores_df['total_score'], all_scores_df['interface_score'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_trendline = np.linspace(xmin, xmax, 100) \n",
    "    ax.plot(x_trendline, p(x_trendline), \"k\")\n",
    "    ax.set_title(f'Pearson r: {correlation:.2f}')\n",
    "    ax.set_xlim(total_score_min,total_score_max)\n",
    "    ax.set_ylim(interface_score_min,interface_score_max)\n",
    "    ax.set_xlabel('Total Score')\n",
    "    ax.set_ylabel('Interface Score')\n",
    "\n",
    "def plot_combined_score_v_index(ax, combined_scores, all_scores_df):\n",
    "    \n",
    "    combined_scores = pd.Series(combined_scores)\n",
    "    moving_avg = combined_scores.rolling(window=20).mean()\n",
    "    ax.scatter(all_scores_df['index'], combined_scores, c='lightgrey', s=5) \n",
    "    ax.axhline(HIGHSCORE, color='b', alpha = 0.5)\n",
    "    ax.axhline(NEG_BEST, color='r', alpha = 0.5)\n",
    "    PARENTS = [i for i in os.listdir(f'{FOLDER_HOME}/{FOLDER_PARENT}') if i[-4:] == \".pdb\"]\n",
    "    ax.axvline(N_PARENT_JOBS*len(PARENTS), color='k')\n",
    "    ax.plot(range(len(moving_avg)),moving_avg,c=\"k\")\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_xlim(0,MAX_DESIGNS)\n",
    "    ax.set_title('Score vs Index')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Combined Score')    \n",
    "    \n",
    "def plot_combined_score_v_generation(ax, combined_scores, all_scores_df):\n",
    "    \n",
    "    all_scores_df['tmp'] = combined_scores\n",
    "    all_scores_df = all_scores_df.dropna(subset=['tmp'])\n",
    "    \n",
    "    max_gen = int(all_scores_df['generation'].max())\n",
    "    boxplot_data = [all_scores_df[all_scores_df['generation'] == gen]['tmp'] for gen in range(0,max_gen+1,1)]\n",
    "    # Define properties for the outliers\n",
    "    flierprops = dict(marker='o', markerfacecolor='green', markersize=3, linestyle='none')\n",
    "    \n",
    "    ax.boxplot(boxplot_data, positions=range(len(boxplot_data)), flierprops=flierprops)\n",
    "    ax.axhline(HIGHSCORE, color='b')\n",
    "    ax.axhline(NEG_BEST, color='r')\n",
    "    ax.set_xticks(range(len(boxplot_data)))\n",
    "    ax.set_xticklabels(range(0,len(boxplot_data),1))\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_title('Combined Score vs Generations')\n",
    "    ax.set_xlabel('Generation')\n",
    "    ax.set_ylabel('Combined Score')\n",
    "    ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.7)\n",
    "\n",
    "def exponential_func(x, A, k, c):\n",
    "    return c-A*np.exp(-k * x)\n",
    "\n",
    "def plot_combined_score_v_generation_violin(ax, combined_scores, all_scores_df):\n",
    "    \n",
    "    all_scores_df['tmp'] = combined_scores\n",
    "    all_scores_df = all_scores_df.dropna(subset=['tmp'])\n",
    "    \n",
    "    max_gen = int(all_scores_df['generation'].max())\n",
    "    generations = np.arange(0, max_gen + 1)\n",
    "    violin_data = [all_scores_df[all_scores_df['generation'] == gen]['tmp'] for gen in generations]\n",
    "    \n",
    "    # Create violin plots\n",
    "    parts = ax.violinplot(violin_data, positions=generations, showmeans=False, showmedians=True)\n",
    "    \n",
    "    # Customizing the color of violin plots\n",
    "    for pc in parts['bodies']:\n",
    "        pc.set_facecolor('green')\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    # Customizing the color of the median lines\n",
    "    for partname in ('cbars', 'cmins', 'cmaxes'):\n",
    "        vp = parts.get(partname)\n",
    "        if vp:\n",
    "            vp.set_edgecolor('tomato')\n",
    "            vp.set_linewidth(0.5)\n",
    "    \n",
    "    vp = parts.get('cmedians')\n",
    "    if vp:\n",
    "        vp.set_edgecolor('tomato')\n",
    "        vp.set_linewidth(2.0)\n",
    "    \n",
    "    # Fit the data to the exponential function\n",
    "    #weights = np.linspace(1, 0.1, len(generations))\n",
    "    weights = np.ones(len(generations))\n",
    "    #weights[:1] = 0.3\n",
    "    mean_scores = [np.mean(data) for data in violin_data]\n",
    "    popt, pcov = curve_fit(exponential_func, generations, mean_scores, p0=(1, 0.1, 0.7), sigma=weights, maxfev=2000)\n",
    "    \n",
    "    # Plot the fitted curve\n",
    "    fitted_curve = exponential_func(generations, *popt)\n",
    "    ax.plot(generations, fitted_curve, 'r--', label=f'Fit: A*exp(-kt) - c\\nA={popt[0]:.2f}, k={popt[1]:.2f}, c={popt[2]:.2f}')\n",
    "    \n",
    "    ax.axhline(HIGHSCORE, color='b', label='High Score', alpha = 0.5)\n",
    "    ax.axhline(NEG_BEST, color='r', label='Negative Best', alpha = 0.5)\n",
    "\n",
    "    # Select every second generation for ticks\n",
    "    every_second_generation = generations[::2]\n",
    "    ax.set_xticks(every_second_generation)\n",
    "    ax.set_xticklabels(every_second_generation)\n",
    "\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Combined Score vs Generations')\n",
    "    ax.set_xlabel('Generation')\n",
    "    ax.set_ylabel('Combined Score')\n",
    "    ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.7)\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "def plot_score_v_generation_violin(ax, score_type, all_scores_df):\n",
    "    all_scores_df = all_scores_df.dropna(subset=[score_type])\n",
    "    \n",
    "    max_gen = int(all_scores_df['generation'].max())\n",
    "    generations = np.arange(0, max_gen + 1)\n",
    "    violin_data = [all_scores_df[all_scores_df['generation'] == gen][score_type] for gen in generations]\n",
    "    \n",
    "    # Create violin plots\n",
    "    parts = ax.violinplot(violin_data, positions=generations, showmeans=False, showmedians=True)\n",
    "    \n",
    "    # Customizing the color of violin plots\n",
    "    for pc in parts['bodies']:\n",
    "        pc.set_facecolor('green')\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    # Customizing the color of the median lines\n",
    "    for partname in ('cbars', 'cmins', 'cmaxes'):\n",
    "        vp = parts.get(partname)\n",
    "        if vp:\n",
    "            vp.set_edgecolor('tomato')\n",
    "            vp.set_linewidth(0.5)\n",
    "    \n",
    "    vp = parts.get('cmedians')\n",
    "    if vp:\n",
    "        vp.set_edgecolor('tomato')\n",
    "        vp.set_linewidth(2.0)\n",
    "    \n",
    "    # Fit the data to the exponential function\n",
    "    # weights = np.ones(len(generations))\n",
    "    # mean_scores = [np.mean(data) for data in violin_data]\n",
    "    # popt, pcov = curve_fit(exponential_func, generations, mean_scores, p0=(1, 0.1, 0.7), sigma=weights, maxfev=10000)\n",
    "    \n",
    "    # # Plot the fitted curve\n",
    "    # fitted_curve = exponential_func(generations, *popt)\n",
    "    # ax.plot(generations, fitted_curve, 'r--', label=f'Fit: A*exp(-kt) - c\\nA={popt[0]:.2f}, k={popt[1]:.2f}, c={popt[2]:.2f}')\n",
    "    \n",
    "    # ax.axhline(HIGHSCORE, color='b', label='High Score', alpha=0.5)\n",
    "    # ax.axhline(NEG_BEST, color='r', label='Negative Best', alpha=0.5)\n",
    "\n",
    "    # Select every fourth generation for ticks\n",
    "    every_fourth_generation = generations[::4]\n",
    "    ax.set_xticks(every_fourth_generation)\n",
    "    ax.set_xticklabels(every_fourth_generation)\n",
    "\n",
    "    ax.set_title(f'{score_type.replace(\"_\", \" \").title()} vs Generations')\n",
    "    ax.set_xlabel('Generation')\n",
    "    ax.set_ylabel(f'{score_type.replace(\"_\", \" \").title()}')\n",
    "    ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.7)\n",
    "\n",
    "    \n",
    "def plot_mutations_v_generation(ax, all_scores_df,  mut_min, mut_max):\n",
    "    \n",
    "    all_scores_df = all_scores_df.dropna(subset=['mutations'])\n",
    "    \n",
    "    max_gen = int(all_scores_df['generation'].max())\n",
    "    boxplot_data = [all_scores_df[all_scores_df['generation'] == gen]['mutations'] for gen in range(0,max_gen+1,1)]\n",
    "    # Define properties for the outliers\n",
    "    flierprops = dict(marker='o', markerfacecolor='red', markersize=3, linestyle='none')\n",
    "    \n",
    "    ax.boxplot(boxplot_data, positions=range(len(boxplot_data)), flierprops=flierprops)\n",
    "    ax.axhline(len(DESIGN.split(\",\")), color='r')\n",
    "    ax.set_xticks(range(len(boxplot_data)))\n",
    "    ax.set_xticklabels(range(0,len(boxplot_data),1))\n",
    "    ax.set_ylim(mut_min,mut_max)\n",
    "    ax.set_title('Mutations vs Generations')\n",
    "    ax.set_xlabel('Generation')\n",
    "    ax.set_ylabel('Number of Mutations')\n",
    "    ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.7)\n",
    "\n",
    "def plot_mutations_v_generation_violin(ax, all_scores_df, mut_min, mut_max):\n",
    "    \n",
    "    all_scores_df = all_scores_df.dropna(subset=['mutations'])\n",
    "    \n",
    "    max_gen = int(all_scores_df['generation'].max())\n",
    "    generations = np.arange(0, max_gen + 1)\n",
    "    violin_data = [all_scores_df[all_scores_df['generation'] == gen]['mutations'] for gen in generations]\n",
    "    \n",
    "    # Create violin plots\n",
    "    parts = ax.violinplot(violin_data, positions=generations, showmeans=False, showmedians=True)\n",
    "    \n",
    "    # Customizing the color of violin plots\n",
    "    for pc in parts['bodies']:\n",
    "        pc.set_facecolor('green')\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    # Customizing the color of the median lines\n",
    "    for partname in ('cbars', 'cmins', 'cmaxes'):\n",
    "        vp = parts.get(partname)\n",
    "        if vp:\n",
    "            vp.set_edgecolor('tomato')\n",
    "            vp.set_linewidth(0.5)\n",
    "    \n",
    "    vp = parts.get('cmedians')\n",
    "    if vp:\n",
    "        vp.set_edgecolor('tomato')\n",
    "        vp.set_linewidth(2.0)\n",
    "\n",
    "    # Fit the data to the exponential function\n",
    "    # mean_mutations = [np.mean(data) for data in violin_data]\n",
    "    # weights = np.ones(len(generations))  # Uniform weights, adjust as needed\n",
    "    # popt, pcov = curve_fit(exponential_func, generations, mean_mutations, p0=(1, 0.1, 0.7), sigma=weights, maxfev=2000)\n",
    "    \n",
    "    # # Plot the fitted curve\n",
    "    # fitted_curve = exponential_func(generations, *popt)\n",
    "    # ax.plot(generations, fitted_curve, 'r--', label=f'Fit: A*exp(-kt) + c\\nA={popt[0]:.2f}, k={popt[1]:.2f}, c={popt[2]:.2f}')\n",
    "    \n",
    "    \n",
    "    ax.axhline(len(DESIGN.split(\",\")), color='r', label='Design Length')\n",
    "\n",
    "    # Select every second generation for ticks\n",
    "    every_second_generation = generations[::2]\n",
    "    ax.set_xticks(every_second_generation)\n",
    "    ax.set_xticklabels(every_second_generation)\n",
    "    \n",
    "    ax.set_ylim(mut_min, mut_max)\n",
    "    ax.set_title('Mutations vs Generations')\n",
    "    ax.set_xlabel('Generation')\n",
    "    ax.set_ylabel('Number of Mutations')\n",
    "    ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.7)\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "def plot_delta_scores():\n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "    all_scores_df = all_scores_df.dropna(subset=['total_score'])\n",
    "    \n",
    "    # Calculate combined scores using normalized scores\n",
    "    _, _, _, _, combined_scores = normalize_scores(all_scores_df, print_norm=True, norm_all=True)\n",
    "    \n",
    "    # Add combined scores to the DataFrame\n",
    "    all_scores_df['combined_score'] = combined_scores\n",
    "\n",
    "    # Calculate delta scores\n",
    "    all_scores_df['delta_combined'] = all_scores_df.apply(lambda row: row['combined_score'] - all_scores_df.loc[all_scores_df['index'] == int(float(row['parent_index'])), 'combined_score'].values[0] if row['parent_index'] != \"Parent\" else 0, axis=1)\n",
    "    all_scores_df['delta_total'] = all_scores_df.apply(lambda row: row['total_score'] - all_scores_df.loc[all_scores_df['index'] == int(float(row['parent_index'])), 'total_score'].values[0] if row['parent_index'] != \"Parent\" else 0, axis=1)\n",
    "    all_scores_df['delta_interface'] = all_scores_df.apply(lambda row: row['interface_score'] - all_scores_df.loc[all_scores_df['index'] == int(float(row['parent_index'])), 'interface_score'].values[0] if row['parent_index'] != \"Parent\" else 0, axis=1)\n",
    "    all_scores_df['delta_efield'] = all_scores_df.apply(lambda row: row['efield_score'] - all_scores_df.loc[all_scores_df['index'] == int(float(row['parent_index'])), 'efield_score'].values[0] if row['parent_index'] != \"Parent\" else 0, axis=1)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    def plot_violin(ax, delta_scores, title, all_scores_df):\n",
    "        all_scores_df['tmp'] = delta_scores\n",
    "        all_scores_df = all_scores_df.dropna(subset=['tmp'])\n",
    "\n",
    "        max_gen = int(all_scores_df['generation'].max())\n",
    "        generations = np.arange(0, max_gen + 1)\n",
    "        violin_data = [all_scores_df[all_scores_df['generation'] == gen]['tmp'] for gen in generations]\n",
    "\n",
    "        # Create violin plots\n",
    "        parts = ax.violinplot(violin_data, positions=generations, showmeans=False, showmedians=True)\n",
    "\n",
    "        # Customizing the color of violin plots\n",
    "        for pc in parts['bodies']:\n",
    "            pc.set_facecolor('green')\n",
    "            pc.set_edgecolor('black')\n",
    "            pc.set_alpha(0.7)\n",
    "\n",
    "        # Customizing the color of the median lines\n",
    "        for partname in ('cbars', 'cmins', 'cmaxes'):\n",
    "            vp = parts.get(partname)\n",
    "            if vp:\n",
    "                vp.set_edgecolor('tomato')\n",
    "                vp.set_linewidth(0.5)\n",
    "\n",
    "        vp = parts.get('cmedians')\n",
    "        if vp:\n",
    "            vp.set_edgecolor('tomato')\n",
    "            vp.set_linewidth(2.0)\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Generation')\n",
    "        ax.set_ylabel('Delta Score')\n",
    "        ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.7)\n",
    "\n",
    "    plot_violin(axs[0, 0], all_scores_df['delta_combined'], 'Delta Combined Score vs Generations', all_scores_df)\n",
    "    plot_violin(axs[0, 1], all_scores_df['delta_total'], 'Delta Total Score vs Generations', all_scores_df)\n",
    "    plot_violin(axs[1, 0], all_scores_df['delta_interface'], 'Delta Interface Score vs Generations', all_scores_df)\n",
    "    plot_violin(axs[1, 1], all_scores_df['delta_efield'], 'Delta Efield Score vs Generations', all_scores_df)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_tree_lin(leaf_nodes=None):\n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "    _, _, _, _, combined_potentials = normalize_scores(all_scores_df, print_norm=False, norm_all=False, extension=\"potential\")\n",
    "\n",
    "    max_gen = int(all_scores_df['generation'].max())\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for idx, row in all_scores_df.iterrows():\n",
    "        G.add_node(idx, sequence=row['sequence'], interface_potential=row['interface_potential'], gen=int(row['generation']))\n",
    "        if row['parent_index'] != \"Parent\":\n",
    "            parent_idx = int(float(row['parent_index']))\n",
    "            parent_sequence = all_scores_df.loc[all_scores_df.index == parent_idx, 'sequence'].values[0]\n",
    "            current_sequence = row['sequence']\n",
    "            # Calculate Hamming distance\n",
    "            distance = hamming_distance(parent_sequence, current_sequence)\n",
    "            # Add edge with Hamming distance as an attribute\n",
    "            G.add_edge(parent_idx, idx, hamming_distance=distance)\n",
    "\n",
    "    if leaf_nodes is not None:\n",
    "        subgraph_nodes = set()\n",
    "        for leaf in leaf_nodes:\n",
    "            subgraph_nodes.update(nx.ancestors(G, leaf))\n",
    "            subgraph_nodes.add(leaf)\n",
    "        G = G.subgraph(subgraph_nodes)\n",
    "\n",
    "    G_undirected = G.to_undirected()\n",
    "\n",
    "    # Find connected components\n",
    "    connected_components = list(nx.connected_components(G_undirected))\n",
    "\n",
    "    largest_component = max(connected_components, key=len)\n",
    "    # Create a subgraph of G using only the nodes in the largest component\n",
    "    G_largest = G.subgraph(largest_component)\n",
    "\n",
    "    def set_node_positions(G, node, pos, x, y, counts):\n",
    "        pos[node] = (x, y)\n",
    "        neighbors = list(G.successors(node))\n",
    "        next_y = y - counts[node] / 2\n",
    "        for neighbor in neighbors:\n",
    "            set_node_positions(G, neighbor, pos, x + 1, next_y + counts[neighbor] / 2, counts)\n",
    "            next_y += counts[neighbor]\n",
    "\n",
    "    def count_descendants(G, node, counts):\n",
    "        neighbors = list(G.successors(node))\n",
    "        count = 1\n",
    "        for neighbor in neighbors:\n",
    "            count += count_descendants(G, neighbor, counts)\n",
    "        counts[node] = count\n",
    "        return count\n",
    "\n",
    "    counts = {}\n",
    "    root_node = list(largest_component)[0]\n",
    "    count_descendants(G_largest, root_node, counts)\n",
    "\n",
    "    pos = {}\n",
    "    set_node_positions(G_largest, root_node, pos, 0, 0, counts)\n",
    "    y_values = [y for x, y in pos.values()]\n",
    "    y_span = max(y_values) - min(y_values)\n",
    "    print(y_span)\n",
    "\n",
    "    colors = combined_potentials\n",
    "    colors[0] = np.nan\n",
    "    normed_colors = [(x - np.nanmin(colors[1:])) / (np.nanmax(colors[1:]) - np.nanmin(colors[1:])) for x in colors]\n",
    "    normed_colors = np.nan_to_num(normed_colors, nan=0)\n",
    "    normed_colors = normed_colors**2\n",
    "\n",
    "    # Convert positions to polar coordinates\n",
    "    polar_pos = {node: ((x / (max(pos.values(), key=lambda p: p[0])[0] - min(pos.values(), key=lambda p: p[0])[0])) * 2 * np.pi, y) for node, (x, y) in pos.items()}\n",
    "\n",
    "    # Convert polar coordinates to Cartesian coordinates for plotting\n",
    "    cartesian_pos = {node: (radius * np.cos(angle), radius * np.sin(angle)) for node, (radius, angle) in polar_pos.items()}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), dpi=300)\n",
    "\n",
    "    # Draw the graph with the positions set\n",
    "    for start, end in G_largest.edges():\n",
    "        color = plt.cm.coolwarm_r(normed_colors[end])\n",
    "        if float(normed_colors[end]) == 0.0:\n",
    "            color = [0., 0., 0., 1.]\n",
    "        linewidth = 0.1 + 2 * normed_colors[end] * 0.01\n",
    "\n",
    "        x0, y0 = cartesian_pos[start]\n",
    "        x1, y1 = cartesian_pos[end]\n",
    "        ax.plot([x0, x1], [y0, y1], color=color, linewidth=linewidth)\n",
    "\n",
    "    # Adjust axis labels and ticks for the swapped axes\n",
    "    ax.axis('on')\n",
    "    ax.set_title(\"Colored by Potential\")\n",
    "    ax.set_xlabel(\"X Coordinate\")\n",
    "    ax.set_ylabel(\"Y Coordinate\")\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.axis('equal')\n",
    "    ax.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "def plot_tree_nx_all():\n",
    "    PARENT = '/net/bs-gridfs/export/grid/scratch/lmerlicek/design/Input/1ohp.pdb'\n",
    "    from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "    _, _, interface_potentials, _, combined_potentials = normalize_scores(all_scores_df, print_norm=False, norm_all=False, extension=\"potential\")\n",
    "    all_scores_df[\"interface_potential\"] = interface_potentials\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for _, row in all_scores_df.iterrows():\n",
    "        index = int(float(row['index'])) + 1\n",
    "        if not isinstance(row['sequence'], str):\n",
    "            continue\n",
    "        G.add_node(index, sequence=row['sequence'], interface_potential=row['interface_potential'], gen=int(row['generation']) + 1)\n",
    "        if row['parent_index'] != \"Parent\":\n",
    "            parent_idx = int(float(row['parent_index'])) + 1\n",
    "            parent_sequence = all_scores_df.loc[all_scores_df.index == parent_idx - 1, 'sequence'].values[0]\n",
    "            current_sequence = row['sequence']\n",
    "            # Calculate Hamming distance\n",
    "            distance = hamming_distance(parent_sequence, current_sequence)\n",
    "            # Add edge with Hamming distance as an attribute\n",
    "            G.add_edge(parent_idx, index, hamming_distance=distance)\n",
    "\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    # Create a new root node\n",
    "    G.add_node(0, sequence='root', interface_potential=0, gen=0)\n",
    "    \n",
    "    # Connect the new root node to all nodes of generation 1\n",
    "    for node in G.nodes:\n",
    "        if G.nodes[node]['gen'] == 1:\n",
    "            parent_sequence = extract_sequence_from_pdb(PARENT)\n",
    "            current_sequence = G.nodes[node]['sequence']\n",
    "            distance = hamming_distance(parent_sequence, current_sequence)\n",
    "            G.add_edge(0, node, hamming_distance=distance)\n",
    "\n",
    "    # Use graphviz_layout to get the positions for a circular layout\n",
    "    pos = graphviz_layout(G, prog=\"twopi\", args=\"\")\n",
    "\n",
    "    # Normalize scores from 0 to 1\n",
    "    scores = {node: all_scores_df.loc[all_scores_df['index'] == int(node)-1, 'interface_score'].values[0] for node in G.nodes if node != 0}\n",
    "    min_score = min(scores.values())\n",
    "    max_score = max(scores.values())\n",
    "    normalized_scores = {node: (score - min_score) / (max_score - min_score) for node, score in scores.items()}\n",
    "\n",
    "    # Get node colors based on index\n",
    "    node_colors = [plt.cm.viridis(int(node) / len(G.nodes)) for node in G.nodes]\n",
    "\n",
    "    # Mark generation 0 nodes with red\n",
    "    gen_0_nodes = [node for node in G.nodes if G.nodes[node]['gen'] == 0]\n",
    "    for node in gen_0_nodes:\n",
    "        node_colors[list(G.nodes).index(node)] = 'red'\n",
    "\n",
    "    # Normalize Hamming distances for edge colors\n",
    "    hamming_distances = [G.edges[edge]['hamming_distance'] for edge in G.edges]\n",
    "    # Normalize the Hamming distances\n",
    "    min_hamming = min(hamming_distances)\n",
    "    max_hamming = max(hamming_distances)\n",
    "    normalized_hamming = [(dist - min_hamming) / (max_hamming - min_hamming) for dist in hamming_distances]\n",
    "\n",
    "    # Plot the graph\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), dpi=300)\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax, node_size=5, node_color=node_colors, linewidths=0.01)\n",
    "    \n",
    "    # Draw edges with custom color based on normalized Hamming distance\n",
    "    edge_colors = ['blue' if norm_dist == 0 else plt.cm.RdYlGn(norm_dist) for norm_dist in normalized_hamming]\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, width=0.2, edge_color=edge_colors, style='-', arrows=False)\n",
    "\n",
    "    # Create a colorbar as a legend for Hamming distances\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlGn, norm=plt.Normalize(vmin=min_hamming, vmax=max_hamming))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax)\n",
    "    cbar.set_label('Hamming Distance')\n",
    "\n",
    "    ax.set_title(\"Colored by Index, Gen 0 in Red, Edges by Hamming Distance\")\n",
    "    ax.axis(\"equal\")\n",
    "    plt.show()\n",
    "\n",
    "def calculate_rank_order(matrix):\n",
    "    # Calculate the occurrence frequency of each amino acid in each column\n",
    "    unique, counts = np.unique(matrix, return_counts=True)\n",
    "    frequencies = dict(zip(unique, counts))\n",
    "    \n",
    "    # Sort amino acids in each column by their frequency, then alphabetically\n",
    "    sorted_amino_acids = sorted(frequencies.items(), key=lambda x: (x[1], -ord(x[0])), reverse=True)\n",
    "    \n",
    "    # Assign rank order based on sorted position\n",
    "    rank_order = {amino_acid: rank for rank, (amino_acid, _) in enumerate(sorted_amino_acids, start=1)}\n",
    "    \n",
    "    # Replace amino acids with their rank order\n",
    "    rank_matrix = np.vectorize(rank_order.get)(matrix)\n",
    "    \n",
    "    return rank_matrix\n",
    "\n",
    "def seq_to_rank_order_matrix(sequences):\n",
    "    # Convert sequences to a 2D numpy array (matrix) of characters\n",
    "    matrix = np.array([list(seq) for seq in sequences])\n",
    "    \n",
    "    # Initialize an empty matrix to store the rank order numbers\n",
    "    rank_order_matrix = np.zeros(matrix.shape, dtype=int)\n",
    "    \n",
    "    # Calculate rank order for each column\n",
    "    for i in range(matrix.shape[1]):  # Iterate over columns\n",
    "        column = matrix[:, i]\n",
    "        rank_order_matrix[:, i] = calculate_rank_order(column)\n",
    "    \n",
    "    return rank_order_matrix\n",
    "\n",
    "def seq_to_numeric(seq):\n",
    "    # Define a mapping for all 20 standard amino acids plus 'X' for unknown\n",
    "    mapping = {\n",
    "        'A': 1,  'C': 2,  'D': 3,  'E': 4,\n",
    "        'F': 5,  'G': 6,  'H': 7,  'I': 8,\n",
    "        'K': 9,  'L': 10, 'M': 11, 'N': 12,\n",
    "        'P': 13, 'Q': 14, 'R': 15, 'S': 16,\n",
    "        'T': 17, 'V': 18, 'W': 19, 'Y': 20,\n",
    "        'X': 0   # 'X' for any unknown or non-standard amino acid\n",
    "    }\n",
    "    numeric_seq = [mapping[char] for char in seq]\n",
    "    return numeric_seq\n",
    "\n",
    "def plot_pca_umap():\n",
    "    \n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "\n",
    "    all_scores_df = all_scores_df.dropna(subset=['total_score'])\n",
    "    all_scores_df = all_scores_df.dropna(subset=['catalytic_score'])\n",
    "    all_scores_df = all_scores_df.dropna(subset=['interface_score'])\n",
    "    \n",
    "    numeric_seqs = seq_to_rank_order_matrix(all_scores_df['sequence'].tolist())\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(numeric_seqs)\n",
    "\n",
    "    pca3 = PCA(n_components=3)\n",
    "    pca_result3 = pca3.fit_transform(numeric_seqs)\n",
    "\n",
    "    # Analyze PCA loadings for PC1\n",
    "    # loadings = pca.components_.T[:, 0]  # Loadings for PC1\n",
    "    # plt.figure(figsize=(10, 4))\n",
    "    # plt.bar(range(len(loadings)), loadings)\n",
    "    # plt.title('PCA Loadings for PC1')\n",
    "    # plt.xlabel('Sequence Position')\n",
    "    # plt.ylabel('Loading Value')\n",
    "    # plt.show()\n",
    "\n",
    "    # Perform UMAP\n",
    "    reducer = umap.UMAP()\n",
    "    umap_result = reducer.fit_transform(numeric_seqs)\n",
    "\n",
    "    # Create a figure and a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(12, 24))  # Adjust the figure size as needed\n",
    "\n",
    "    # Define a base font size\n",
    "    base_font_size = 10  # Adjust here\n",
    "\n",
    "    # Plot UMAP Interface score\n",
    "    axs[0].scatter(umap_result[:, 0], umap_result[:, 1], c=all_scores_df['interface_score'], cmap='viridis', alpha=0.6, s=1)\n",
    "    cbar = fig.colorbar(axs[0].collections[0], ax=axs[0], label='Interface Score')\n",
    "    axs[0].set_title('UMAP of Sequences - Interface score', fontsize=base_font_size * 2)\n",
    "    axs[0].set_xlabel('UMAP1', fontsize=base_font_size * 2)\n",
    "    axs[0].set_ylabel('UMAP2', fontsize=base_font_size * 2)\n",
    "    cbar.set_label('Interface Score', size=base_font_size * 2)\n",
    "\n",
    "    # Filter the DataFrame to include only rows where 'total_score' is <= -340\n",
    "    filtered_df = all_scores_df[all_scores_df['total_score'] <= -340]\n",
    "    filtered_umap_result = umap_result[all_scores_df['total_score'] <= -340]\n",
    "\n",
    "    # Now plot using the filtered data\n",
    "    axs[1].scatter(filtered_umap_result[:, 0], filtered_umap_result[:, 1], c=filtered_df['total_score'], cmap='viridis', alpha=0.6, s=1)\n",
    "    cbar = fig.colorbar(axs[1].collections[0], ax=axs[1], label='Total Score')\n",
    "    axs[1].set_title('UMAP of Sequences - Total score', fontsize=base_font_size * 2)\n",
    "    axs[1].set_xlabel('UMAP1', fontsize=base_font_size * 2)\n",
    "    axs[1].set_ylabel('UMAP2', fontsize=base_font_size * 2)\n",
    "    cbar.set_label('Total Score', size=base_font_size * 2)\n",
    "\n",
    "    # Plot UMAP with 'index' as the color\n",
    "    axs[2].scatter(umap_result[:, 0], umap_result[:, 1], c=all_scores_df['index'], cmap='viridis', alpha=0.6, s=1)\n",
    "    cbar = fig.colorbar(axs[2].collections[0], ax=axs[2], label='Generation')\n",
    "    axs[2].set_title('UMAP of Sequences - Generation', fontsize=base_font_size * 2)\n",
    "    axs[2].set_xlabel('UMAP1', fontsize=base_font_size * 2)\n",
    "    axs[2].set_ylabel('UMAP2', fontsize=base_font_size * 2)\n",
    "    cbar.set_label('Generation', size=base_font_size * 2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_esm_umap():\n",
    "\n",
    "    #ESM embeddings and UMAP\n",
    "    def prepare_data(sequences):\n",
    "        \"\"\" Convert a list of protein sequences to the model's input format. \"\"\"\n",
    "        batch_tokens = []\n",
    "        for seq in sequences:\n",
    "            tokens = torch.tensor([alphabet.encode(seq)], dtype=torch.long)\n",
    "            batch_tokens.append(tokens)\n",
    "        return torch.cat(batch_tokens)\n",
    "\n",
    "    # 1. Load ESM model\n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess data\n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "    all_scores_df.dropna(subset=['total_score', 'catalytic_score', 'interface_score', 'sequence'], inplace=True)\n",
    "\n",
    "    # Extract sequences\n",
    "    sequences = all_scores_df['sequence'].tolist()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokens = prepare_data(sequences)\n",
    "        results = model(tokens, repr_layers=[33])  # Specify the layer you want\n",
    "        token_embeddings = results[\"representations\"][33]\n",
    "\n",
    "        # Mean pooling over positions\n",
    "        sequence_embeddings = token_embeddings.mean(dim=1)\n",
    "        \n",
    "    embeddings_array = sequence_embeddings.cpu().numpy()\n",
    "\n",
    "    # Perform UMAP\n",
    "    reducer = umap.UMAP()\n",
    "    umap_result = reducer.fit_transform(embeddings_array)\n",
    "\n",
    "    # Create a figure and a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(12, 24))  # Adjust the figure size as needed\n",
    "\n",
    "    # Define a base font size\n",
    "    base_font_size = 10  # Adjust here\n",
    "\n",
    "    # Plot UMAP Interface score\n",
    "    axs[0].scatter(umap_result[:, 0], umap_result[:, 1], c=all_scores_df['interface_score'], cmap='viridis', alpha=0.6, s=1)\n",
    "    cbar = fig.colorbar(axs[0].collections[0], ax=axs[0], label='Interface Score')\n",
    "    axs[0].set_title('UMAP of Sequences - Interface score', fontsize=base_font_size * 2)\n",
    "    axs[0].set_xlabel('UMAP1', fontsize=base_font_size * 2)\n",
    "    axs[0].set_ylabel('UMAP2', fontsize=base_font_size * 2)\n",
    "    cbar.set_label('Interface Score', size=base_font_size * 2)\n",
    "\n",
    "    # Filter the DataFrame to include only rows where 'total_score' is <= -340\n",
    "    filtered_df = all_scores_df[all_scores_df['total_score'] <= -340]\n",
    "    filtered_umap_result = umap_result[all_scores_df['total_score'] <= -340]\n",
    "\n",
    "    # Now plot using the filtered data\n",
    "    axs[1].scatter(filtered_umap_result[:, 0], filtered_umap_result[:, 1], c=filtered_df['total_score'], cmap='viridis', alpha=0.6, s=1)\n",
    "    cbar = fig.colorbar(axs[1].collections[0], ax=axs[1], label='Total Score')\n",
    "    axs[1].set_title('UMAP of Sequences - Total score', fontsize=base_font_size * 2)\n",
    "    axs[1].set_xlabel('UMAP1', fontsize=base_font_size * 2)\n",
    "    axs[1].set_ylabel('UMAP2', fontsize=base_font_size * 2)\n",
    "    cbar.set_label('Total Score', size=base_font_size * 2)\n",
    "\n",
    "    # Plot UMAP with 'index' as the color\n",
    "    axs[2].scatter(umap_result[:, 0], umap_result[:, 1], c=all_scores_df['index'], cmap='viridis', alpha=0.6, s=1)\n",
    "    cbar = fig.colorbar(axs[2].collections[0], ax=axs[2], label='Generation')\n",
    "    axs[2].set_title('UMAP of Sequences - Generation', fontsize=base_font_size * 2)\n",
    "    axs[2].set_xlabel('UMAP1', fontsize=base_font_size * 2)\n",
    "    axs[2].set_ylabel('UMAP2', fontsize=base_font_size * 2)\n",
    "    cbar.set_label('Generation', size=base_font_size * 2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def find_mutations(seq1, seq2):\n",
    "    # Function to compare sequences and find mutation positions\n",
    "    return [i for i, (a, b) in enumerate(zip(seq1, seq2)) if a != b]\n",
    "\n",
    "def normalize_columnwise(matrix):\n",
    "    min_vals = matrix.min(axis=0)\n",
    "    max_vals = matrix.max(axis=0)\n",
    "    # Avoid division by zero\n",
    "    denom = np.where((max_vals - min_vals) == 0, 1, (max_vals - min_vals))\n",
    "    normalized_matrix = (matrix - min_vals) / denom\n",
    "    return normalized_matrix\n",
    "\n",
    "def plot_mut_location():\n",
    "    # Load the data\n",
    "    all_scores_df = pd.read_csv(ALL_SCORES_CSV)\n",
    "\n",
    "    all_scores_df = all_scores_df.dropna(subset=['sequence'])\n",
    "\n",
    "    # Assuming the maximum length of sequences is 125\n",
    "    max_length = 125\n",
    "    max_generation = int(all_scores_df['generation'].max())\n",
    "\n",
    "    # Initialize a matrix to hold mutation frequencies\n",
    "    mutation_matrix = np.zeros((max_length, max_generation + 1))\n",
    "\n",
    "    # Populate the mutation matrix\n",
    "    for _, row in all_scores_df.iterrows():\n",
    "        if pd.notnull(row['parent_index']) and row['parent_index'] != \"Parent\":  # Check if there's a valid parent\n",
    "            parent_seq = all_scores_df.loc[all_scores_df['index'] == float(row['parent_index']), 'sequence'].values[0]\n",
    "            mutations = find_mutations(row['sequence'], parent_seq)\n",
    "            for pos in mutations:\n",
    "                mutation_matrix[pos, int(row['generation'])] += 1\n",
    "\n",
    "    # Normalize the mutation_matrix column-wise (i.e., each generation separately)\n",
    "    normalized_mutation_matrix = normalize_columnwise(mutation_matrix)\n",
    "\n",
    "   # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    c = ax.imshow(normalized_mutation_matrix, aspect='auto', origin='lower', cmap='viridis', extent=[0, max_generation, 0, max_length])\n",
    "    ax.set_xlabel('Generation')\n",
    "    ax.set_ylabel('Position along AA chain')\n",
    "    ax.set_title('Frequency of Mutation Over Generations')\n",
    "    fig.colorbar(c, ax=ax, label='Normalized Frequency of Mutation')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e1fb9",
   "metadata": {},
   "source": [
    "## Functions needed to run AIzyme algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c129bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AIzyme Functions loaded!\")\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2a4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
